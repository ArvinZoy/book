<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>3.5 Dimensionality reduction techniques: visualizing complex data sets in 2D | Computational Genomics With R</title>
  <meta name="description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="3.5 Dimensionality reduction techniques: visualizing complex data sets in 2D | Computational Genomics With R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://compmgenomr.github.io/book/" />
  <meta property="og:image" content="https://compmgenomr.github.io/book/images/cover.jpg" />
  <meta property="og:description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience" />
  <meta name="github-repo" content="compgenomr/bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.5 Dimensionality reduction techniques: visualizing complex data sets in 2D | Computational Genomics With R" />
  
  <meta name="twitter:description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience" />
  <meta name="twitter:image" content="https://compmgenomr.github.io/book/images/cover.jpg" />

<meta name="author" content="Altuna Akalin">


<meta name="date" content="2019-03-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="clustering-grouping-samples-based-on-their-similarity.html">
<link rel="next" href="exercises-1.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-83786243-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-83786243-1');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Genomics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="who-is-this-book-for.html"><a href="who-is-this-book-for.html"><i class="fa fa-check"></i>Who is this book for?</a><ul>
<li class="chapter" data-level="" data-path="who-is-this-book-for.html"><a href="who-is-this-book-for.html#what-will-you-get-out-of-this"><i class="fa fa-check"></i>What will you get out of this?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="structure-of-the-book.html"><a href="structure-of-the-book.html"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="software-information-and-conventions.html"><a href="software-information-and-conventions.html"><i class="fa fa-check"></i>Software information and conventions</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="how-to-contribute.html"><a href="how-to-contribute.html"><i class="fa fa-check"></i>How to contribute</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Genomics</a><ul>
<li class="chapter" data-level="1.1" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html"><i class="fa fa-check"></i><b>1.1</b> Genes, DNA and central dogma</a><ul>
<li class="chapter" data-level="1.1.1" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#what-is-a-genome"><i class="fa fa-check"></i><b>1.1.1</b> What is a genome?</a></li>
<li class="chapter" data-level="1.1.2" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#what-is-a-gene"><i class="fa fa-check"></i><b>1.1.2</b> What is a gene?</a></li>
<li class="chapter" data-level="1.1.3" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#how-genes-are-controlled-the-transcriptional-and-the-post-transcriptional-regulation"><i class="fa fa-check"></i><b>1.1.3</b> How genes are controlled ? The transcriptional and the post-transcriptional regulation</a></li>
<li class="chapter" data-level="1.1.4" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#what-does-a-gene-look-like"><i class="fa fa-check"></i><b>1.1.4</b> What does a gene look like?</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="elements-of-gene-regulation.html"><a href="elements-of-gene-regulation.html"><i class="fa fa-check"></i><b>1.2</b> Elements of gene regulation</a><ul>
<li class="chapter" data-level="1.2.1" data-path="elements-of-gene-regulation.html"><a href="elements-of-gene-regulation.html#transcriptional-regulation"><i class="fa fa-check"></i><b>1.2.1</b> Transcriptional regulation</a></li>
<li class="chapter" data-level="1.2.2" data-path="elements-of-gene-regulation.html"><a href="elements-of-gene-regulation.html#post-transcriptional-regulation"><i class="fa fa-check"></i><b>1.2.2</b> Post-transcriptional regulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="shaping-the-genome-dna-mutation.html"><a href="shaping-the-genome-dna-mutation.html"><i class="fa fa-check"></i><b>1.3</b> Shaping the genome: DNA mutation</a></li>
<li class="chapter" data-level="1.4" data-path="high-throughput-experimental-methods-in-genomics.html"><a href="high-throughput-experimental-methods-in-genomics.html"><i class="fa fa-check"></i><b>1.4</b> High-throughput experimental methods in genomics</a><ul>
<li class="chapter" data-level="1.4.1" data-path="high-throughput-experimental-methods-in-genomics.html"><a href="high-throughput-experimental-methods-in-genomics.html#the-general-idea-behind-high-throughput-techniques"><i class="fa fa-check"></i><b>1.4.1</b> The general idea behind high-throughput techniques</a></li>
<li class="chapter" data-level="1.4.2" data-path="high-throughput-experimental-methods-in-genomics.html"><a href="high-throughput-experimental-methods-in-genomics.html#high-throughput-sequencing"><i class="fa fa-check"></i><b>1.4.2</b> High-throughput sequencing</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="visualization-and-data-repositories-for-genomics.html"><a href="visualization-and-data-repositories-for-genomics.html"><i class="fa fa-check"></i><b>1.5</b> Visualization and data repositories for genomics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Rintro.html"><a href="Rintro.html"><i class="fa fa-check"></i><b>2</b> Introduction to R for genomic data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html"><i class="fa fa-check"></i><b>2.1</b> Steps of (genomic) data analysis</a><ul>
<li class="chapter" data-level="2.1.1" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#data-collection"><i class="fa fa-check"></i><b>2.1.1</b> Data collection</a></li>
<li class="chapter" data-level="2.1.2" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#data-quality-check-and-cleaning"><i class="fa fa-check"></i><b>2.1.2</b> Data quality check and cleaning</a></li>
<li class="chapter" data-level="2.1.3" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#data-processing"><i class="fa fa-check"></i><b>2.1.3</b> Data processing</a></li>
<li class="chapter" data-level="2.1.4" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#exploratory-data-analysis-and-modeling"><i class="fa fa-check"></i><b>2.1.4</b> Exploratory data analysis and modeling</a></li>
<li class="chapter" data-level="2.1.5" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#visualization-and-reporting"><i class="fa fa-check"></i><b>2.1.5</b> Visualization and reporting</a></li>
<li class="chapter" data-level="2.1.6" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#why-use-r-for-genomics"><i class="fa fa-check"></i><b>2.1.6</b> Why use R for genomics ?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html"><i class="fa fa-check"></i><b>2.2</b> Getting started with R</a><ul>
<li class="chapter" data-level="2.2.1" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#installing-packages"><i class="fa fa-check"></i><b>2.2.1</b> Installing packages</a></li>
<li class="chapter" data-level="2.2.2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#installing-packages-in-custom-locations"><i class="fa fa-check"></i><b>2.2.2</b> Installing packages in custom locations</a></li>
<li class="chapter" data-level="2.2.3" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#getting-help-on-functions-and-packages"><i class="fa fa-check"></i><b>2.2.3</b> Getting help on functions and packages</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="computations-in-r.html"><a href="computations-in-r.html"><i class="fa fa-check"></i><b>2.3</b> Computations in R</a></li>
<li class="chapter" data-level="2.4" data-path="data-structures.html"><a href="data-structures.html"><i class="fa fa-check"></i><b>2.4</b> Data structures</a><ul>
<li class="chapter" data-level="2.4.1" data-path="data-structures.html"><a href="data-structures.html#vectors"><i class="fa fa-check"></i><b>2.4.1</b> Vectors</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-structures.html"><a href="data-structures.html#matrices"><i class="fa fa-check"></i><b>2.4.2</b> Matrices</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-structures.html"><a href="data-structures.html#data-frames"><i class="fa fa-check"></i><b>2.4.3</b> Data Frames</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-structures.html"><a href="data-structures.html#lists"><i class="fa fa-check"></i><b>2.4.4</b> Lists</a></li>
<li class="chapter" data-level="2.4.5" data-path="data-structures.html"><a href="data-structures.html#factors"><i class="fa fa-check"></i><b>2.4.5</b> Factors</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-types.html"><a href="data-types.html"><i class="fa fa-check"></i><b>2.5</b> Data types</a></li>
<li class="chapter" data-level="2.6" data-path="reading-and-writing-data.html"><a href="reading-and-writing-data.html"><i class="fa fa-check"></i><b>2.6</b> Reading and writing data</a></li>
<li class="chapter" data-level="2.7" data-path="plotting-in-r.html"><a href="plotting-in-r.html"><i class="fa fa-check"></i><b>2.7</b> Plotting in R</a></li>
<li class="chapter" data-level="2.8" data-path="saving-plots.html"><a href="saving-plots.html"><i class="fa fa-check"></i><b>2.8</b> Saving plots</a></li>
<li class="chapter" data-level="2.9" data-path="functions-and-control-structures-for-ifelse-etc-.html"><a href="functions-and-control-structures-for-ifelse-etc-.html"><i class="fa fa-check"></i><b>2.9</b> Functions and control structures (for, if/else etc.)</a><ul>
<li class="chapter" data-level="2.9.1" data-path="functions-and-control-structures-for-ifelse-etc-.html"><a href="functions-and-control-structures-for-ifelse-etc-.html#user-defined-functions"><i class="fa fa-check"></i><b>2.9.1</b> User defined functions</a></li>
<li class="chapter" data-level="2.9.2" data-path="functions-and-control-structures-for-ifelse-etc-.html"><a href="functions-and-control-structures-for-ifelse-etc-.html#loops-and-looping-structures-in-r"><i class="fa fa-check"></i><b>2.9.2</b> Loops and looping structures in R</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.10</b> Exercises</a><ul>
<li class="chapter" data-level="2.10.1" data-path="exercises.html"><a href="exercises.html#computations-in-r-1"><i class="fa fa-check"></i><b>2.10.1</b> Computations in R</a></li>
<li class="chapter" data-level="2.10.2" data-path="exercises.html"><a href="exercises.html#data-structures-in-r"><i class="fa fa-check"></i><b>2.10.2</b> Data structures in R</a></li>
<li class="chapter" data-level="2.10.3" data-path="exercises.html"><a href="exercises.html#reading-in-and-writing-data-out-in-r"><i class="fa fa-check"></i><b>2.10.3</b> Reading in and writing data out in R</a></li>
<li class="chapter" data-level="2.10.4" data-path="exercises.html"><a href="exercises.html#plotting-in-r-1"><i class="fa fa-check"></i><b>2.10.4</b> Plotting in R</a></li>
<li class="chapter" data-level="2.10.5" data-path="exercises.html"><a href="exercises.html#functions-and-control-structures-for-ifelse-etc.-1"><i class="fa fa-check"></i><b>2.10.5</b> Functions and control structures (for, if/else etc.)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="stats.html"><a href="stats.html"><i class="fa fa-check"></i><b>3</b> Statistics and Exploratory Data Analysis for Genomics</a><ul>
<li class="chapter" data-level="3.1" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><i class="fa fa-check"></i><b>3.1</b> How to summarize collection of data points: The idea behind statistical distributions</a><ul>
<li class="chapter" data-level="3.1.1" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-central-tendency-mean-and-median"><i class="fa fa-check"></i><b>3.1.1</b> Describing the central tendency: mean and median</a></li>
<li class="chapter" data-level="3.1.2" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-spread-measurements-of-variation"><i class="fa fa-check"></i><b>3.1.2</b> Describing the spread: measurements of variation</a></li>
<li class="chapter" data-level="3.1.3" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#precision-of-estimates-confidence-intervals"><i class="fa fa-check"></i><b>3.1.3</b> Precision of estimates: Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html"><i class="fa fa-check"></i><b>3.2</b> How to test for differences between samples</a><ul>
<li class="chapter" data-level="3.2.1" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#randomization-based-testing-for-difference-of-the-means"><i class="fa fa-check"></i><b>3.2.1</b> randomization based testing for difference of the means</a></li>
<li class="chapter" data-level="3.2.2" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#using-t-test-for-difference-of-the-means-between-two-samples"><i class="fa fa-check"></i><b>3.2.2</b> Using t-test for difference of the means between two samples</a></li>
<li class="chapter" data-level="3.2.3" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#multiple-testing-correction"><i class="fa fa-check"></i><b>3.2.3</b> multiple testing correction</a></li>
<li class="chapter" data-level="3.2.4" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#moderated-t-tests-using-information-from-multiple-comparisons"><i class="fa fa-check"></i><b>3.2.4</b> moderated t-tests: using information from multiple comparisons</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html"><i class="fa fa-check"></i><b>3.3</b> Relationship between variables: linear models and correlation</a><ul>
<li class="chapter" data-level="3.3.1" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#how-to-fit-a-line"><i class="fa fa-check"></i><b>3.3.1</b> How to fit a line</a></li>
<li class="chapter" data-level="3.3.2" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#how-to-estimate-the-error-of-the-coefficients"><i class="fa fa-check"></i><b>3.3.2</b> How to estimate the error of the coefficients</a></li>
<li class="chapter" data-level="3.3.3" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#accuracy-of-the-model"><i class="fa fa-check"></i><b>3.3.3</b> Accuracy of the model</a></li>
<li class="chapter" data-level="3.3.4" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#regression-with-categorical-variables"><i class="fa fa-check"></i><b>3.3.4</b> Regression with categorical variables</a></li>
<li class="chapter" data-level="3.3.5" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#regression-pitfalls"><i class="fa fa-check"></i><b>3.3.5</b> Regression pitfalls</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html"><i class="fa fa-check"></i><b>3.4</b> Clustering: grouping samples based on their similarity</a><ul>
<li class="chapter" data-level="3.4.1" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#distance-metrics"><i class="fa fa-check"></i><b>3.4.1</b> Distance metrics</a></li>
<li class="chapter" data-level="3.4.2" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#hiearchical-clustering"><i class="fa fa-check"></i><b>3.4.2</b> Hiearchical clustering</a></li>
<li class="chapter" data-level="3.4.3" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#k-means-clustering"><i class="fa fa-check"></i><b>3.4.3</b> K-means clustering</a></li>
<li class="chapter" data-level="3.4.4" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#how-to-choose-k-the-number-of-clusters"><i class="fa fa-check"></i><b>3.4.4</b> how to choose “k”, the number of clusters</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><i class="fa fa-check"></i><b>3.5</b> Dimensionality reduction techniques: visualizing complex data sets in 2D</a><ul>
<li class="chapter" data-level="3.5.1" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#principal-component-analysis"><i class="fa fa-check"></i><b>3.5.1</b> Principal component analysis</a></li>
<li class="chapter" data-level="3.5.2" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#other-dimension-reduction-techniques-using-other-matrix-factorization-methods"><i class="fa fa-check"></i><b>3.5.2</b> Other dimension reduction techniques using other matrix factorization methods</a></li>
<li class="chapter" data-level="3.5.3" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#multi-dimensional-scaling"><i class="fa fa-check"></i><b>3.5.3</b> Multi-dimensional scaling</a></li>
<li class="chapter" data-level="3.5.4" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#t-distributed-stochastic-neighbor-embedding-t-sne"><i class="fa fa-check"></i><b>3.5.4</b> t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a><ul>
<li class="chapter" data-level="3.6.1" data-path="exercises-1.html"><a href="exercises-1.html#how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1"><i class="fa fa-check"></i><b>3.6.1</b> How to summarize collection of data points: The idea behind statistical distributions</a></li>
<li class="chapter" data-level="3.6.2" data-path="exercises-1.html"><a href="exercises-1.html#how-to-test-for-differences-in-samples"><i class="fa fa-check"></i><b>3.6.2</b> How to test for differences in samples</a></li>
<li class="chapter" data-level="3.6.3" data-path="exercises-1.html"><a href="exercises-1.html#relationship-between-variables-linear-models-and-correlation-1"><i class="fa fa-check"></i><b>3.6.3</b> Relationship between variables: linear models and correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="genomicIntervals.html"><a href="genomicIntervals.html"><i class="fa fa-check"></i><b>4</b> Operations on Genomic Intervals and Genome Arithmetic</a><ul>
<li class="chapter" data-level="4.1" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html"><i class="fa fa-check"></i><b>4.1</b> Operations on Genomic Intervals with GenomicRanges package</a><ul>
<li class="chapter" data-level="4.1.1" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html#how-to-create-and-manipulate-a-granges-object"><i class="fa fa-check"></i><b>4.1.1</b> How to create and manipulate a GRanges object</a></li>
<li class="chapter" data-level="4.1.2" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html#getting-genomic-regions-into-r-as-granges-objects"><i class="fa fa-check"></i><b>4.1.2</b> Getting genomic regions into R as GRanges objects</a></li>
<li class="chapter" data-level="4.1.3" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html#finding-regions-that-dodo-not-overlap-with-another-set-of-regions"><i class="fa fa-check"></i><b>4.1.3</b> Finding regions that do/do not overlap with another set of regions</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="dealing-with-mapped-high-throughput-sequencing-reads.html"><a href="dealing-with-mapped-high-throughput-sequencing-reads.html"><i class="fa fa-check"></i><b>4.2</b> Dealing with mapped high-throughput sequencing reads</a><ul>
<li class="chapter" data-level="4.2.1" data-path="dealing-with-mapped-high-throughput-sequencing-reads.html"><a href="dealing-with-mapped-high-throughput-sequencing-reads.html#counting-mapped-reads-for-a-set-of-regions"><i class="fa fa-check"></i><b>4.2.1</b> Counting mapped reads for a set of regions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="dealing-with-continuous-scores-over-the-genome.html"><a href="dealing-with-continuous-scores-over-the-genome.html"><i class="fa fa-check"></i><b>4.3</b> Dealing with continuous scores over the genome</a><ul>
<li class="chapter" data-level="4.3.1" data-path="dealing-with-continuous-scores-over-the-genome.html"><a href="dealing-with-continuous-scores-over-the-genome.html#extracting-subsections-of-rle-and-rlelist-objects"><i class="fa fa-check"></i><b>4.3.1</b> Extracting subsections of Rle and RleList objects</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="genomic-intervals-with-more-information-summarizedexperiment-class.html"><a href="genomic-intervals-with-more-information-summarizedexperiment-class.html"><i class="fa fa-check"></i><b>4.4</b> Genomic intervals with more information: SummarizedExperiment class</a><ul>
<li class="chapter" data-level="4.4.1" data-path="genomic-intervals-with-more-information-summarizedexperiment-class.html"><a href="genomic-intervals-with-more-information-summarizedexperiment-class.html#create-a-summarizedexperiment-object"><i class="fa fa-check"></i><b>4.4.1</b> Create a SummarizedExperiment object</a></li>
<li class="chapter" data-level="4.4.2" data-path="genomic-intervals-with-more-information-summarizedexperiment-class.html"><a href="genomic-intervals-with-more-information-summarizedexperiment-class.html#subset-and-manipulate-the-summarizedexperiment-object"><i class="fa fa-check"></i><b>4.4.2</b> Subset and manipulate the SummarizedExperiment object</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html"><i class="fa fa-check"></i><b>4.5</b> Visualizing and summarizing genomic intervals</a><ul>
<li class="chapter" data-level="4.5.1" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html#visualizing-intervals-on-a-locus-of-interest"><i class="fa fa-check"></i><b>4.5.1</b> Visualizing intervals on a locus of interest</a></li>
<li class="chapter" data-level="4.5.2" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html#summaries-of-genomic-intervals-on-multiple-loci"><i class="fa fa-check"></i><b>4.5.2</b> Summaries of genomic intervals on multiple loci</a></li>
<li class="chapter" data-level="4.5.3" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html#making-karyograms-and-circos-plots"><i class="fa fa-check"></i><b>4.5.3</b> Making karyograms and circos plots</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a><ul>
<li class="chapter" data-level="4.6.1" data-path="exercises-2.html"><a href="exercises-2.html#operations-on-genomic-intervals-with-genomicranges-package-1"><i class="fa fa-check"></i><b>4.6.1</b> Operations on Genomic Intervals with GenomicRanges package</a></li>
<li class="chapter" data-level="4.6.2" data-path="exercises-2.html"><a href="exercises-2.html#dealing-with-mapped-high-throughput-sequencing-reads-1"><i class="fa fa-check"></i><b>4.6.2</b> Dealing with mapped high-throughput sequencing reads</a></li>
<li class="chapter" data-level="4.6.3" data-path="exercises-2.html"><a href="exercises-2.html#dealing-with-contiguous-scores-over-the-genome"><i class="fa fa-check"></i><b>4.6.3</b> Dealing with contiguous scores over the genome</a></li>
<li class="chapter" data-level="4.6.4" data-path="exercises-2.html"><a href="exercises-2.html#visualizing-and-summarizing-genomic-intervals-1"><i class="fa fa-check"></i><b>4.6.4</b> Visualizing and summarizing genomic intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="processingReads.html"><a href="processingReads.html"><i class="fa fa-check"></i><b>5</b> Quality check, processing and alignment of high-throughput sequencing reads</a><ul>
<li class="chapter" data-level="5.1" data-path="fasta-and-fastq-formats.html"><a href="fasta-and-fastq-formats.html"><i class="fa fa-check"></i><b>5.1</b> FASTA and FASTQ formats</a></li>
<li class="chapter" data-level="5.2" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html"><i class="fa fa-check"></i><b>5.2</b> Quality check on sequencing reads</a><ul>
<li class="chapter" data-level="5.2.1" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#sequence-quality-per-basecycle"><i class="fa fa-check"></i><b>5.2.1</b> Sequence quality per base/cycle</a></li>
<li class="chapter" data-level="5.2.2" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#sequence-content-per-basecycle"><i class="fa fa-check"></i><b>5.2.2</b> Sequence content per base/cycle</a></li>
<li class="chapter" data-level="5.2.3" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#read-frequency-plot"><i class="fa fa-check"></i><b>5.2.3</b> Read frequency plot</a></li>
<li class="chapter" data-level="5.2.4" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#other-quality-metrics-and-qc-tools"><i class="fa fa-check"></i><b>5.2.4</b> Other quality metrics and QC tools</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="filtering-and-trimming-reads.html"><a href="filtering-and-trimming-reads.html"><i class="fa fa-check"></i><b>5.3</b> Filtering and trimming reads</a></li>
<li class="chapter" data-level="5.4" data-path="mappingaligning-reads-to-the-genome.html"><a href="mappingaligning-reads-to-the-genome.html"><i class="fa fa-check"></i><b>5.4</b> Mapping/aligning reads to the genome</a></li>
<li class="chapter" data-level="5.5" data-path="further-processing-of-aligned-reads.html"><a href="further-processing-of-aligned-reads.html"><i class="fa fa-check"></i><b>5.5</b> Further processing of aligned reads</a></li>
<li class="chapter" data-level="5.6" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rna-seq-analysis-overview.html"><a href="rna-seq-analysis-overview.html"><i class="fa fa-check"></i><b>6</b> RNA-seq analysis overview</a><ul>
<li class="chapter" data-level="6.1" data-path="what-is-gene-expression.html"><a href="what-is-gene-expression.html"><i class="fa fa-check"></i><b>6.1</b> What is gene expression?</a></li>
<li class="chapter" data-level="6.2" data-path="methods-to-detect-gene-expression.html"><a href="methods-to-detect-gene-expression.html"><i class="fa fa-check"></i><b>6.2</b> Methods to detect gene expression</a></li>
<li class="chapter" data-level="6.3" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><i class="fa fa-check"></i><b>6.3</b> Gene Expression Analysis Using High-throughput Sequencing Technologies</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#processing-raw-data"><i class="fa fa-check"></i><b>6.3.1</b> Processing raw data</a></li>
<li class="chapter" data-level="6.3.2" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#alignment"><i class="fa fa-check"></i><b>6.3.2</b> Alignment</a></li>
<li class="chapter" data-level="6.3.3" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#quantification"><i class="fa fa-check"></i><b>6.3.3</b> Quantification</a></li>
<li class="chapter" data-level="6.3.4" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#normalization-of-read-counts"><i class="fa fa-check"></i><b>6.3.4</b> Normalization of read counts</a></li>
<li class="chapter" data-level="6.3.5" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#demonstration-in-r"><i class="fa fa-check"></i><b>6.3.5</b> Demonstration in R</a></li>
<li class="chapter" data-level="6.3.6" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#exploratory-analysis-of-the-read-count-table"><i class="fa fa-check"></i><b>6.3.6</b> Exploratory analysis of the read count table</a></li>
<li class="chapter" data-level="6.3.7" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#differential-expression-analysis"><i class="fa fa-check"></i><b>6.3.7</b> Differential expression analysis</a></li>
<li class="chapter" data-level="6.3.8" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#functional-enrichment-analysis"><i class="fa fa-check"></i><b>6.3.8</b> Functional Enrichment Analysis</a></li>
<li class="chapter" data-level="6.3.9" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#accounting-for-additional-sources-of-variation"><i class="fa fa-check"></i><b>6.3.9</b> Accounting for additional sources of variation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="other-applications-of-rna-seq.html"><a href="other-applications-of-rna-seq.html"><i class="fa fa-check"></i><b>6.4</b> Other applications of RNA-seq</a></li>
<li class="chapter" data-level="6.5" data-path="reproducibility.html"><a href="reproducibility.html"><i class="fa fa-check"></i><b>6.5</b> Reproducibility</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chipseq.html"><a href="chipseq.html"><i class="fa fa-check"></i><b>7</b> ChIP-seq analysis</a></li>
<li class="chapter" data-level="8" data-path="bsseq.html"><a href="bsseq.html"><i class="fa fa-check"></i><b>8</b> Bisulfite sequencing analysis</a></li>
<li class="chapter" data-level="9" data-path="multiomics.html"><a href="multiomics.html"><i class="fa fa-check"></i><b>9</b> Multi-omics analysis</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Genomics With R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d" class="section level2">
<h2><span class="header-section-number">3.5</span> Dimensionality reduction techniques: visualizing complex data sets in 2D</h2>
<p>In statistics, dimension reduction techniques are a set of processes for reducing the number of random variables by obtaining a set of principal variables. For example, in the context of a gene expression matrix accross different patient samples, this might mean getting a set of new variables that cover the variation in sets of genes. This way samples can be represented by a couple of principal variables instead of thousands of genes. This is useful for visualization, clustering and predictive modeling.</p>
<div id="principal-component-analysis" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Principal component analysis</h3>
<p>Principal component analysis (PCA) is maybe the most popular technique to examine high-dimensional data. There are multiple interpretations of how PCA reduces dimensionality. We will first focus on geometrical interpretation, where this operation can be interpreted as rotating the orignal dimensions of the data. For this, we go back to our example gene expression data set. In this example, we will represent our patients with expression profiles of just two genes, CD33 (ENSG00000105383) and PYGL (ENSG00000100504) genes. This way we can visualize them in a scatterplot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000100504&quot;</span>,],
     mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000105383&quot;</span>,],<span class="dt">pch=</span><span class="dv">19</span>,
     <span class="dt">ylab=</span><span class="st">&quot;CD33 (ENSG00000105383)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;PYGL (ENSG00000100504)&quot;</span>)</code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/unnamed-chunk-155-1.png" width="60%" style="display: block; margin: auto;" /> PCA rotates the original data space such that the axes of the new coordinate system point into the directions of highest variance of the data. The axes or new variables are termed principal components (PCs) and are ordered by variance: The first component, PC 1, represents the direction of the highest variance of the data. The direction of the second component, PC 2, represents the highest of the remaining variance orthogonal to the first component. This can be naturally extended to obtain the required number of components which together span a component space covering the desired amount of variance. In our toy example with only two genes, the principal componets are drawn over the original scatter plot and in the next plot we show the new coordinate system based on the pricinpal components. We will calculate the PCA with the <code>princomp()</code> function, this function returns the new coordinates as well. These new coordinates are simply a projection of data over the new coordinates. We will decorate the scatter plots with eigenvectors showing the direction of greatest variation. Then, we will plot the new coordinates. These are automatically calculated by <code>princomp()</code> function. Notice that we are using the <code>scale()</code> function when plotting coordinates and also before calculating PCA. This function centers the data, meaning substracts the mean of the each column vector from the elements in the vector. This essentially gives the columns a zero mean. It also divides the data by the standard deviation of the centered columns. These two operations helps bring the data to a common scale which is important for PCA not to be affected by different scales in the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))

<span class="co"># create the subset of the data with two genes only</span>
<span class="co"># notice that we transpose the matrix so samples are </span>
<span class="co"># on the columns</span>
sub.mat=<span class="kw">t</span>(mat[<span class="kw">rownames</span>(mat) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ENSG00000100504&quot;</span>,<span class="st">&quot;ENSG00000105383&quot;</span>),])

<span class="co"># ploting our genes of interest as scatter plots</span>
<span class="kw">plot</span>(<span class="kw">scale</span>(mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000100504&quot;</span>,]),
     <span class="kw">scale</span>(mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000105383&quot;</span>,]),
     <span class="dt">pch=</span><span class="dv">19</span>,
     <span class="dt">ylab=</span><span class="st">&quot;CD33 (ENSG00000105383)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;PYGL (ENSG00000100504)&quot;</span>,
     <span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,
     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>))

<span class="co"># create the legend for the Leukemia types</span>
<span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>,
       <span class="dt">legend=</span><span class="kw">unique</span>(annotation_col<span class="op">$</span>LeukemiaType),
       <span class="dt">fill =</span><span class="kw">palette</span>(<span class="st">&quot;default&quot;</span>),
       <span class="dt">border=</span><span class="ot">NA</span>,<span class="dt">box.col=</span><span class="ot">NA</span>)

<span class="co"># calculate the PCA only for our genes and all the samples</span>
pr=<span class="kw">princomp</span>(<span class="kw">scale</span>(sub.mat))


<span class="co"># plot the direction of eigenvectors</span>
<span class="co"># pr$loadings returned by princomp has the eigenvectors</span>
<span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span> pr<span class="op">$</span>loadings[<span class="dv">1</span>,<span class="dv">1</span>], 
         <span class="dt">y1 =</span> pr<span class="op">$</span>loadings[<span class="dv">2</span>,<span class="dv">1</span>],<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span> pr<span class="op">$</span>loadings[<span class="dv">1</span>,<span class="dv">2</span>], 
         <span class="dt">y1 =</span> pr<span class="op">$</span>loadings[<span class="dv">2</span>,<span class="dv">2</span>],<span class="dt">col=</span><span class="st">&quot;gray&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)


<span class="co"># plot the samples in the new coordinate system</span>
<span class="kw">plot</span>(<span class="op">-</span>pr<span class="op">$</span>scores,<span class="dt">pch=</span><span class="dv">19</span>,
     <span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,
     <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>))

<span class="co"># plot the new coordinate basis vectors</span>
<span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span><span class="op">-</span><span class="dv">2</span>, 
         <span class="dt">y1 =</span> <span class="dv">0</span>,<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span> <span class="dv">0</span>, 
         <span class="dt">y1 =</span> <span class="op">-</span><span class="dv">1</span>,<span class="dt">col=</span><span class="st">&quot;gray&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:pcaRot"></span>
<img src="compgenomrReloaded_files/figure-html/pcaRot-1.png" alt="Geometric interpretation of PCA finding eigenvectors that point to direction of highest variance. Eigenvectors can be used as a new coordinate system." width="60%" />
<p class="caption">
FIGURE 3.21: Geometric interpretation of PCA finding eigenvectors that point to direction of highest variance. Eigenvectors can be used as a new coordinate system.
</p>
</div>
<p>As you can see, the new coordinate system is useful by itself.The X-axis which represents the first component separates the data along the lympoblastic and myeloid leukemias.</p>
<p>PCA in this case is obtained by calculating eigenvectors of the covariance matrix via an operation called eigen decomposition. Covariance matrix is obtained by covariance of pairwise variables of our expression matrix, which is simply <span class="math inline">\({ \operatorname{cov} (X,Y)={\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-\mu_X)(y_{i}-\mu_Y)}\)</span>, where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> expression values of genes in a sample in our example. This is a measure of how things vary together, if high expressed genes in sample A are also highly expressed in sample B and lowly expressed in sample A are also lowly expressed in sample B, then sample A and B will have positive covariance. If the opposite is true then they will have negative covariance. This quantity is related to correlation and in fact correlation is standardized covariance. Covariance of variables can be obtained with <code>cov()</code> function, and eigen decomposition of such a matrix will produce a set of ortahogonal vectors that span the directions of highest variation. In 2D, you can think of this operation as rotating two perpendicular lines together until they point to the directions where most of the variation in the data lies on, similar to the figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:pcaRot">3.21</a>. An important intuition is that, after the rotation prescribed by eigenvectors is complete the covariance between variables in this rotated dataset will be zero. There is a proper mathematical relationship between covariances of the rotated dataset and the original dataset. That’s why operating on covariance matrix is related to the rotation of the original dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cov.mat=<span class="kw">cov</span>(sub.mat) <span class="co"># calculate covariance matrix</span>
cov.mat
<span class="kw">eigen</span>(cov.mat) <span class="co"># obtain eigen decomposition for eigen values and vectors</span></code></pre></div>
<p>Eigenvectors and eigenvalues of the covariance matrix indicates the direction and the magnitute of variation of the data. In our visual example the eigenvectors are so-called principal components. The eigenvector indicates the direction and the eigen values indicate the variation in that direction. Eigenvectors and values exist in pairs: every eigenvector has a corresponding eigenvalue and the eigenvectors are linearly independent from each other, this means they are orthogonal or uncorrelated in the our working example above. The eigenvectors are ranked by their corresponding eigen value, the higher the eigen value the more important the eigenvector is, because it explains more of the variation compared to the other eigenvectors. This feature of PCA makes the dimension reduction possible. We can sometimes display data sets that have many variables only in 2D or 3D because the these top eigenvectors are sometimes enough to capture most of variation in the data.</p>
<div id="singular-value-decomposition-and-principal-component-analysis" class="section level4">
<h4><span class="header-section-number">3.5.1.1</span> Singular value decomposition and principal component analysis</h4>
A more common way to calculate PCA is through something called singular value decomposition (SVD). This results in another interpretation of PCA, which is called “latent factor” or “latent component” interpretation. In a moment, it will be more clear what we mean by “latent factors”. SVD is a matrix factorization or decomposition algorithm that decomposes an input matrix,<span class="math inline">\(X\)</span>, to three matrices as follows: <span class="math inline">\(\displaystyle \mathrm{X} = USV^T\)</span>. In essence many matrices can be decomposed as a product of multiple matrices and we will come to other techniques later in this chapter. Singular Value Decomposition is shown in figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDcartoon">3.22</a>. <span class="math inline">\(U\)</span> is the matrix with eigenarrays on the columns and this has the same dimensions as the input matrix, you might see elsewhere the columns are named as eigenassays. <span class="math inline">\(S\)</span> is the matrix that contain the singular values on the diagonal. The singular values are also known as eigenvalues and their square is proportional to explained variation by each eigenvector. Finally, the matrix <span class="math inline">\(V^T\)</span> contains the eigenvectors on its rows. It is interpretation is still the same. Geometrically, eigenvectors point to the direction of highest variance in the data. They are uncorrolated or geometrically orthogonal to each other. These interpretations are identical to the ones we made before. The slight difference is that the decomposition seem to output <span class="math inline">\(V^T\)</span> which is just the transpose of the matrix <span class="math inline">\(V\)</span>. However, the SVD algorithms in R usually return the matrix <span class="math inline">\(V\)</span>. If you want the eigenvectors, you either simply use the columns of matrix <span class="math inline">\(V\)</span> or rows of <span class="math inline">\(V^T\)</span>.
<div class="figure" style="text-align: center"><span id="fig:SVDcartoon"></span>
<img src="images/SVDcartoon.png" alt="Singular Value Decomposition (SVD) explained in a diagram. " width="60%" />
<p class="caption">
FIGURE 3.22: Singular Value Decomposition (SVD) explained in a diagram.
</p>
</div>
<p>One thing that is new in the figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDcartoon">3.22</a> is the concept of eigenarrays. The eigenarrays or sometimes called eigenassays reprensent the sample space and can be used to plot the relationship between samples rather than genes. In this way, SVD offers additional information than the PCA using the covariance matrix. It offers us a way to summarize both genes and samples. As we can project the gene expression profiles over the top two eigengenes and get a 2D representation of genes, but with SVD we can also project the samples over the the top two eigenarrays and get a representation of samples in 2D scatterplot. Eigenvector could represent independent expression programs across samples, such as cell-cycle if we had time-based expression profiles. However, there is no guarantee that each eigenvector will be biologically meaningful. Similarly each eigenarray represent samples with specific expression characteristics. For example, the samples that have a particular pathway activated might be corrolated to an eigenarray returned by SVD.</p>
<p>Previously, in order to map samples to the reduced 2D space we had to transpose the genes-by-samples matrix when using <code>princomp()</code> function. We will now first use SVD on genes-by-samples matrix to get eigenarrays and use that to plot samples on the reduced dimensions. We will project the columns in our original expression data on eigenarrays and use the first two dimensions in the scatter plot. If you look at the code you will see that for the projection we use <span class="math inline">\(U^T X\)</span> operation, which is just <span class="math inline">\(V^T\)</span> if you follow the linear algebra. We will also perform the PCA this time with <code>prcomp()</code> function on the transposed genes-by-samples matrix to get a similar information, and plot the samples on the reduced coordinates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
d=<span class="kw">svd</span>(<span class="kw">scale</span>(mat)) <span class="co"># apply SVD</span>
assays=<span class="kw">t</span>(d<span class="op">$</span>u) <span class="op">%*%</span><span class="st"> </span><span class="kw">scale</span>(mat) <span class="co"># projection on eigenassays</span>
<span class="kw">plot</span>(assays[<span class="dv">1</span>,],assays[<span class="dv">2</span>,],<span class="dt">pch=</span><span class="dv">19</span>,
     <span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType)
<span class="co">#plot(d$v[,1],d$v[,2],pch=19,</span>
<span class="co">#     col=annotation_col$LeukemiaType)</span>
pr=<span class="kw">prcomp</span>(<span class="kw">t</span>(mat),<span class="dt">center=</span><span class="ot">TRUE</span>,<span class="dt">scale=</span><span class="ot">TRUE</span>) <span class="co"># apply PCA on transposed matrix</span>

<span class="co"># plot new coordinates from PCA, projections on eigenvectors</span>
<span class="co"># since the matrix is transposed eigenvectors represent </span>
<span class="kw">plot</span>(pr<span class="op">$</span>x[,<span class="dv">1</span>],pr<span class="op">$</span>x[,<span class="dv">2</span>],<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:svd"></span>
<img src="compgenomrReloaded_files/figure-html/svd-1.png" alt="SVD on matrix and its transpose" width="65%" />
<p class="caption">
FIGURE 3.23: SVD on matrix and its transpose
</p>
</div>
<p>As you can see in the figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:svd">3.23</a>, the two approaches yield separation of samples, although they are slightly different. The difference comes from the centering and scaling. In the first case, we scale and center columns and the second case we scale and center rows since the matrix is transposed. If we do not do any scaling or centering we would get identical plots.</p>
<div id="eigenvectors-as-latent-factorsvariables" class="section level5">
<h5><span class="header-section-number">3.5.1.1.1</span> Eigenvectors as latent factors/variables</h5>
Finally, we can introduce the latent factor interpretation of PCA via SVD. As we have already mentioned eigenvectors can also be interpreted as expression programs that are shared by several genes such as cell cycle expression program when measuring gene expression accross samples taken in different time points. In this intrepretation, linear combination of expression programs makes up the expression profile of the genes. Linear combination simply means multiplying the expression program with a weight and adding them up. Our <span class="math inline">\(USV^T\)</span> matrix multiplication can be rearranged to yield such an understanding, we can multiply eigenarrays <span class="math inline">\(U\)</span> with the diagonal eigenvalues <span class="math inline">\(S\)</span>, to produce a m-by-n weights matrix called <span class="math inline">\(W\)</span>, so <span class="math inline">\(W=US\)</span> and we can re-write the equation as just weights by eigenvectors matrix, <span class="math inline">\(X=WV^T\)</span> as shown in figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDasWeigths">3.24</a>.
<div class="figure" style="text-align: center"><span id="fig:SVDasWeigths"></span>
<img src="images/SVDasWeights.png" alt="Singular Value Decomposition (SVD) reorgonized as multiplication of m-by-n weights matrix and eigenvectors " width="70%" />
<p class="caption">
FIGURE 3.24: Singular Value Decomposition (SVD) reorgonized as multiplication of m-by-n weights matrix and eigenvectors
</p>
</div>
This simple transformation now makes it clear that indeed if eigenvectors are representing expression programs, their linear combination is making up individual gene expression profiles. As an example, we can show the liner combination of the first two eigenvectors can approximate the expression profile of an hypothetical gene in the gene expression matrix. The figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDlatentExample">3.25</a> shows eigenvector 1 and eigenvector 2 combined with certain weights in <span class="math inline">\(W\)</span> matrix can approximate gene expression pattern our example gene.
<div class="figure" style="text-align: center"><span id="fig:SVDlatentExample"></span>
<img src="images/SVDlatentExample.png" alt="Gene expression of a gene can be thought as linear combination of eigenvectors. " width="708" />
<p class="caption">
FIGURE 3.25: Gene expression of a gene can be thought as linear combination of eigenvectors.
</p>
</div>
<p>However, SVD does not care about biology. The eigenvectors are just obtained from the data with constraints of ortagonality and the direction of variation. There are examples of eigenvectors representing real expression programs but that does not mean eigenvectors will always be biologically meaningful. Sometimes combination of them might make more sense in biology than single eigenvectors. This is also the same for the other matrix factorization techniques we describe below.</p>
</div>
</div>
</div>
<div id="other-dimension-reduction-techniques-using-other-matrix-factorization-methods" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Other dimension reduction techniques using other matrix factorization methods</h3>
<p>We must mention a few other techniques that are similar to SVD in spirit. Remember we mentioned that every matrix can be decomposed to other matrices where matrix multiplication operations reconstruct the original matrix. In the case of SVD/PCA, the constraint is that eigenvectors/arrays are ortogonal, however there are other decomposition algorithms with other constraints.</p>
<div id="independent-component-analysis-ica" class="section level4">
<h4><span class="header-section-number">3.5.2.1</span> Independent component analysis (ICA)</h4>
We will first start with independent component analysis (ICA) which is an extension of PCA. ICA algorithm decomposes a given matrix <span class="math inline">\(X\)</span> as follows: <span class="math inline">\(X=SA\)</span>. The rows of <span class="math inline">\(A\)</span> could be interpreted similar to the eigengenes and columns of <span class="math inline">\(S\)</span> could be interpreted as eigenarrays, these components are sometimes called metagenes and metasamples in the literature. Traditionally, <span class="math inline">\(S\)</span> is called source matrix and <span class="math inline">\(A\)</span> is called mixing matrix. ICA is developed for a problem called “blind-source separation”. In this problem, multiple microphones record sound from multiple instruments, and the task is disentagle sounds from original instruments since each microphone is recording a combination of sounds. In this respect, the matrix <span class="math inline">\(S\)</span> contains the original signals (sounds from different instruments) and their linear combinations identified by the weights in <span class="math inline">\(A\)</span>, and the product of <span class="math inline">\(A\)</span> and <span class="math inline">\(S\)</span> makes up the matrix <span class="math inline">\(X\)</span>, which is the observed signal from different microphones. With this interpretation in mind, if the interest is strictly expression patterns similar that represent the hidden expression programs we see that genes-by-samples matrix is transposed to a samples-by-genes matrix, so that the columns of <span class="math inline">\(S\)</span> represent these expression patterns , here refered to as “metagenes”, hopefully representing distinct expression programs (Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:ICAcartoon">3.26</a> ).
<div class="figure" style="text-align: center"><span id="fig:ICAcartoon"></span>
<img src="images/ICAcartoon.png" alt="Independent Component Analysis (ICA)" width="944" />
<p class="caption">
FIGURE 3.26: Independent Component Analysis (ICA)
</p>
</div>
<p>ICA requires that the columns of <span class="math inline">\(S\)</span> matrix, the “metagenes” in our example above to be statistical independent. This is a stronger constraint than uncorrelatedness. In this case, there should be no relationship between non-linear transformation of the data either. There are different ways of ensuring this statistical indepedence and this is the main constraint when finding the optimal <span class="math inline">\(A\)</span> and <span class="math inline">\(S\)</span> matrices. The various ICA algorithms use different proxies for statistical independence, and the definition of that proxy is the main difference between many ICA algorithms. The algorithm we are going to use requires that metagenes or sources in the <span class="math inline">\(S\)</span> matrix are non-gaussian as possible. Non-gaussianity is shown to be related to statistical independence [REF]. Below, we are using <code>fastICA::fastICA()</code> function to extract 2 components and plot the rows of matrix <span class="math inline">\(A\)</span> which represents metagenes. This way, we can visualize samples in a 2D plot. If we wanted to plot the relationship between genes we would use the the columns of matrix <span class="math inline">\(S\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(fastICA)
ica.res=<span class="kw">fastICA</span>(<span class="kw">t</span>(mat),<span class="dt">n.comp=</span><span class="dv">2</span>) <span class="co"># apply ICA</span>

<span class="co"># plot reduced dimensions</span>
<span class="kw">plot</span>(ica.res<span class="op">$</span>S[,<span class="dv">1</span>],ica.res<span class="op">$</span>S[,<span class="dv">2</span>],<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType)</code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/unnamed-chunk-157-1.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="non-negative-matrix-factorization-nmf" class="section level4">
<h4><span class="header-section-number">3.5.2.2</span> Non-negative matrix factorization (NMF)</h4>
<p>Non-negative matrix factorization algorithms are series of algorithms that aim to decompose the matrix <span class="math inline">\(X\)</span> into the product or matrices <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span>, <span class="math inline">\(X=WH\)</span> (Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:NMFcartoon">3.27</a>). The constraint is that <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span> must contain non-negative values, so must <span class="math inline">\(X\)</span>. This is well suited for data sets that can not contain negative values such as gene expression. This also implies addivity of components, in our example expression of a gene across samples are addition of multiple metagenes. Unlike ICA and SVD/PCA, the metagenes can never be combined in subtractive way. In this sense, expression programs potentially captured by metagenes are combined additively.</p>
<div class="figure" style="text-align: center"><span id="fig:NMFcartoon"></span>
<img src="images/NMFcartoon.png" alt="Non-negative matrix factorization" width="768" />
<p class="caption">
FIGURE 3.27: Non-negative matrix factorization
</p>
</div>
<p>The algorithms that compute NMF tries to minimize the cost function <span class="math inline">\(D(X,WH)\)</span>, which is the distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(WH\)</span>. The early algorithms just use the euclidean distance which translates to <span class="math inline">\(\sum(X-WH)^2\)</span>, this is also known as Frobenious norm and you will see in the literature it is written as :<span class="math inline">\(\||V-WH||_{F}\)</span> However this is not the only distance metric, other distance metrics are also used in NMF algorithms. In addition, there could be other parameters to optimize that relates to sparseness of the <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span> matrices. With sparse <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span>, each entry in the <span class="math inline">\(X\)</span> matrix is expressed as the sum of a small number of components. This makes the interpretation easier, if the weights are 0 than there is not contribution from the corresponding factors.</p>
<p>Below, we are plotting the values of metagenes (rows of <span class="math inline">\(H\)</span>) for component 1 and 3. In this context, these values can also be interpreted as relationship between samples. If we wanted to plot the relationship between genes we would plot the columns of <span class="math inline">\(W\)</span> matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(NMF)
res=<span class="kw">nmf</span>(mat,<span class="dt">rank=</span><span class="dv">3</span>,<span class="dt">seed=</span><span class="dv">123</span>) <span class="co"># nmf with 3 components/factors</span>
w &lt;-<span class="st"> </span><span class="kw">basis</span>(res) <span class="co"># get W</span>
h &lt;-<span class="st"> </span><span class="kw">coef</span>(res)  <span class="co"># get H</span>

<span class="co"># plot 1st factor against 3rd factor</span>
<span class="kw">plot</span>(h[<span class="dv">1</span>,],h[<span class="dv">3</span>,],<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,<span class="dt">pch=</span><span class="dv">19</span>)</code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/nmfCode-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>We should add the note that due to random starting points of the optimization algorithm, NMF is usually run multiple times and a consensus clustering approach is used when clustering samples. This simply means that samples are clustered together if they cluster together in multiple runs of the NMF. The NMF package we used above has built-in ways to achieve this. In addition, NMF is a family of algorithms the choice of cost function to optimize the difference between <span class="math inline">\(X\)</span> and <span class="math inline">\(WH\)</span> and the methods used for optimization creates multiple variants of NMF. The “method” parameter in the above <code>nmf()</code> function controls the which algorithm for NMF.</p>
</div>
<div id="chosing-the-number-of-components-and-ranking-components-in-importance" class="section level4">
<h4><span class="header-section-number">3.5.2.3</span> chosing the number of components and ranking components in importance</h4>
<p>In both ICA and NMF, there is no well-defined way to rank components or to select the number of components. There are couple of approaches that might suit to both ICA and NMF for ranking components. One can use the norms of columns/rows in mixing matrices. This could simply mean take the sum of absolute values in mixing matrices. In our examples above, For our ICA example above, ICA we would take the sum of the absolute values of the rows of <span class="math inline">\(A\)</span> since we transposed the input matrix <span class="math inline">\(X\)</span> before ICA. And for the NMF, we would use the columns of <span class="math inline">\(W\)</span>. These ideas assume that the larger coefficients in the weight or mixing matrices indicate more important components.</p>
<p>For selecting the optimal number of components, NMF package provides different strategies. One way is to calculate RSS for each <span class="math inline">\(k\)</span>, number of components, and take the <span class="math inline">\(k\)</span> where the RSS curve starts to stabilize.However, these strategies require that you run the algorithm with multiple possible component numbers. <code>nmf</code> function will run these automatically when the <code>rank</code> argument is a vector of numbers. For ICA there is no straightforward way to choose the right number of components, a common strategy is to start with as many components as variables and try to rank them by their usefullness.</p>

<div class="rmdtip">
<p><strong>Want to know more ?</strong></p>
<p>NMF package vignette has extensive information on how to run NMF to get stable resuts and getting an estimate of components <a href="https://cran.r-project.org/web/packages/NMF/vignettes/NMF-vignette.pdf" class="uri">https://cran.r-project.org/web/packages/NMF/vignettes/NMF-vignette.pdf</a></p>
</div>

</div>
</div>
<div id="multi-dimensional-scaling" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Multi-dimensional scaling</h3>
<p>MDS is a set of data analysis techniques that display the structure of distance data in a high dimensional space into a lower dimensional space without much loss of information. The overall goal of MDS is to faithfully represent these distances with the lowest possible dimensions. So called “classical multi-dimensional scaling” algorithm, tries to minimize the following function:</p>
<p><span class="math inline">\({\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\Biggl (}{\frac {\sum _{i,j}{\bigl (}d_{ij}-\|z_{i}-z_{j}\|{\bigr )}^{2}}{\sum _{i,j}d_{ij}^{2}}}{\Biggr )}^{1/2}}\)</span></p>
<p>Here the function compares the new data points on lower dimension <span class="math inline">\((z_{1},z_{2},...,z_{N})\)</span> to the input distances between data points or distance between samples in our gene expression example. It turns out, this problem can be efficiently solved with SVD/PCA on the scaled distance matrix, the projection on eigenvectors will be the most optimal solution for the equation above. Therefore, classical MDS is sometimes called Principal Coordinates Analysis in the litereuature. However, later variants improve on classical MDS this by using this as a starting point and optimize a slightly different cost function that again measures how well the low-dimensional distances correspond to high-dimensional distances. This variant is called non-metric MDS and due to the nature of the cost function, it assumes a less stringent relationship between the low-dimensional distances $|z_{i}-z_{j}| and input distances <span class="math inline">\(d_{ij}\)</span>. Formally, this procedure tries to optimize the following function.</p>
<p><span class="math inline">\({\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\Biggl (}{\frac {\sum _{i,j}{\bigl (}\|z_{i}-z_{j}\|-\theta(d_{ij}){\bigr )}^{2}}{\sum _{i,j}\|z_{i}-z_{j}\|^{2}}}{\Biggr )}^{1/2}}\)</span></p>
<p>The core of a non-metric MDS algorithm is a twofold optimization process. First the optimal monotonic transformation of the distances has to be found, this is shown in the above formula as <span class="math inline">\(\theta(d_{ij})\)</span>. Secondly, the points on a low dimension configuration have to be optimally arranged, so that their distances match the scaled distances as closely as possible. This two steps are repeated until some convergence criteria is reached. This usually means that the cost function does not improve much after certain number of iterations. The basic steps in a non-metric MDS algorithm are: 1. Find a random low dimensional configuration of points, or in the variant we will be using below we start with the configuration returned by classical MDS 2. Calculate the distances between the points in the low dimension $|z_{i}-z_{j}|, <span class="math inline">\(z_{i}\)</span> and <span class="math inline">\(z_{j}\)</span> are vector of positions for sample <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. 3. Find the optimal monotonic transformation of the input distance, <span class="math inline">\({\textstyle \theta(d_{ij})}\)</span>, to approximate input distances to low-dimensional distances. This is achieved by isotonic regression, where a monotonically increasing free-form function is fit. This step practically ensures that ranking of low-dimensional distances are similar to rankings of input distances. 4. Minimize the stress function by re-configuring low-dimensional space and keeping <span class="math inline">\(\theta\)</span> function constant. 5. repeat from step 2 until convergence.</p>
<p>We will now demonstrate both classical MDS and Kruskal’s isometric MDS.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mds=<span class="kw">cmdscale</span>(<span class="kw">dist</span>(<span class="kw">t</span>(mat)))
isomds=MASS<span class="op">::</span><span class="kw">isoMDS</span>(<span class="kw">dist</span>(<span class="kw">t</span>(mat)))</code></pre></div>
<pre><code>## initial  value 15.907414 
## final  value 13.462986 
## converged</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the patients in the 2D space</span>
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(mds,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,
     <span class="dt">main=</span><span class="st">&quot;classical MDS&quot;</span>)
<span class="kw">plot</span>(isomds<span class="op">$</span>points,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,
     <span class="dt">main=</span><span class="st">&quot;isotonic MDS&quot;</span>)</code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/mds2-1.png" width="60%" style="display: block; margin: auto;" /> In this example, there is not much difference between isotonic MDS and classical MDS. However, there might be cases where different MDS methods provides visiable changes in the scatter plots.</p>
</div>
<div id="t-distributed-stochastic-neighbor-embedding-t-sne" class="section level3">
<h3><span class="header-section-number">3.5.4</span> t-Distributed Stochastic Neighbor Embedding (t-SNE)</h3>
<p>t-SNE maps the distances in high-dimensional space to lower dimensions and it is similar to MDS method in this respect. But the benefit of this particular method is that it tries to preserve the local structure of the data so the distances and grouping of the points we observe in a lower dimensions such as a 2D scatter plot is as close as possible to the distances we observe in the high-dimensional space. As with other dimension reduction methods, you can choose how many lower dimensions you need. The main difference of t-SNE is that it tries to preserve the local structure of the data. This kind of local structure embedding is missing in the MDS algorithm which also has a similar goal. MDS tries to optimize the distances as a whole, whereas t-SNE optimizes the distances with the local structure in mind. This is defined by the “perplexity” parameter in the arguments. This parameter controls how much the local structure influences the distance calculation. The lower the value the more the local structure is take into account. Similar to MDS, the process is an optimization algorithm. Here, we also try to minimize the divergence between observed distances and lower dimension distances. However, in the case of t-SNE, the observed distances and lower dimensional distances are transformed using a probabilistic framework with their local variance in mind.</p>
<p>From here on, we will provide a bit more detail on how the algorithm works in case conceptual description above is too shallow. In t-SNE the euclidiean distances between data points are transformed into a conditional similarity between points. This is done by assuming a normal distribution on each data point with a variance calculated ultimately by the use of “perplexity” parameter. The perplexity paramater is, in a sense, a guess about the number of the closest neighbors each point has. Setting it to higher values gives more weight to global structure. Given <span class="math inline">\(d_{ij}\)</span> is the euclidean distance between point <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, the similarity score <span class="math inline">\(p_{ij}\)</span> is calculated as shown below.</p>
<p><span class="math inline">\(p_{j | i} = \frac{\exp(-\|d_{ij}\|^2 / 2 σ_i^2)}{∑_{k \neq i} \exp(-\|d_{ik}\|^2 / 2 σ_i^2)}\)</span></p>
<p>This distance is symmetrized by incorparating $p_{i | j} as shown below.</p>
<p><span class="math inline">\(p_{i j}=\frac{p_{j|i} + p_{i|j}}{2n}\)</span></p>
<p>For the distances in the reduced dimension, we use t-distribution with one degree of freedom. In the formula below, <span class="math inline">\(| y_i-y_j\|^2\)</span> is euclidean distance between points <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in the reduced dimensions.</p>
<p><span class="math display">\[
q_{i j} = \frac{(1+ \| y_i-y_j\|^2)^{-1}}{(∑_{k \neq l} 1+ \| y_k-y_l\|^2)^{-1} }
\]</span></p>
<p>As most of the algorithms we have seen in this section, t-SNE is an optimization process in essence. In every iteration the points along lower dimensions are re-arranged to minimize the formulated difference between the the observed joint probabilities (<span class="math inline">\(p_{i j}\)</span>) and low-dimensional joint probabilities (<span class="math inline">\(q_{i j}\)</span>). Here we are trying to compare probability distributions. In this case, this is done using a method called Kullback-Leibler divergence, or KL-divergence. In the formula below, since the <span class="math inline">\(p_{i j}\)</span> is pre-defined using original distances, only way to optimize is to play with <span class="math inline">\(q_{i j}\)</span>) because it depends on the configuration of points in the lower dimensional space. This configuration is optimized to minimize the KL-divergence between <span class="math inline">\(p_{i j}\)</span> and <span class="math inline">\(q_{i j}\)</span>.</p>
<p><span class="math display">\[
KL(P||Q) = \sum_{i, j} p_{ij} \, \log \frac{p_{ij}}{q_{ij}}.
\]</span> Strictly speaking, KL-divergence measures how well the distribution <span class="math inline">\(P\)</span> which is observed using the original data points can be aproximated by distribution <span class="math inline">\(Q\)</span>, which is modeled using points on the lower dimension. If the distributions are identical KL-divergence would be 0. Naturally, the more divergent the distributions are the higher the KL-divergence will be.</p>
<p>We will now show how to use t-SNE on our gene expression data set. We are setting the random seed because again t-SNE optimization algorithm have random starting points and this might create non-identical results in every run. After calculating the t-SNE lower dimension embeddings we will plot the points in a 2D scatter plot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;Rtsne&quot;</span>)
<span class="kw">set.seed</span>(<span class="dv">42</span>) <span class="co"># Set a seed if you want reproducible results</span>
tsne_out &lt;-<span class="st"> </span><span class="kw">Rtsne</span>(<span class="kw">t</span>(mat),<span class="dt">perplexity =</span> <span class="dv">10</span>) <span class="co"># Run TSNE</span>
 <span class="co">#image(t(as.matrix(dist(tsne_out$Y))))</span>
<span class="co"># Show the objects in the 2D tsne representation</span>
<span class="kw">plot</span>(tsne_out<span class="op">$</span>Y,<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,
     <span class="dt">pch=</span><span class="dv">19</span>)

<span class="co"># create the legend for the Leukemia types</span>
<span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>,
       <span class="dt">legend=</span><span class="kw">unique</span>(annotation_col<span class="op">$</span>LeukemiaType),
       <span class="dt">fill =</span><span class="kw">palette</span>(<span class="st">&quot;default&quot;</span>),
       <span class="dt">border=</span><span class="ot">NA</span>,<span class="dt">box.col=</span><span class="ot">NA</span>)</code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/tsne-1.png" width="60%" style="display: block; margin: auto;" /> As you might have noticed, we set again a random seed with <code>set.seed()</code> function. The optimization algorithm starts with random configuration of points in the lower dimension space, and each iteration it tries to improve on the previous lower dimension confugration, that is why starting points can result in different final outcomes.</p>

<div class="rmdtip">
<p><strong>Want to know more ?</strong></p>
<ul>
<li>How perplexity effects t-sne, interactive examples <a href="https://distill.pub/2016/misread-tsne/" class="uri">https://distill.pub/2016/misread-tsne/</a></li>
<li>more on perplexity: <a href="https://blog.paperspace.com/dimension-reduction-with-t-sne/" class="uri">https://blog.paperspace.com/dimension-reduction-with-t-sne/</a></li>
<li>Intro to t-SNE <a href="https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm" class="uri">https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm</a></li>
</ul>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="clustering-grouping-samples-based-on-their-similarity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exercises-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compgenomr/book/edit/master/03-StatsForGenomics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["compgenomrReloaded.pdf"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
