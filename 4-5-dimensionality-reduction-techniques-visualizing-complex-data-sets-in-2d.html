<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Computational Genomics With R" />
<meta property="og:type" content="book" />
<meta property="og:url" content="https://compmgenomr.github.io/book/" />
<meta property="og:image" content="https://compmgenomr.github.io/book/images/cover.jpg" />
<meta property="og:description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience" />
<meta name="github-repo" content="rstudio/bookdown" />

<meta name="author" content="Altuna Akalin" />

<meta name="date" content="2019-01-22" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience">

<title>Computational Genomics With R</title>

<script src="libs/jquery/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap/js/bootstrap.min.js"></script>
<script src="libs/bootstrap/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap/shim/respond.min.js"></script>
<script src="libs/navigation/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#preface">Preface</a><ul>
<li class="has-sub"><a href="who-is-this-book-for.html#who-is-this-book-for">Who is this book for?</a><ul>
<li><a href="who-is-this-book-for.html#what-will-you-get-out-of-this">What will you get out of this?</a></li>
</ul></li>
<li><a href="structure-of-the-book.html#structure-of-the-book">Structure of the book</a></li>
<li><a href="software-information-and-conventions.html#software-information-and-conventions">Software information and conventions</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
</ul></li>
<li><a href="1-how-to-contribute.html#how-to-contribute"><span class="toc-section-number">1</span> How to contribute</a></li>
<li><a href="about-the-authors.html#about-the-authors">About the Authors</a></li>
<li class="has-sub"><a href="2-intro.html#intro"><span class="toc-section-number">2</span> Introduction to Genomics</a><ul>
<li class="has-sub"><a href="2-1-genes-dna-and-central-dogma.html#genes-dna-and-central-dogma"><span class="toc-section-number">2.1</span> Genes, DNA and central dogma</a><ul>
<li><a href="2-1-genes-dna-and-central-dogma.html#what-is-a-genome"><span class="toc-section-number">2.1.1</span> What is a genome?</a></li>
<li><a href="2-1-genes-dna-and-central-dogma.html#what-is-a-gene"><span class="toc-section-number">2.1.2</span> What is a gene?</a></li>
<li><a href="2-1-genes-dna-and-central-dogma.html#how-genes-are-controlled-the-transcriptional-and-the-post-transcriptional-regulation"><span class="toc-section-number">2.1.3</span> How genes are controlled ? The transcriptional and the post-transcriptional regulation</a></li>
<li><a href="2-1-genes-dna-and-central-dogma.html#what-does-a-gene-look-like"><span class="toc-section-number">2.1.4</span> What does a gene look like?</a></li>
</ul></li>
<li class="has-sub"><a href="2-2-elements-of-gene-regulation.html#elements-of-gene-regulation"><span class="toc-section-number">2.2</span> Elements of gene regulation</a><ul>
<li><a href="2-2-elements-of-gene-regulation.html#transcriptional-regulation"><span class="toc-section-number">2.2.1</span> Transcriptional regulation</a></li>
<li><a href="2-2-elements-of-gene-regulation.html#post-transcriptional-regulation"><span class="toc-section-number">2.2.2</span> Post-transcriptional regulation</a></li>
</ul></li>
<li><a href="2-3-shaping-the-genome-dna-mutation.html#shaping-the-genome-dna-mutation"><span class="toc-section-number">2.3</span> Shaping the genome: DNA mutation</a></li>
<li class="has-sub"><a href="2-4-high-throughput-experimental-methods-in-genomics.html#high-throughput-experimental-methods-in-genomics"><span class="toc-section-number">2.4</span> High-throughput experimental methods in genomics</a><ul>
<li><a href="2-4-high-throughput-experimental-methods-in-genomics.html#the-general-idea-behind-high-throughput-techniques"><span class="toc-section-number">2.4.1</span> The general idea behind high-throughput techniques</a></li>
<li><a href="2-4-high-throughput-experimental-methods-in-genomics.html#high-throughput-sequencing"><span class="toc-section-number">2.4.2</span> High-throughput sequencing</a></li>
</ul></li>
<li><a href="2-5-visualization-and-data-repositories-for-genomics.html#visualization-and-data-repositories-for-genomics"><span class="toc-section-number">2.5</span> Visualization and data repositories for genomics</a></li>
</ul></li>
<li class="has-sub"><a href="3-Rintro.html#Rintro"><span class="toc-section-number">3</span> Introduction to R for genomic data analysis</a><ul>
<li class="has-sub"><a href="3-1-steps-of-genomic-data-analysis.html#steps-of-genomic-data-analysis"><span class="toc-section-number">3.1</span> Steps of (genomic) data analysis</a><ul>
<li><a href="3-1-steps-of-genomic-data-analysis.html#data-collection"><span class="toc-section-number">3.1.1</span> Data collection</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#data-quality-check-and-cleaning"><span class="toc-section-number">3.1.2</span> Data quality check and cleaning</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#data-processing"><span class="toc-section-number">3.1.3</span> Data processing</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#exploratory-data-analysis-and-modeling"><span class="toc-section-number">3.1.4</span> Exploratory data analysis and modeling</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#visualization-and-reporting"><span class="toc-section-number">3.1.5</span> Visualization and reporting</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#why-use-r-for-genomics"><span class="toc-section-number">3.1.6</span> Why use R for genomics ?</a></li>
</ul></li>
<li class="has-sub"><a href="3-2-getting-started-with-r.html#getting-started-with-r"><span class="toc-section-number">3.2</span> Getting started with R</a><ul>
<li><a href="3-2-getting-started-with-r.html#installing-packages"><span class="toc-section-number">3.2.1</span> Installing packages</a></li>
<li><a href="3-2-getting-started-with-r.html#installing-packages-in-custom-locations"><span class="toc-section-number">3.2.2</span> Installing packages in custom locations</a></li>
<li><a href="3-2-getting-started-with-r.html#getting-help-on-functions-and-packages"><span class="toc-section-number">3.2.3</span> Getting help on functions and packages</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="4-stats.html#stats"><span class="toc-section-number">4</span> Statistics and Exploratory Data Analysis for Genomics</a><ul>
<li class="has-sub"><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions"><span class="toc-section-number">4.1</span> How to summarize collection of data points: The idea behind statistical distributions</a><ul>
<li><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-central-tendency-mean-and-median"><span class="toc-section-number">4.1.1</span> Describing the central tendency: mean and median</a></li>
<li><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-spread-measurements-of-variation"><span class="toc-section-number">4.1.2</span> Describing the spread: measurements of variation</a></li>
<li><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#precision-of-estimates-confidence-intervals"><span class="toc-section-number">4.1.3</span> Precision of estimates: Confidence intervals</a></li>
</ul></li>
<li class="has-sub"><a href="4-2-how-to-test-for-differences-between-samples.html#how-to-test-for-differences-between-samples"><span class="toc-section-number">4.2</span> How to test for differences between samples</a><ul>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#randomization-based-testing-for-difference-of-the-means"><span class="toc-section-number">4.2.1</span> randomization based testing for difference of the means</a></li>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#using-t-test-for-difference-of-the-means-between-two-samples"><span class="toc-section-number">4.2.2</span> Using t-test for difference of the means between two samples</a></li>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#multiple-testing-correction"><span class="toc-section-number">4.2.3</span> multiple testing correction</a></li>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#moderated-t-tests-using-information-from-multiple-comparisons"><span class="toc-section-number">4.2.4</span> moderated t-tests: using information from multiple comparisons</a></li>
</ul></li>
<li class="has-sub"><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#relationship-between-variables-linear-models-and-correlation"><span class="toc-section-number">4.3</span> Relationship between variables: linear models and correlation</a><ul>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#how-to-fit-a-line"><span class="toc-section-number">4.3.1</span> How to fit a line</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#how-to-estimate-the-error-of-the-coefficients"><span class="toc-section-number">4.3.2</span> How to estimate the error of the coefficients</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#accuracy-of-the-model"><span class="toc-section-number">4.3.3</span> Accuracy of the model</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#regression-with-categorical-variables"><span class="toc-section-number">4.3.4</span> Regression with categorical variables</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#regression-pitfalls"><span class="toc-section-number">4.3.5</span> Regression pitfalls</a></li>
</ul></li>
<li class="has-sub"><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#clustering-grouping-samples-based-on-their-similarity"><span class="toc-section-number">4.4</span> Clustering: grouping samples based on their similarity</a><ul>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#distance-metrics"><span class="toc-section-number">4.4.1</span> Distance metrics</a></li>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#hiearchical-clustering"><span class="toc-section-number">4.4.2</span> Hiearchical clustering</a></li>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#k-means-clustering"><span class="toc-section-number">4.4.3</span> K-means clustering</a></li>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#how-to-choose-k-the-number-of-clusters"><span class="toc-section-number">4.4.4</span> how to choose “k”, the number of clusters</a></li>
</ul></li>
<li class="has-sub"><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d"><span class="toc-section-number">4.5</span> Dimensionality reduction techniques: visualizing complex data sets in 2D</a><ul>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#principal-component-analysis"><span class="toc-section-number">4.5.1</span> Principal component analysis</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#other-dimension-reduction-techniques-using-other-matrix-factorization-methods"><span class="toc-section-number">4.5.2</span> Other dimension reduction techniques using other matrix factorization methods</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#multi-dimensional-scaling"><span class="toc-section-number">4.5.3</span> Multi-dimensional scaling</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#t-distributed-stochastic-neighbor-embedding-t-sne"><span class="toc-section-number">4.5.4</span> t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1"><span class="toc-section-number">4.5.5</span> How to summarize collection of data points: The idea behind statistical distributions</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#how-to-test-for-differences-in-samples"><span class="toc-section-number">4.5.6</span> How to test for differences in samples</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#relationship-between-variables-linear-models-and-correlation-1"><span class="toc-section-number">4.5.7</span> Relationship between variables: linear models and correlation</a></li>
</ul></li>
</ul></li>
<li><a href="5-genomicIntervals.html#genomicIntervals"><span class="toc-section-number">5</span> Operations on Genomic Intervals and Genome Arithmetic</a></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d" class="section level2">
<h2><span class="header-section-number">4.5</span> Dimensionality reduction techniques: visualizing complex data sets in 2D</h2>
<p>In statistics, dimension reduction techniques are a set of processes for reducing the number of random variables by obtaining a set of principal variables. For example, in the context of a gene expression matrix accross different patient samples, this might mean getting a set of new variables that cover the variation in sets of genes. This way samples can be represented by a couple of principal variables instead of thousands of genes. This is useful for visualization, clustering and predictive modeling.</p>
<div id="principal-component-analysis" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Principal component analysis</h3>
<p>Principal component analysis (PCA) is maybe the most popular technique to examine high-dimensional data. There are multiple interpretations of how PCA reduces dimensionality. We will first focus on geometrical interpretation, where this operation can be interpreted as rotating the orignal dimensions of the data. For this, we go back to our example gene expression data set. In this example, we will represent our patients with expression profiles of just two genes, CD33 (ENSG00000105383) and PYGL (ENSG00000100504) genes. This way we can visualize them in a scatterplot.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" title="1"><span class="kw">plot</span>(mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000100504&quot;</span>,],</a>
<a class="sourceLine" id="cb84-2" title="2">     mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000105383&quot;</span>,],<span class="dt">pch=</span><span class="dv">19</span>,</a>
<a class="sourceLine" id="cb84-3" title="3">     <span class="dt">ylab=</span><span class="st">&quot;CD33 (ENSG00000105383)&quot;</span>,</a>
<a class="sourceLine" id="cb84-4" title="4">     <span class="dt">xlab=</span><span class="st">&quot;PYGL (ENSG00000100504)&quot;</span>)</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/unnamed-chunk-35-1.png" width="672" style="display: block; margin: auto;" />
PCA rotates the original data space such that the axes of the new coordinate system point into the directions of highest variance of the data. The axes or new variables are termed principal components (PCs) and are ordered by variance: The first component, PC 1, represents the direction of the highest variance of the data. The direction of the second component, PC 2, represents the highest of the remaining variance orthogonal to the first component. This can be naturally extended to obtain the required number of components which together span a component space covering the desired amount of variance. In our toy example with only two genes, the principal componets are drawn over the original scatter plot and in the next plot we show the new coordinate system based on the pricinpal components. We will calculate the PCA with the <code>princomp()</code> function, this function returns the new coordinates as well. These new coordinates are simply a projection of data over the new coordinates. We will decorate the scatter plots with eigenvectors showing the direction of greatest variation. Then, we will plot the new coordinates. These are automatically calculated by <code>princomp()</code> function. Notice that we are using the <code>scale()</code> function when plotting coordinates and also before calculating PCA. This function centers the data, meaning substracts the mean of the each column vector from the elements in the vector. This essentially gives the columns a zero mean. It also divides the data by the standard deviation of the centered columns. These two operations helps bring the data to a common scale which is important for PCA not to be affected by different scales in the data.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb85-2" title="2"></a>
<a class="sourceLine" id="cb85-3" title="3"><span class="co"># create the subset of the data with two genes only</span></a>
<a class="sourceLine" id="cb85-4" title="4"><span class="co"># notice that we transpose the matrix so samples are </span></a>
<a class="sourceLine" id="cb85-5" title="5"><span class="co"># on the columns</span></a>
<a class="sourceLine" id="cb85-6" title="6">sub.mat=<span class="kw">t</span>(mat[<span class="kw">rownames</span>(mat) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ENSG00000100504&quot;</span>,<span class="st">&quot;ENSG00000105383&quot;</span>),])</a>
<a class="sourceLine" id="cb85-7" title="7"></a>
<a class="sourceLine" id="cb85-8" title="8"><span class="co"># ploting our genes of interest as scatter plots</span></a>
<a class="sourceLine" id="cb85-9" title="9"><span class="kw">plot</span>(<span class="kw">scale</span>(mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000100504&quot;</span>,]),</a>
<a class="sourceLine" id="cb85-10" title="10">     <span class="kw">scale</span>(mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000105383&quot;</span>,]),</a>
<a class="sourceLine" id="cb85-11" title="11">     <span class="dt">pch=</span><span class="dv">19</span>,</a>
<a class="sourceLine" id="cb85-12" title="12">     <span class="dt">ylab=</span><span class="st">&quot;CD33 (ENSG00000105383)&quot;</span>,</a>
<a class="sourceLine" id="cb85-13" title="13">     <span class="dt">xlab=</span><span class="st">&quot;PYGL (ENSG00000100504)&quot;</span>,</a>
<a class="sourceLine" id="cb85-14" title="14">     <span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,</a>
<a class="sourceLine" id="cb85-15" title="15">     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb85-16" title="16"></a>
<a class="sourceLine" id="cb85-17" title="17"><span class="co"># create the legend for the Leukemia types</span></a>
<a class="sourceLine" id="cb85-18" title="18"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>,</a>
<a class="sourceLine" id="cb85-19" title="19">       <span class="dt">legend=</span><span class="kw">unique</span>(annotation_col<span class="op">$</span>LeukemiaType),</a>
<a class="sourceLine" id="cb85-20" title="20">       <span class="dt">fill =</span><span class="kw">palette</span>(<span class="st">&quot;default&quot;</span>),</a>
<a class="sourceLine" id="cb85-21" title="21">       <span class="dt">border=</span><span class="ot">NA</span>,<span class="dt">box.col=</span><span class="ot">NA</span>)</a>
<a class="sourceLine" id="cb85-22" title="22"></a>
<a class="sourceLine" id="cb85-23" title="23"><span class="co"># calculate the PCA only for our genes and all the samples</span></a>
<a class="sourceLine" id="cb85-24" title="24">pr=<span class="kw">princomp</span>(<span class="kw">scale</span>(sub.mat))</a>
<a class="sourceLine" id="cb85-25" title="25"></a>
<a class="sourceLine" id="cb85-26" title="26"></a>
<a class="sourceLine" id="cb85-27" title="27"><span class="co"># plot the direction of eigenvectors</span></a>
<a class="sourceLine" id="cb85-28" title="28"><span class="co"># pr$loadings returned by princomp has the eigenvectors</span></a>
<a class="sourceLine" id="cb85-29" title="29"><span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span> pr<span class="op">$</span>loadings[<span class="dv">1</span>,<span class="dv">1</span>], </a>
<a class="sourceLine" id="cb85-30" title="30">         <span class="dt">y1 =</span> pr<span class="op">$</span>loadings[<span class="dv">2</span>,<span class="dv">1</span>],<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb85-31" title="31"><span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span> pr<span class="op">$</span>loadings[<span class="dv">1</span>,<span class="dv">2</span>], </a>
<a class="sourceLine" id="cb85-32" title="32">         <span class="dt">y1 =</span> pr<span class="op">$</span>loadings[<span class="dv">2</span>,<span class="dv">2</span>],<span class="dt">col=</span><span class="st">&quot;gray&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb85-33" title="33"></a>
<a class="sourceLine" id="cb85-34" title="34"></a>
<a class="sourceLine" id="cb85-35" title="35"><span class="co"># plot the samples in the new coordinate system</span></a>
<a class="sourceLine" id="cb85-36" title="36"><span class="kw">plot</span>(<span class="op">-</span>pr<span class="op">$</span>scores,<span class="dt">pch=</span><span class="dv">19</span>,</a>
<a class="sourceLine" id="cb85-37" title="37">     <span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,</a>
<a class="sourceLine" id="cb85-38" title="38">     <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb85-39" title="39"></a>
<a class="sourceLine" id="cb85-40" title="40"><span class="co"># plot the new coordinate basis vectors</span></a>
<a class="sourceLine" id="cb85-41" title="41"><span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span><span class="op">-</span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb85-42" title="42">         <span class="dt">y1 =</span> <span class="dv">0</span>,<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb85-43" title="43"><span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span> <span class="dv">0</span>, </a>
<a class="sourceLine" id="cb85-44" title="44">         <span class="dt">y1 =</span> <span class="dv">-1</span>,<span class="dt">col=</span><span class="st">&quot;gray&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:pcaRot"></span>
<img src="compgenomrReloaded_files/figure-html/pcaRot-1.png" alt="Geometric interpretation of PCA finding eigenvectors that point to direction of highest variance. Eigenvectors can be used as a new coordinate system." width="60%" />
<p class="caption">
FIGURE 4.21: Geometric interpretation of PCA finding eigenvectors that point to direction of highest variance. Eigenvectors can be used as a new coordinate system.
</p>
</div>
<p>As you can see, the new coordinate system is useful by itself.The X-axis which represents the first component separates the data along the lympoblastic and myeloid leukemias.</p>
<p>PCA in this case is obtained by calculating eigenvectors of the covariance matrix via an operation called eigen decomposition. Covariance matrix is obtained by covariance of pairwise variables of our expression matrix, which is simply <span class="math inline">\({ \operatorname{cov} (X,Y)={\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-\mu_X)(y_{i}-\mu_Y)}\)</span>, where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> expression values of genes in a sample in our example. This is a measure of how things vary together, if high expressed genes in sample A are also highly expressed in sample B and lowly expressed in sample A are also lowly expressed in sample B, then sample A and B will have positive covariance. If the opposite is true then they will have negative covariance. This quantity is related to correlation and in fact correlation is standardized covariance. Covariance of variables can be obtained with <code>cov()</code> function, and eigen decomposition of such a matrix will produce a set of ortahogonal vectors that span the directions of highest variation. In 2D, you can think of this operation as rotating two perpendicular lines together until they point to the directions where most of the variation in the data lies on, similar to the figure <a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:pcaRot">4.21</a>. An important intuition is that, after the rotation prescribed by eigenvectors is complete the covariance between variables in this rotated dataset will be zero. There is a proper mathematical relationship between covariances of the rotated dataset and the original dataset. That’s why operating on covariance matrix is related to the rotation of the original dataset.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" title="1">cov.mat=<span class="kw">cov</span>(sub.mat) <span class="co"># calculate covariance matrix</span></a>
<a class="sourceLine" id="cb86-2" title="2">cov.mat</a>
<a class="sourceLine" id="cb86-3" title="3"><span class="kw">eigen</span>(cov.mat) <span class="co"># obtain eigen decomposition for eigen values and vectors</span></a></code></pre></div>
<p>Eigenvectors and eigenvalues of the covariance matrix indicates the direction and the magnitute of variation of the data. In our visual example the eigenvectors are so-called principal components. The eigenvector indicates the direction and the eigen values indicate the variation in that direction. Eigenvectors and values exist in pairs: every eigenvector has a corresponding eigenvalue and the eigenvectors are linearly independent from each other, this means they are orthogonal or uncorrelated in the our working example above. The eigenvectors are ranked by their corresponding eigen value, the higher the eigen value the more important the eigenvector is, because it explains more of the variation compared to the other eigenvectors. This feature of PCA makes the dimension reduction possible. We can sometimes display data sets that have many variables only in 2D or 3D because the these top eigenvectors are sometimes enough to capture most of variation in the data.</p>
<div id="singular-value-decomposition-and-principal-component-analysis" class="section level4">
<h4><span class="header-section-number">4.5.1.1</span> Singular value decomposition and principal component analysis</h4>
A more common way to calculate PCA is through something called singular value decomposition (SVD). This results in another interpretation of PCA, which is called “latent factor” or “latent component” interpretation. In a moment, it will be more clear what we mean by “latent factors”. SVD is a matrix factorization or decomposition algorithm that decomposes an input matrix,<span class="math inline">\(X\)</span>, to three matrices as follows: <span class="math inline">\(\displaystyle \mathrm{X} = USV^T\)</span>. In essence many matrices can be decomposed as a product of multiple matrices and we will come to other techniques later in this chapter. Singular Value Decomposition is shown in figure <a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDcartoon">4.22</a>. <span class="math inline">\(U\)</span> is the matrix with eigenarrays on the columns and this has the same dimensions as the input matrix, you might see elsewhere the columns are named as eigenassays. <span class="math inline">\(S\)</span> is the matrix that contain the singular values on the diagonal. The singular values are also known as eigenvalues and their square is proportional to explained variation by each eigenvector. Finally, the matrix <span class="math inline">\(V^T\)</span> contains the eigenvectors on its rows. It is interpretation is still the same. Geometrically, eigenvectors point to the direction of highest variance in the data. They are uncorrolated or geometrically orthogonal to each other. These interpretations are identical to the ones we made before. The slight difference is that the decomposition seem to output <span class="math inline">\(V^T\)</span> which is just the transpose of the matrix <span class="math inline">\(V\)</span>. However, the SVD algorithms in R usually return the matrix <span class="math inline">\(V\)</span>. If you want the eigenvectors, you either simply use the columns of matrix <span class="math inline">\(V\)</span> or rows of <span class="math inline">\(V^T\)</span>.
<div class="figure" style="text-align: center"><span id="fig:SVDcartoon"></span>
<img src="images/SVDcartoon.png" alt="Singular Value Decomposition (SVD) explained in a diagram. " width="60%" />
<p class="caption">
FIGURE 4.22: Singular Value Decomposition (SVD) explained in a diagram.
</p>
</div>
<p>One thing that is new in the figure <a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDcartoon">4.22</a> is the concept of eigenarrays. The eigenarrays or sometimes called eigenassays reprensent the sample space and can be used to plot the relationship between samples rather than genes. In this way, SVD offers additional information than the PCA using the covariance matrix. It offers us a way to summarize both genes and samples. As we can project the gene expression profiles over the top two eigengenes and get a 2D representation of genes, but with SVD we can also project the samples over the the top two eigenarrays and get a representation of samples in 2D scatterplot. Eigenvector could represent independent expression programs across samples, such as cell-cycle if we had time-based expression profiles. However, there is no guarantee that each eigenvector will be biologically meaningful. Similarly each eigenarray represent samples with specific expression characteristics. For example, the samples that have a particular pathway activated might be corrolated to an eigenarray returned by SVD.</p>
<p>Previously, in order to map samples to the reduced 2D space we had to transpose the genes-by-samples matrix when using <code>princomp()</code> function. We will now first use SVD on genes-by-samples matrix to get eigenarrays and use that to plot samples on the reduced dimensions. We will project the columns in our original expression data on eigenarrays and use the first two dimensions in the scatter plot. If you look at the code you will see that for the projection we use <span class="math inline">\(U^T X\)</span> operation, which is just <span class="math inline">\(V^T\)</span> if you follow the linear algebra. We will also perform the PCA this time with <code>prcomp()</code> function on the transposed genes-by-samples matrix to get a similar information, and plot the samples on the reduced coordinates.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb87-2" title="2">d=<span class="kw">svd</span>(<span class="kw">scale</span>(mat)) <span class="co"># apply SVD</span></a>
<a class="sourceLine" id="cb87-3" title="3">assays=<span class="kw">t</span>(d<span class="op">$</span>u) <span class="op">%*%</span><span class="st"> </span><span class="kw">scale</span>(mat) <span class="co"># projection on eigenassays</span></a>
<a class="sourceLine" id="cb87-4" title="4"><span class="kw">plot</span>(assays[<span class="dv">1</span>,],assays[<span class="dv">2</span>,],<span class="dt">pch=</span><span class="dv">19</span>,</a>
<a class="sourceLine" id="cb87-5" title="5">     <span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType)</a>
<a class="sourceLine" id="cb87-6" title="6"><span class="co">#plot(d$v[,1],d$v[,2],pch=19,</span></a>
<a class="sourceLine" id="cb87-7" title="7"><span class="co">#     col=annotation_col$LeukemiaType)</span></a>
<a class="sourceLine" id="cb87-8" title="8">pr=<span class="kw">prcomp</span>(<span class="kw">t</span>(mat),<span class="dt">center=</span><span class="ot">TRUE</span>,<span class="dt">scale=</span><span class="ot">TRUE</span>) <span class="co"># apply PCA on transposed matrix</span></a>
<a class="sourceLine" id="cb87-9" title="9"></a>
<a class="sourceLine" id="cb87-10" title="10"><span class="co"># plot new coordinates from PCA, projections on eigenvectors</span></a>
<a class="sourceLine" id="cb87-11" title="11"><span class="co"># since the matrix is transposed eigenvectors represent </span></a>
<a class="sourceLine" id="cb87-12" title="12"><span class="kw">plot</span>(pr<span class="op">$</span>x[,<span class="dv">1</span>],pr<span class="op">$</span>x[,<span class="dv">2</span>],<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:svd"></span>
<img src="compgenomrReloaded_files/figure-html/svd-1.png" alt="SVD on matrix and its transpose" width="75%" />
<p class="caption">
FIGURE 4.23: SVD on matrix and its transpose
</p>
</div>
<p>As you can see in the figure <a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:svd">4.23</a>, the two approaches yield separation of samples, although they are slightly different. The difference comes from the centering and scaling. In the first case, we scale and center columns and the second case we scale and center rows since the matrix is transposed. If we do not do any scaling or centering we would get identical plots.</p>
<div id="eigenvectors-as-latent-factorsvariables" class="section level5">
<h5><span class="header-section-number">4.5.1.1.1</span> Eigenvectors as latent factors/variables</h5>
Finally, we can introduce the latent factor interpretation of PCA via SVD. As we have already mentioned eigenvectors can also be interpreted as expression programs that are shared by several genes such as cell cycle expression program when measuring gene expression accross samples taken in different time points. In this intrepretation, linear combination of expression programs makes up the expression profile of the genes. Linear combination simply means multiplying the expression program with a weight and adding them up. Our <span class="math inline">\(USV^T\)</span> matrix multiplication can be rearranged to yield such an understanding, we can multiply eigenarrays <span class="math inline">\(U\)</span> with the diagonal eigenvalues <span class="math inline">\(S\)</span>, to produce a m-by-n weights matrix called <span class="math inline">\(W\)</span>, so <span class="math inline">\(W=US\)</span> and we can re-write the equation as just weights by eigenvectors matrix, <span class="math inline">\(X=WV^T\)</span> as shown in figure <a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDasWeigths">4.24</a>.
<div class="figure" style="text-align: center"><span id="fig:SVDasWeigths"></span>
<img src="images/SVDasWeights.png" alt="Singular Value Decomposition (SVD) reorgonized as multiplication of m-by-n weights matrix and eigenvectors " width="70%" />
<p class="caption">
FIGURE 4.24: Singular Value Decomposition (SVD) reorgonized as multiplication of m-by-n weights matrix and eigenvectors
</p>
</div>
This simple transformation now makes it clear that indeed if eigenvectors are representing expression programs, their linear combination is making up individual gene expression profiles. As an example, we can show the liner combination of the first two eigenvectors can approximate the expression profile of an hypothetical gene in the gene expression matrix. The figure <a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDlatentExample">4.25</a> shows eigenvector 1 and eigenvector 2 combined with certain weights in <span class="math inline">\(W\)</span> matrix can approximate gene expression pattern our example gene.
<div class="figure" style="text-align: center"><span id="fig:SVDlatentExample"></span>
<img src="images/SVDlatentExample.png" alt="Gene expression of a gene can be thought as linear combination of eigenvectors. " width="708" />
<p class="caption">
FIGURE 4.25: Gene expression of a gene can be thought as linear combination of eigenvectors.
</p>
</div>
<p>However, SVD does not care about biology. The eigenvectors are just obtained from the data with constraints of ortagonality and the direction of variation. There are examples of eigenvectors representing
real expression programs but that does not mean eigenvectors will always be biologically meaningful. Sometimes combination of them might make more sense in biology than single eigenvectors. This is also the same for the other matrix factorization techniques we describe below.</p>
</div>
</div>
</div>
<div id="other-dimension-reduction-techniques-using-other-matrix-factorization-methods" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Other dimension reduction techniques using other matrix factorization methods</h3>
<p>We must mention a few other techniques that are similar to SVD in spirit. Remember we mentioned that every matrix can be decomposed to other matrices where matrix multiplication operations reconstruct the original matrix. In the case of SVD/PCA, the constraint is that eigenvectors/arrays are ortogonal, however there are other decomposition algorithms with other constraints.</p>
<div id="independent-component-analysis-ica" class="section level4">
<h4><span class="header-section-number">4.5.2.1</span> Independent component analysis (ICA)</h4>
We will first start with independent component analysis (ICA) which is an extension of PCA. ICA algorithm decomposes a given matrix <span class="math inline">\(X\)</span> as follows: <span class="math inline">\(X=SA\)</span>. The rows of <span class="math inline">\(A\)</span> could be interpreted similar to the eigengenes and columns of <span class="math inline">\(S\)</span> could be interpreted as eigenarrays, these components are sometimes called metagenes and metasamples in the literature. Traditionally, <span class="math inline">\(S\)</span> is called source matrix and <span class="math inline">\(A\)</span> is called mixing matrix. ICA is developed for a problem called “blind-source separation”. In this problem, multiple microphones record sound from multiple instruments, and the task is disentagle sounds from original instruments since each microphone is recording a combination of sounds. In this respect, the matrix <span class="math inline">\(S\)</span> contains the original signals (sounds from different instruments) and their linear combinations identified by the weights in <span class="math inline">\(A\)</span>, and the product of <span class="math inline">\(A\)</span> and <span class="math inline">\(S\)</span> makes up the matrix <span class="math inline">\(X\)</span>, which is the observed signal from different microphones. With this interpretation in mind, if the interest is strictly expression patterns similar that represent the hidden expression programs we see that genes-by-samples matrix is transposed to a samples-by-genes matrix, so that the columns of <span class="math inline">\(S\)</span> represent these expression patterns , here refered to as “metagenes”, hopefully representing distinct expression programs (Figure <a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:ICAcartoon">4.26</a> ).
<div class="figure" style="text-align: center"><span id="fig:ICAcartoon"></span>
<img src="images/ICAcartoon.png" alt="Independent Component Analysis (ICA)" width="944" />
<p class="caption">
FIGURE 4.26: Independent Component Analysis (ICA)
</p>
</div>
<p>ICA requires that the columns of <span class="math inline">\(S\)</span> matrix, the “metagenes” in our example above to be statistical independent. This is a stronger constraint than uncorrelatedness. In this case, there should be no relationship between non-linear transformation of the data either. There are different ways of ensuring this statistical indepedence and this is the main constraint when finding the optimal <span class="math inline">\(A\)</span> and <span class="math inline">\(S\)</span> matrices. The various ICA algorithms use different proxies for statistical independence, and the definition of that proxy is the main difference between many ICA algorithms. The algorithm we are going to use requires that metagenes or sources in the <span class="math inline">\(S\)</span> matrix are non-gaussian as possible. Non-gaussianity is shown to be related to statistical independence [REF]. Below, we are using <code>fastICA::fastICA()</code> function to extract 2 components and plot the rows of matrix <span class="math inline">\(A\)</span> which represents metagenes. This way, we can visualize samples in a 2D plot. If we wanted to plot the relationship between genes we would use the the columns of matrix <span class="math inline">\(S\)</span>.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" title="1"><span class="kw">library</span>(fastICA)</a>
<a class="sourceLine" id="cb88-2" title="2">ica.res=<span class="kw">fastICA</span>(<span class="kw">t</span>(mat),<span class="dt">n.comp=</span><span class="dv">2</span>) <span class="co"># apply ICA</span></a>
<a class="sourceLine" id="cb88-3" title="3"></a>
<a class="sourceLine" id="cb88-4" title="4"><span class="co"># plot reduced dimensions</span></a>
<a class="sourceLine" id="cb88-5" title="5"><span class="kw">plot</span>(ica.res<span class="op">$</span>S[,<span class="dv">1</span>],ica.res<span class="op">$</span>S[,<span class="dv">2</span>],<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType)</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/unnamed-chunk-37-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="non-negative-matrix-factorization-nmf" class="section level4">
<h4><span class="header-section-number">4.5.2.2</span> Non-negative matrix factorization (NMF)</h4>
<p>Non-negative matrix factorization algorithms are series of algorithms that aim to decompose the matrix <span class="math inline">\(X\)</span> into the product or matrices <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span>, <span class="math inline">\(X=WH\)</span> (Figure <a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:NMFcartoon">4.27</a>). The constraint is that <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span> must contain non-negative values, so must <span class="math inline">\(X\)</span>. This is well suited for data sets that can not contain negative values such as gene expression. This also implies addivity of components, in our example expression of a gene across samples are addition of multiple metagenes. Unlike ICA and SVD/PCA, the metagenes can never be combined in subtractive way. In this sense, expression programs potentially captured by metagenes are combined additively.</p>
<div class="figure" style="text-align: center"><span id="fig:NMFcartoon"></span>
<img src="images/NMFcartoon.png" alt="Non-negative matrix factorization" width="70%" />
<p class="caption">
FIGURE 4.27: Non-negative matrix factorization
</p>
</div>
<p>The algorithms that compute NMF tries to minimize the cost function <span class="math inline">\(D(X,WH)\)</span>, which is the distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(WH\)</span>. The early algorithms just use the euclidean distance which translates to <span class="math inline">\(\sum(X-WH)^2\)</span>, this is also known as Frobenious norm and you will see in the literature it is written as :<span class="math inline">\(\||V-WH||_{F}\)</span>
However this is not the only distance metric, other distance metrics are also used in NMF algorithms. In addition, there could be other parameters to optimize that relates to sparseness of the <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span> matrices. With sparse <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span>, each entry in the <span class="math inline">\(X\)</span> matrix is expressed as the sum of a small number of components. This makes the interpretation easier, if the weights are 0 than there is not contribution from the corresponding factors.</p>
<p>Below, we are plotting the values of metagenes (rows of <span class="math inline">\(H\)</span>) for component 1 and 3. In this context, these values can also be interpreted as relationship between samples. If we wanted to plot the relationship between genes we would plot the columns of <span class="math inline">\(W\)</span> matrix.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" title="1"><span class="kw">library</span>(NMF)</a>
<a class="sourceLine" id="cb89-2" title="2">res=<span class="kw">nmf</span>(mat,<span class="dt">rank=</span><span class="dv">3</span>,<span class="dt">seed=</span><span class="dv">123</span>) <span class="co"># nmf with 3 components/factors</span></a>
<a class="sourceLine" id="cb89-3" title="3">w &lt;-<span class="st"> </span><span class="kw">basis</span>(res) <span class="co"># get W</span></a>
<a class="sourceLine" id="cb89-4" title="4">h &lt;-<span class="st"> </span><span class="kw">coef</span>(res)  <span class="co"># get H</span></a>
<a class="sourceLine" id="cb89-5" title="5"></a>
<a class="sourceLine" id="cb89-6" title="6"><span class="co"># plot 1st factor against 3rd factor</span></a>
<a class="sourceLine" id="cb89-7" title="7"><span class="kw">plot</span>(h[<span class="dv">1</span>,],h[<span class="dv">3</span>,],<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,<span class="dt">pch=</span><span class="dv">19</span>)</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/nmfCode-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We should add the note that due to random starting points of the optimization algorithm, NMF is usually run multiple times and a consensus clustering approach is used when clustering samples. This simply means that samples are clustered together if they cluster together in multiple runs of the NMF. The NMF package we used above has built-in ways to achieve this. In addition, NMF is a family of algorithms the choice of cost function to optimize the difference between <span class="math inline">\(X\)</span> and <span class="math inline">\(WH\)</span> and the methods used for optimization creates multiple variants of NMF. The “method” parameter in the above <code>nmf()</code> function controls the which algorithm for NMF.</p>
</div>
<div id="chosing-the-number-of-components-and-ranking-components-in-importance" class="section level4">
<h4><span class="header-section-number">4.5.2.3</span> chosing the number of components and ranking components in importance</h4>
<p>In both ICA and NMF, there is no well-defined way to rank components or to select the number of components. There are couple of approaches that might suit to both ICA and NMF for ranking components. One can use the norms of columns/rows in mixing matrices. This could simply mean take the sum of absolute values in mixing matrices. In our examples above, For our ICA example above, ICA we would take the sum of the absolute values of the rows of <span class="math inline">\(A\)</span> since we transposed the input matrix <span class="math inline">\(X\)</span> before ICA. And for the NMF, we would use the columns of <span class="math inline">\(W\)</span>. These ideas assume that the larger coefficients in the weight or mixing matrices indicate more important components.</p>
<p>For selecting the optimal number of components, NMF package provides different strategies. One way is to calculate RSS for each <span class="math inline">\(k\)</span>, number of components, and take the <span class="math inline">\(k\)</span> where the RSS curve starts to stabilize.However, these strategies require that you run the algorithm with multiple possible component numbers. <code>nmf</code> function will run these automatically when the <code>rank</code> argument is a vector of numbers. For ICA there is no straightforward way to choose the right number of components, a common strategy is to start with as many components as variables and try to rank them by their usefullness.</p>

<div class="rmdtip">
<p><strong>Want to know more ?</strong></p>
<p>NMF package vignette has extensive information on how to run NMF to get stable resuts and getting an estimate of components <a href="https://cran.r-project.org/web/packages/NMF/vignettes/NMF-vignette.pdf" class="uri">https://cran.r-project.org/web/packages/NMF/vignettes/NMF-vignette.pdf</a></p>
</div>

</div>
</div>
<div id="multi-dimensional-scaling" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Multi-dimensional scaling</h3>
<p>MDS is a set of data analysis techniques that display the structure of distance data in a high dimensional space into a lower dimensional space without much loss of information. The overall goal of MDS is to faithfully represent these distances with the lowest possible dimensions. So called “classical multi-dimensional scaling” algorithm, tries to minimize the following function:</p>
<p><span class="math inline">\({\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\Biggl (}{\frac {\sum _{i,j}{\bigl (}d_{ij}-\|z_{i}-z_{j}\|{\bigr )}^{2}}{\sum _{i,j}d_{ij}^{2}}}{\Biggr )}^{1/2}}\)</span></p>
<p>Here the function compares the new data points on lower dimension <span class="math inline">\((z_{1},z_{2},...,z_{N})\)</span> to the input distances between data points or distance between samples in our gene expression example. It turns out, this problem can be efficiently solved with SVD/PCA on the scaled distance matrix, the projection on eigenvectors will be the most optimal solution for the equation above. Therefore, classical MDS is sometimes called Principal Coordinates Analysis in the litereuature. However, later variants improve on classical MDS this by using this as a starting point and optimize a slightly different cost function that again measures how well the low-dimensional distances correspond to high-dimensional distances. This variant is called non-metric MDS and due to the nature of the cost function, it assumes a less stringent relationship between the low-dimensional distances $|z_{i}-z_{j}| and input distances <span class="math inline">\(d_{ij}\)</span>. Formally, this procedure tries to optimize the following function.</p>
<p><span class="math inline">\({\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\Biggl (}{\frac {\sum _{i,j}{\bigl (}\|z_{i}-z_{j}\|-\theta(d_{ij}){\bigr )}^{2}}{\sum _{i,j}\|z_{i}-z_{j}\|^{2}}}{\Biggr )}^{1/2}}\)</span></p>
<p>The core of a non-metric MDS algorithm is a twofold optimization process. First the optimal monotonic transformation of the distances has to be found, this is shown in the above formula as <span class="math inline">\(\theta(d_{ij})\)</span>. Secondly, the points on a low dimension configuration have to be optimally arranged, so that their distances match the scaled distances as closely as possible. This two steps are repeated until some convergence criteria is reached. This usually means that the cost function does not improve much after certain number of iterations. The basic steps in a non-metric MDS algorithm are:
1. Find a random low dimensional configuration of points, or in the variant we will be using below we start with the configuration returned by classical MDS
2. Calculate the distances between the points in the low dimension $|z_{i}-z_{j}|, <span class="math inline">\(z_{i}\)</span> and <span class="math inline">\(z_{j}\)</span> are vector of positions for sample <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.
3. Find the optimal monotonic transformation of the input distance, <span class="math inline">\({\textstyle \theta(d_{ij})}\)</span>, to approximate input distances to low-dimensional distances. This is achieved by isotonic regression, where a monotonically increasing free-form function is fit. This step practically ensures that ranking of low-dimensional distances are similar to rankings of input distances.
4. Minimize the stress function by re-configuring low-dimensional space and keeping <span class="math inline">\(\theta\)</span> function constant.
5. repeat from step 2 until convergence.</p>
<p>We will now demonstrate both classical MDS and Kruskal’s isometric MDS.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" title="1">mds=<span class="kw">cmdscale</span>(<span class="kw">dist</span>(<span class="kw">t</span>(mat)))</a>
<a class="sourceLine" id="cb90-2" title="2">isomds=MASS<span class="op">::</span><span class="kw">isoMDS</span>(<span class="kw">dist</span>(<span class="kw">t</span>(mat)))</a></code></pre></div>
<pre><code>## initial  value 15.907414 
## final  value 13.462986 
## converged</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" title="1"><span class="co"># plot the patients in the 2D space</span></a>
<a class="sourceLine" id="cb92-2" title="2"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb92-3" title="3"><span class="kw">plot</span>(mds,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,</a>
<a class="sourceLine" id="cb92-4" title="4">     <span class="dt">main=</span><span class="st">&quot;classical MDS&quot;</span>)</a>
<a class="sourceLine" id="cb92-5" title="5"><span class="kw">plot</span>(isomds<span class="op">$</span>points,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,</a>
<a class="sourceLine" id="cb92-6" title="6">     <span class="dt">main=</span><span class="st">&quot;isotonic MDS&quot;</span>)</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/mds2-1.png" width="672" style="display: block; margin: auto;" />
In this example, there is not much difference between isotonic MDS and classical MDS. However, there might be cases where different MDS methods provides visiable changes in the scatter plots.</p>
</div>
<div id="t-distributed-stochastic-neighbor-embedding-t-sne" class="section level3">
<h3><span class="header-section-number">4.5.4</span> t-Distributed Stochastic Neighbor Embedding (t-SNE)</h3>
<p>t-SNE maps the distances in high-dimensional space to lower dimensions and it is similar to MDS method in this respect. But the benefit of this particular method is that it tries to preserve the local structure of the data so the distances and grouping of the points we observe in a lower dimensions such as a 2D scatter plot is as close as possible to the distances we observe in the high-dimensional space. As with other dimension reduction methods, you can choose how many lower dimensions you need. The main difference of t-SNE is that it tries to preserve the local structure of the data. This kind of local structure embedding is missing in the MDS algorithm which also has a similar goal. MDS tries to optimize the distances as a whole, whereas t-SNE optimizes the distances with the local structure in mind. This is defined by the “perplexity” parameter in the arguments. This parameter controls how much the local structure influences the distance calculation. The lower the value the more the local structure is take into account. Similar to MDS, the process is an optimization algorithm. Here, we also try to minimize the divergence between observed distances and lower dimension distances. However, in the case of t-SNE, the observed distances and lower dimensional distances are transformed using a probabilistic framework with their local variance in mind.</p>
<p>From here on, we will provide a bit more detail on how the algorithm works in case conceptual description above is too shallow. In t-SNE the euclidiean distances between data points are transformed into a conditional similarity between points. This is done by assuming a normal distribution on each data point with a variance calculated ultimately by the use of “perplexity” parameter. The perplexity paramater is, in a sense, a guess about the number of the closest neighbors each point has. Setting it to higher values gives more weight to global structure. Given <span class="math inline">\(d_{ij}\)</span> is the euclidean distance between point <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, the similarity score <span class="math inline">\(p_{ij}\)</span> is calculated as shown below.</p>
<p><span class="math inline">\(p_{j | i} = \frac{\exp(-\|d_{ij}\|^2 / 2 σ_i^2)}{∑_{k \neq i} \exp(-\|d_{ik}\|^2 / 2 σ_i^2)}\)</span></p>
<p>This distance is symmetrized by incorparating $p_{i | j} as shown below.</p>
<p><span class="math inline">\(p_{i j}=\frac{p_{j|i} + p_{i|j}}{2n}\)</span></p>
<p>For the distances in the reduced dimension, we use t-distribution with one degree of freedom. In the formula below, <span class="math inline">\(| y_i-y_j\|^2\)</span> is euclidean distance between points <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in the reduced dimensions.</p>
<p><span class="math display">\[
q_{i j} = \frac{(1+ \| y_i-y_j\|^2)^{-1}}{(∑_{k \neq l} 1+ \| y_k-y_l\|^2)^{-1} }
\]</span></p>
<p>As most of the algorithms we have seen in this section, t-SNE is an optimization process in essence. In every iteration the points along lower dimensions are re-arranged to minimize the formulated difference between the the observed joint probabilities (<span class="math inline">\(p_{i j}\)</span>) and low-dimensional joint probabilities (<span class="math inline">\(q_{i j}\)</span>). Here we are trying to compare probability distributions. In this case, this is done using a method called Kullback-Leibler divergence, or KL-divergence. In the formula below, since the <span class="math inline">\(p_{i j}\)</span> is pre-defined using original distances, only way to optimize is to play with <span class="math inline">\(q_{i j}\)</span>) because it depends on the configuration of points in the lower dimensional space. This configuration is optimized to minimize the KL-divergence between <span class="math inline">\(p_{i j}\)</span> and <span class="math inline">\(q_{i j}\)</span>.</p>
<p><span class="math display">\[
KL(P||Q) = \sum_{i, j} p_{ij} \, \log \frac{p_{ij}}{q_{ij}}.
\]</span>
Strictly speaking, KL-divergence measures how well the distribution <span class="math inline">\(P\)</span> which is observed using the original data points can be aproximated by distribution <span class="math inline">\(Q\)</span>, which is modeled using points on the lower dimension. If the distributions are identical KL-divergence would be 0. Naturally, the more divergent the distributions are the higher the KL-divergence will be.</p>
<p>We will now show how to use t-SNE on our gene expression data set. We are setting the random seed because again t-SNE optimization algorithm have random starting points and this might create non-identical results in every run. After calculating the t-SNE lower dimension embeddings we will plot the points in a 2D scatter plot.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" title="1"><span class="kw">library</span>(<span class="st">&quot;Rtsne&quot;</span>)</a>
<a class="sourceLine" id="cb93-2" title="2"><span class="kw">set.seed</span>(<span class="dv">42</span>) <span class="co"># Set a seed if you want reproducible results</span></a>
<a class="sourceLine" id="cb93-3" title="3">tsne_out &lt;-<span class="st"> </span><span class="kw">Rtsne</span>(<span class="kw">t</span>(mat),<span class="dt">perplexity =</span> <span class="dv">10</span>) <span class="co"># Run TSNE</span></a>
<a class="sourceLine" id="cb93-4" title="4"> <span class="kw">image</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(<span class="kw">dist</span>(tsne_out<span class="op">$</span>Y))))</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/tsne-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" title="1"><span class="co"># Show the objects in the 2D tsne representation</span></a>
<a class="sourceLine" id="cb94-2" title="2"><span class="kw">plot</span>(tsne_out<span class="op">$</span>Y,<span class="dt">col=</span>annotation_col<span class="op">$</span>LeukemiaType,</a>
<a class="sourceLine" id="cb94-3" title="3">     <span class="dt">pch=</span><span class="dv">19</span>)</a>
<a class="sourceLine" id="cb94-4" title="4"></a>
<a class="sourceLine" id="cb94-5" title="5"><span class="co"># create the legend for the Leukemia types</span></a>
<a class="sourceLine" id="cb94-6" title="6"><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>,</a>
<a class="sourceLine" id="cb94-7" title="7">       <span class="dt">legend=</span><span class="kw">unique</span>(annotation_col<span class="op">$</span>LeukemiaType),</a>
<a class="sourceLine" id="cb94-8" title="8">       <span class="dt">fill =</span><span class="kw">palette</span>(<span class="st">&quot;default&quot;</span>),</a>
<a class="sourceLine" id="cb94-9" title="9">       <span class="dt">border=</span><span class="ot">NA</span>,<span class="dt">box.col=</span><span class="ot">NA</span>)</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/tsne-2.png" width="672" style="display: block; margin: auto;" />
As you might have noticed, we set again a random seed with <code>set.seed()</code> function. The optimization algorithm starts with random configuration of points in the lower dimension space, and each iteration it tries to improve on the previous lower dimension confugration, that is why starting points can result in different final outcomes.</p>

<div class="rmdtip">
<p><strong>Want to know more ?</strong></p>
<ul>
<li>How perplexity effects t-sne, interactive examples <a href="https://distill.pub/2016/misread-tsne/" class="uri">https://distill.pub/2016/misread-tsne/</a></li>
<li>more on perplexity: <a href="https://blog.paperspace.com/dimension-reduction-with-t-sne/" class="uri">https://blog.paperspace.com/dimension-reduction-with-t-sne/</a></li>
<li>Intro to t-SNE <a href="https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm" class="uri">https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm</a></li>
</ul>
</div>

<p>##Exercises</p>
</div>
<div id="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1" class="section level3">
<h3><span class="header-section-number">4.5.5</span> How to summarize collection of data points: The idea behind statistical distributions</h3>
<div id="section" class="section level4">
<h4><span class="header-section-number">4.5.5.1</span> </h4>
<p>Calculate the means and variances
of the rows of the following simulated data set, plot the distributions
of means and variances using <code>hist()</code> and <code>boxplot()</code> functions.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" title="1"><span class="kw">set.seed</span>(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb95-2" title="2"></a>
<a class="sourceLine" id="cb95-3" title="3"><span class="co">#sample data matrix from normal distribution</span></a>
<a class="sourceLine" id="cb95-4" title="4">gset=<span class="kw">rnorm</span>(<span class="dv">600</span>,<span class="dt">mean=</span><span class="dv">200</span>,<span class="dt">sd=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb95-5" title="5">data=<span class="kw">matrix</span>(gset,<span class="dt">ncol=</span><span class="dv">6</span>)</a></code></pre></div>
</div>
<div id="section-1" class="section level4">
<h4><span class="header-section-number">4.5.5.2</span> </h4>
<p>Using the data generated above, calculate the standard deviation of the
distribution of the means using <code>sd()</code> function. Compare that to the expected
standard error obtained from central limit theorem keeping in mind the
population parameters were <span class="math inline">\(\sigma=70\)</span> and <span class="math inline">\(n=6\)</span>. How does the estimate
from the random samples change if we simulate more data with
<code>data=matrix(rnorm(6000,mean=200,sd=70),ncol=6)</code></p>
</div>
<div id="section-2" class="section level4">
<h4><span class="header-section-number">4.5.5.3</span> </h4>
<ol start="0" style="list-style-type: decimal">
<li>simulate 30 random variables using <code>rpois()</code> function, do this 1000 times and calculate means of sample. Plot the sampling distributions of the means
using a histogram. Get the 2.5th and 97.5th percentiles of the
distribution.</li>
<li>Use <code>t.test</code> function to calculate confidence intervals
of the first random sample <code>pois1</code> simulated from<code>rpois()</code> function below.</li>
<li>Use bootstrap confidence interval for the mean on <code>pois1</code></li>
<li>compare all the estimates</li>
</ol>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" title="1"><span class="kw">set.seed</span>(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb96-2" title="2"></a>
<a class="sourceLine" id="cb96-3" title="3"><span class="co">#sample 30 values from poisson dist with lamda paramater =30</span></a>
<a class="sourceLine" id="cb96-4" title="4">pois1=<span class="kw">rpois</span>(<span class="dv">30</span>,<span class="dt">lambda=</span><span class="dv">5</span>)</a></code></pre></div>
</div>
<div id="section-3" class="section level4">
<h4><span class="header-section-number">4.5.5.4</span> </h4>
<p>Optional exercise:
Try to recreate the following figure, which demonstrates the CLT concept.
<img src="compgenomrReloaded_files/figure-html/unnamed-chunk-42-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="how-to-test-for-differences-in-samples" class="section level3">
<h3><span class="header-section-number">4.5.6</span> How to test for differences in samples</h3>
<div id="section-4" class="section level4">
<h4><span class="header-section-number">4.5.6.1</span> </h4>
<p>Test the difference of means of the following simulated genes
using the randomization, t-test and <code>wilcox.test()</code> functions.
Plot the distributions using histograms and boxplots.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" title="1"><span class="kw">set.seed</span>(<span class="dv">101</span>)</a>
<a class="sourceLine" id="cb97-2" title="2">gene1=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">4</span>,<span class="dt">sd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb97-3" title="3">gene2=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">3</span>,<span class="dt">sd=</span><span class="dv">3</span>)</a></code></pre></div>
</div>
<div id="section-5" class="section level4">
<h4><span class="header-section-number">4.5.6.2</span> </h4>
<p>Test the difference of means of the following simulated genes
using the randomization, t-test and <code>wilcox.test()</code> functions.
Plot the distributions using histograms and boxplots.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" title="1"><span class="kw">set.seed</span>(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb98-2" title="2">gene1=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">4</span>,<span class="dt">sd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb98-3" title="3">gene2=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">2</span>,<span class="dt">sd=</span><span class="dv">2</span>)</a></code></pre></div>
</div>
<div id="section-6" class="section level4">
<h4><span class="header-section-number">4.5.6.3</span> </h4>
<p>read the gene expression data set with <code>data=readRDS("StatisticsForGenomics/geneExpMat.rds")</code>.
The data has 100 differentially expressed genes.First 3 columns
are the test samples, and the last 3 are the control samples. Do
a t-test for each gene (each row is a gene), record the p-values.
Then, do a moderated t-test, as shown in the lecture notes and record
the p-values. Do a p-value histogram and compare two approaches in terms
of the number of significant tests with 0.05 threshold.
On the p-values use FDR (BH), bonferroni and q-value adjustment methods.
Calculate how many adjusted p-values are below 0.05 for each approach.</p>
</div>
</div>
<div id="relationship-between-variables-linear-models-and-correlation-1" class="section level3">
<h3><span class="header-section-number">4.5.7</span> Relationship between variables: linear models and correlation</h3>
<div id="section-7" class="section level4">
<h4><span class="header-section-number">4.5.7.1</span> </h4>
<p>Below we are going to simulate X and Y values.</p>
<ol style="list-style-type: decimal">
<li>Run the code then fit a line to predict Y based on X.</li>
<li>Plot the scatter plot and the fitted line.</li>
<li>Calculate correlation and R^2.</li>
<li>Run the <code>summary()</code> function and
try to extract P-values for the model from the object
returned by <code>summary</code>. see <code>?summary.lm</code></li>
<li>Plot the residuals vs fitted values plot, by calling <code>plot</code>
function with <code>which=1</code> as the second argument. First argument
is the model returned by <code>lm</code>.</li>
</ol>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" title="1"><span class="co"># set random number seed, so that the random numbers from the text</span></a>
<a class="sourceLine" id="cb99-2" title="2"><span class="co"># is the same when you run the code.</span></a>
<a class="sourceLine" id="cb99-3" title="3"><span class="kw">set.seed</span>(<span class="dv">32</span>)</a>
<a class="sourceLine" id="cb99-4" title="4"></a>
<a class="sourceLine" id="cb99-5" title="5"><span class="co"># get 50 X values between 1 and 100</span></a>
<a class="sourceLine" id="cb99-6" title="6">x =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">50</span>,<span class="dv">1</span>,<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb99-7" title="7"></a>
<a class="sourceLine" id="cb99-8" title="8"><span class="co"># set b0,b1 and varience (sigma)</span></a>
<a class="sourceLine" id="cb99-9" title="9">b0 =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb99-10" title="10">b1 =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb99-11" title="11">sigma =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb99-12" title="12"><span class="co"># simulate error terms from normal distribution</span></a>
<a class="sourceLine" id="cb99-13" title="13">eps =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dv">0</span>,sigma)</a>
<a class="sourceLine" id="cb99-14" title="14"><span class="co"># get y values from the linear equation and addition of error terms</span></a>
<a class="sourceLine" id="cb99-15" title="15">y =<span class="st"> </span>b0 <span class="op">+</span><span class="st"> </span>b1<span class="op">*</span>x<span class="op">+</span><span class="st"> </span>eps</a></code></pre></div>
</div>
<div id="section-8" class="section level4">
<h4><span class="header-section-number">4.5.7.2</span> </h4>
<p>Read the data set histone modification data set with using a variation of:
<code>df=readRDS("StatisticsForGenomics_data/HistoneModeVSgeneExp.rds")</code>. There
are 3 columns in the data set these are measured levels of H3K4me3,
H3K27me3 and gene expression per gene.</p>
<ol style="list-style-type: decimal">
<li>plot the scatter plot for H3K4me3 vs expression</li>
<li>plot the scatter plot for H3K27me3 vs expression</li>
<li>fit the model model for prediction of expression data using:
<ul>
<li>only H3K4me3 as explanatory variable</li>
<li>only H3K27me3 as explanatory variable</li>
<li>using both H3K4me3 and H3K27me3 as explanatory variables</li>
</ul></li>
<li>inspect summary() function output in each case, which terms are significant</li>
<li>Is using H3K4me3 and H3K27me3 better than the model with only H3K4me3.</li>
<li>Plot H3k4me3 vs H3k27me3. Inspect the points that does not
follow a linear trend. Are they clustered at certain segments
of the plot. Bonus: Is there any biological or technical interpretation
for those points ?</li>
</ol>

</div>
</div>
</div>
<!-- </div> -->
<p style="text-align: center;">
<a href="4-4-clustering-grouping-samples-based-on-their-similarity.html"><button class="btn btn-default">Previous</button></a>
<a href="5-genomicIntervals.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
