<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Computational Genomics With R" />
<meta property="og:type" content="book" />
<meta property="og:url" content="https://compmgenomr.github.io/book/" />
<meta property="og:image" content="https://compmgenomr.github.io/book/images/cover.jpg" />
<meta property="og:description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience" />
<meta name="github-repo" content="rstudio/bookdown" />

<meta name="author" content="Altuna Akalin" />

<meta name="date" content="2019-01-22" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience">

<title>Computational Genomics With R</title>

<script src="libs/jquery/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap/js/bootstrap.min.js"></script>
<script src="libs/bootstrap/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap/shim/respond.min.js"></script>
<script src="libs/navigation/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#preface">Preface</a><ul>
<li class="has-sub"><a href="who-is-this-book-for.html#who-is-this-book-for">Who is this book for?</a><ul>
<li><a href="who-is-this-book-for.html#what-will-you-get-out-of-this">What will you get out of this?</a></li>
</ul></li>
<li><a href="structure-of-the-book.html#structure-of-the-book">Structure of the book</a></li>
<li><a href="software-information-and-conventions.html#software-information-and-conventions">Software information and conventions</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
</ul></li>
<li><a href="1-how-to-contribute.html#how-to-contribute"><span class="toc-section-number">1</span> How to contribute</a></li>
<li><a href="about-the-authors.html#about-the-authors">About the Authors</a></li>
<li class="has-sub"><a href="2-intro.html#intro"><span class="toc-section-number">2</span> Introduction to Genomics</a><ul>
<li class="has-sub"><a href="2-1-genes-dna-and-central-dogma.html#genes-dna-and-central-dogma"><span class="toc-section-number">2.1</span> Genes, DNA and central dogma</a><ul>
<li><a href="2-1-genes-dna-and-central-dogma.html#what-is-a-genome"><span class="toc-section-number">2.1.1</span> What is a genome?</a></li>
<li><a href="2-1-genes-dna-and-central-dogma.html#what-is-a-gene"><span class="toc-section-number">2.1.2</span> What is a gene?</a></li>
<li><a href="2-1-genes-dna-and-central-dogma.html#how-genes-are-controlled-the-transcriptional-and-the-post-transcriptional-regulation"><span class="toc-section-number">2.1.3</span> How genes are controlled ? The transcriptional and the post-transcriptional regulation</a></li>
<li><a href="2-1-genes-dna-and-central-dogma.html#what-does-a-gene-look-like"><span class="toc-section-number">2.1.4</span> What does a gene look like?</a></li>
</ul></li>
<li class="has-sub"><a href="2-2-elements-of-gene-regulation.html#elements-of-gene-regulation"><span class="toc-section-number">2.2</span> Elements of gene regulation</a><ul>
<li><a href="2-2-elements-of-gene-regulation.html#transcriptional-regulation"><span class="toc-section-number">2.2.1</span> Transcriptional regulation</a></li>
<li><a href="2-2-elements-of-gene-regulation.html#post-transcriptional-regulation"><span class="toc-section-number">2.2.2</span> Post-transcriptional regulation</a></li>
</ul></li>
<li><a href="2-3-shaping-the-genome-dna-mutation.html#shaping-the-genome-dna-mutation"><span class="toc-section-number">2.3</span> Shaping the genome: DNA mutation</a></li>
<li class="has-sub"><a href="2-4-high-throughput-experimental-methods-in-genomics.html#high-throughput-experimental-methods-in-genomics"><span class="toc-section-number">2.4</span> High-throughput experimental methods in genomics</a><ul>
<li><a href="2-4-high-throughput-experimental-methods-in-genomics.html#the-general-idea-behind-high-throughput-techniques"><span class="toc-section-number">2.4.1</span> The general idea behind high-throughput techniques</a></li>
<li><a href="2-4-high-throughput-experimental-methods-in-genomics.html#high-throughput-sequencing"><span class="toc-section-number">2.4.2</span> High-throughput sequencing</a></li>
</ul></li>
<li><a href="2-5-visualization-and-data-repositories-for-genomics.html#visualization-and-data-repositories-for-genomics"><span class="toc-section-number">2.5</span> Visualization and data repositories for genomics</a></li>
</ul></li>
<li class="has-sub"><a href="3-Rintro.html#Rintro"><span class="toc-section-number">3</span> Introduction to R for genomic data analysis</a><ul>
<li class="has-sub"><a href="3-1-steps-of-genomic-data-analysis.html#steps-of-genomic-data-analysis"><span class="toc-section-number">3.1</span> Steps of (genomic) data analysis</a><ul>
<li><a href="3-1-steps-of-genomic-data-analysis.html#data-collection"><span class="toc-section-number">3.1.1</span> Data collection</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#data-quality-check-and-cleaning"><span class="toc-section-number">3.1.2</span> Data quality check and cleaning</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#data-processing"><span class="toc-section-number">3.1.3</span> Data processing</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#exploratory-data-analysis-and-modeling"><span class="toc-section-number">3.1.4</span> Exploratory data analysis and modeling</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#visualization-and-reporting"><span class="toc-section-number">3.1.5</span> Visualization and reporting</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#why-use-r-for-genomics"><span class="toc-section-number">3.1.6</span> Why use R for genomics ?</a></li>
</ul></li>
<li class="has-sub"><a href="3-2-getting-started-with-r.html#getting-started-with-r"><span class="toc-section-number">3.2</span> Getting started with R</a><ul>
<li><a href="3-2-getting-started-with-r.html#installing-packages"><span class="toc-section-number">3.2.1</span> Installing packages</a></li>
<li><a href="3-2-getting-started-with-r.html#installing-packages-in-custom-locations"><span class="toc-section-number">3.2.2</span> Installing packages in custom locations</a></li>
<li><a href="3-2-getting-started-with-r.html#getting-help-on-functions-and-packages"><span class="toc-section-number">3.2.3</span> Getting help on functions and packages</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="4-stats.html#stats"><span class="toc-section-number">4</span> Statistics and Exploratory Data Analysis for Genomics</a><ul>
<li class="has-sub"><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions"><span class="toc-section-number">4.1</span> How to summarize collection of data points: The idea behind statistical distributions</a><ul>
<li><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-central-tendency-mean-and-median"><span class="toc-section-number">4.1.1</span> Describing the central tendency: mean and median</a></li>
<li><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-spread-measurements-of-variation"><span class="toc-section-number">4.1.2</span> Describing the spread: measurements of variation</a></li>
<li><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#precision-of-estimates-confidence-intervals"><span class="toc-section-number">4.1.3</span> Precision of estimates: Confidence intervals</a></li>
</ul></li>
<li class="has-sub"><a href="4-2-how-to-test-for-differences-between-samples.html#how-to-test-for-differences-between-samples"><span class="toc-section-number">4.2</span> How to test for differences between samples</a><ul>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#randomization-based-testing-for-difference-of-the-means"><span class="toc-section-number">4.2.1</span> randomization based testing for difference of the means</a></li>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#using-t-test-for-difference-of-the-means-between-two-samples"><span class="toc-section-number">4.2.2</span> Using t-test for difference of the means between two samples</a></li>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#multiple-testing-correction"><span class="toc-section-number">4.2.3</span> multiple testing correction</a></li>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#moderated-t-tests-using-information-from-multiple-comparisons"><span class="toc-section-number">4.2.4</span> moderated t-tests: using information from multiple comparisons</a></li>
</ul></li>
<li class="has-sub"><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#relationship-between-variables-linear-models-and-correlation"><span class="toc-section-number">4.3</span> Relationship between variables: linear models and correlation</a><ul>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#how-to-fit-a-line"><span class="toc-section-number">4.3.1</span> How to fit a line</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#how-to-estimate-the-error-of-the-coefficients"><span class="toc-section-number">4.3.2</span> How to estimate the error of the coefficients</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#accuracy-of-the-model"><span class="toc-section-number">4.3.3</span> Accuracy of the model</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#regression-with-categorical-variables"><span class="toc-section-number">4.3.4</span> Regression with categorical variables</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#regression-pitfalls"><span class="toc-section-number">4.3.5</span> Regression pitfalls</a></li>
</ul></li>
<li class="has-sub"><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#clustering-grouping-samples-based-on-their-similarity"><span class="toc-section-number">4.4</span> Clustering: grouping samples based on their similarity</a><ul>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#distance-metrics"><span class="toc-section-number">4.4.1</span> Distance metrics</a></li>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#hiearchical-clustering"><span class="toc-section-number">4.4.2</span> Hiearchical clustering</a></li>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#k-means-clustering"><span class="toc-section-number">4.4.3</span> K-means clustering</a></li>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#how-to-choose-k-the-number-of-clusters"><span class="toc-section-number">4.4.4</span> how to choose “k”, the number of clusters</a></li>
</ul></li>
<li class="has-sub"><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d"><span class="toc-section-number">4.5</span> Dimensionality reduction techniques: visualizing complex data sets in 2D</a><ul>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#principal-component-analysis"><span class="toc-section-number">4.5.1</span> Principal component analysis</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#other-dimension-reduction-techniques-using-other-matrix-factorization-methods"><span class="toc-section-number">4.5.2</span> Other dimension reduction techniques using other matrix factorization methods</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#multi-dimensional-scaling"><span class="toc-section-number">4.5.3</span> Multi-dimensional scaling</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#t-distributed-stochastic-neighbor-embedding-t-sne"><span class="toc-section-number">4.5.4</span> t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1"><span class="toc-section-number">4.5.5</span> How to summarize collection of data points: The idea behind statistical distributions</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#how-to-test-for-differences-in-samples"><span class="toc-section-number">4.5.6</span> How to test for differences in samples</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#relationship-between-variables-linear-models-and-correlation-1"><span class="toc-section-number">4.5.7</span> Relationship between variables: linear models and correlation</a></li>
</ul></li>
</ul></li>
<li><a href="5-genomicIntervals.html#genomicIntervals"><span class="toc-section-number">5</span> Operations on Genomic Intervals and Genome Arithmetic</a></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="relationship-between-variables-linear-models-and-correlation" class="section level2">
<h2><span class="header-section-number">4.3</span> Relationship between variables: linear models and correlation</h2>
<p>In genomics, we would often need to measure or model the relationship between
variables. We might want to know about expression of a particular gene in liver
in relation to the dosage of a drug that patient receives. Or, we may want to know
DNA methylation of certain locus in the genome in relation to age of the sample
donor’s. Or, we might be interested in the relationship between histone
modifications and gene expression. Is there a linear relationship, the more
histone modification the more the gene is expressed ?</p>
<p>In these
situations and many more, linear regression or linear models can be used to
model the relationship with a “dependent” or “response” variable (expression or
methylation
in the above examples) and one or more “independent”" or “explanatory” variables (age, drug dosage or histone modification in the above examples). Our simple linear model has the
following components.</p>
<p><span class="math display">\[  Y= \beta_0+\beta_1X + \epsilon \]</span></p>
<p>In the equation above, <span class="math inline">\(Y\)</span> is the response variable and <span class="math inline">\(X\)</span> is the explanatory
variable. <span class="math inline">\(\epsilon\)</span> is the mean-zero error term. Since, the line fit will not
be able to precisely predict the <span class="math inline">\(Y\)</span> values, there will be some error associated
with each prediction when we compare it to the original <span class="math inline">\(Y\)</span> values. This error
is captured in <span class="math inline">\(\epsilon\)</span> term. We can alternatively write the model as
follows to emphasize that the model approximates <span class="math inline">\(Y\)</span>, in this case notice that we removed the <span class="math inline">\(\epsilon\)</span> term: <span class="math inline">\(Y \sim \beta_0+\beta_1X\)</span></p>
<p>The graph below shows the relationship between
histone modification (trimethylated forms of histone H3 at lysine 4, aka H3K4me3)
and gene expression for 100 genes. The blue line is our model with estimated
coefficients (<span class="math inline">\(\hat{y}=\hat{\beta}_0 + \hat{\beta}_1X\)</span>, where <span class="math inline">\(\hat{\beta}_0\)</span>
and <span class="math inline">\(\hat{\beta}_1\)</span> the estimated values of <span class="math inline">\(\beta_0\)</span> and
<span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\hat{y}\)</span> indicates the prediction). The red lines indicate the individual
errors per data point, indicated as <span class="math inline">\(\epsilon\)</span> in the formula above.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-23"></span>
<img src="compgenomrReloaded_files/figure-html/unnamed-chunk-23-1.png" alt="Relationship between histone modification score and gene expression. Increasing histone modification, H3K4me3, seems to be associated with increasing gene expression. Each dot is a gene" width="60%" />
<p class="caption">
FIGURE 4.12: Relationship between histone modification score and gene expression. Increasing histone modification, H3K4me3, seems to be associated with increasing gene expression. Each dot is a gene
</p>
</div>
<p>There could be more than one explanatory variable, we then simply add more <span class="math inline">\(X\)</span>
and <span class="math inline">\(\beta\)</span> to our model. If there are two explanatory variables our model
will look like this:</p>
<p><span class="math display">\[  Y= \beta_0+\beta_1X_1 +\beta_2X_2 + \epsilon \]</span></p>
<p>In this case, we will be fitting a plane rather than a line. However, the fitting
process which we will describe in the later sections will not change. For our
gene expression problem. We can introduce one more histone modification, H3K27me3. We will then have a linear model with 2 explanatory variables and the
fitted plane will look like the one below. The gene expression values are shown
as dots below and above the fitted plane.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-24"></span>
<img src="compgenomrReloaded_files/figure-html/unnamed-chunk-24-1.png" alt="Association of Gene expression with H3K4me3 and H27Kme3 histone modifications." width="70%" />
<p class="caption">
FIGURE 4.13: Association of Gene expression with H3K4me3 and H27Kme3 histone modifications.
</p>
</div>
<div id="matrix-notation-for-linear-models" class="section level4">
<h4><span class="header-section-number">4.3.0.1</span> Matrix notation for linear models</h4>
<p>We can naturally have more explanatory variables than just two.The formula
below has <span class="math inline">\(n\)</span> explanatory variables.</p>
<p><span class="math display">\[Y= \beta_0+\beta_1X_1+\beta_2X_2 +  \beta_3X_3 + .. + \beta_nX_n +\epsilon\]</span></p>
<p>If there are many variables, it would be easier
to write the model in matrix notation. The matrix form of linear model with
two explanatory variables will look like the one
below. First matrix would be our data matrix. This contains our explanatory
variables and a column of 1s. The second term is a column vector of <span class="math inline">\(\beta\)</span>
values. We add a vector of error terms,<span class="math inline">\(\epsilon\)</span>s to the matrix multiplication.</p>
<p><span class="math display">\[
 \mathbf{Y} = \left[\begin{array}{rrr}
1 &amp; X_{1,1} &amp; X_{1,2} \\
1 &amp; X_{2,1} &amp; X_{2,2} \\
1 &amp; X_{3,1} &amp; X_{3,2} \\
1 &amp; X_{4,1} &amp; X_{4,2}
\end{array}\right]
%
\left[\begin{array}{rrr}
\beta_0 \\
\beta_1 \\
\beta_2 
\end{array}\right]
% 
+
\left[\begin{array}{rrr}
\epsilon_1 \\
\epsilon_2 \\ 
\epsilon_3 \\ 
\epsilon_0
\end{array}\right]
\]</span></p>
<p>The multiplication of data matrix and <span class="math inline">\(\beta\)</span> vector and addition of the
error terms simply results in the the following set of equations per data point:</p>
<p><span class="math display">\[
\begin{aligned}
Y_1= \beta_0+\beta_1X_{1,1}+\beta_2X_{1,2} +\epsilon_1 \\
Y_2= \beta_0+\beta_1X_{2,1}+\beta_2X_{2,2} +\epsilon_2 \\
Y_3= \beta_0+\beta_1X_{3,1}+\beta_2X_{3,2} +\epsilon_3 \\
Y_4= \beta_0+\beta_1X_{4,1}+\beta_2X_{4,2} +\epsilon_4 
\end{aligned}
\]</span></p>
<p>This expression involving the multiplication of the data matrix, the
<span class="math inline">\(\beta\)</span> vector and vector of error terms (<span class="math inline">\(\epsilon\)</span>)
could be simply written as follows.</p>
<p><span class="math display">\[Y=X\beta + \epsilon\]</span></p>
<p>In the equation above <span class="math inline">\(Y\)</span> is the vector of response variables and <span class="math inline">\(X\)</span> is the
data matrix and <span class="math inline">\(\beta\)</span> is the vector of coefficients.
This notation is more concise and often used in scientific papers. However, this
also means you need some understanding of linear algebra to follow the math
laid out in such resources.</p>
</div>
<div id="how-to-fit-a-line" class="section level3">
<h3><span class="header-section-number">4.3.1</span> How to fit a line</h3>
<p>At this point a major questions is left unanswered: How did we fit this line?
We basically need to define <span class="math inline">\(\beta\)</span> values in a structured way.
There are multiple ways or understanding how
to do this, all of which converges to the same
end point. We will describe them one by one.</p>
<div id="the-cost-or-loss-function-approach" class="section level4">
<h4><span class="header-section-number">4.3.1.1</span> The cost or loss function approach</h4>
<p>This is the first approach and in my opinion is easiest to understand.
We try to optimize a function, often called “cost function” or “loss function”.
The cost function
is the sum of squared differences between the predicted <span class="math inline">\(\hat{Y}\)</span> values from our model
and the original <span class="math inline">\(Y\)</span> values. The optimization procedure tries to find <span class="math inline">\(\beta\)</span> values
that minimizes this difference between reality and the predicted values.</p>
<p><span class="math display">\[min \sum{(y_i-(\beta_0+\beta_1x_i))^2}\]</span></p>
<p>Note that this is related to the the error term, <span class="math inline">\(\epsilon\)</span>, we already mentioned
above, we are trying to minimize the squared sum of <span class="math inline">\(\epsilon_i\)</span> for each data
point. We can do this minimization by a bit of calculus.
The rough algorithm is as follows:</p>
<ol style="list-style-type: decimal">
<li>Pick a random starting point, random <span class="math inline">\(\beta\)</span> values</li>
<li>Take the partial derivatives of the cost function to see which direction is
the way to go in the cost function.</li>
<li>Take a step toward the direction that minimizes the cost function.
<ul>
<li>step size is parameter to choose, there are many variants.</li>
</ul></li>
<li>repeat step 2,3 until convergence.</li>
</ol>
<p>This is the basis of “gradient descent” algorithm. With the help of partial
derivatives we define a “gradient” on the cost function and follow that through
multiple iterations and until convergence, meaning until the results do not
improve defined by a margin. The algorithm usually converges to optimum <span class="math inline">\(\beta\)</span>
values. Below, we show the cost function over various <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>
values for the histone modification and gene expression data set. The algorithm
will pick a point on this graph and traverse it incrementally based on the
derivatives and converge on the bottom of the cost function “well”.</p>
<div class="figure" style="text-align: center"><span id="fig:3dcostfunc"></span>
<img src="compgenomrReloaded_files/figure-html/3dcostfunc-1.png" alt="Cost function landscape for linear regression with changing beta values. The optimization process tries to find the lowest point in this landscape by implementing a strategy for updating beta values towards the lowest point in the landscape." width="672" />
<p class="caption">
FIGURE 4.14: Cost function landscape for linear regression with changing beta values. The optimization process tries to find the lowest point in this landscape by implementing a strategy for updating beta values towards the lowest point in the landscape.
</p>
</div>
</div>
<div id="not-cost-function-but-maximum-likelihood-function" class="section level4">
<h4><span class="header-section-number">4.3.1.2</span> Not cost function but maximum likelihood function</h4>
<p>We can also think of this problem from more a statistical point of view. In
essence, we are looking for best statistical parameters, in this
case <span class="math inline">\(\beta\)</span> values, for our model that are most likely to produce such a
scatter of data points given the explanatory variables.This is called
“Maximum likelihood” approach. Probability of
observing a <span class="math inline">\(Y\)</span> value, given that the distribution of it on a given <span class="math inline">\(X\)</span>
value follows a normal
distribution with mean <span class="math inline">\(\beta_0+\beta_1x_i\)</span> and variance <span class="math inline">\(s^2\)</span> , and is shown below. Note that this assumes variance is constant and <span class="math inline">\(s^2=\frac{\sum{\epsilon_i}}{n-2}\)</span> is an unbiased estimation for population variance, <span class="math inline">\(\sigma^2\)</span>.</p>
<p><span class="math display">\[P(y_{i})=\frac{1}{s\sqrt{2\pi} }e^{-\frac{1}{2}\left(\frac{y_i-(\beta_0 + \beta_1x_i)}{s}\right)^2}\]</span></p>
<p>Following from this, then the likelihood function ,shown as <span class="math inline">\(L\)</span> below, for
linear regression is multiplication of <span class="math inline">\(P(y_{i})\)</span> for all data points.</p>
<p><span class="math display">\[L=P(y_1)P(y_2)P(y_3)..P(y_n)=\prod\limits_{i=1}^n{P_i}\]</span></p>
<p>This can be simplified to this by some algebra and taking logs (since it is
easier to add than multiply)</p>
<p><span class="math display">\[ln(L) = -nln(s\sqrt{2\pi}) - \frac{1}{2s^2} \sum\limits_{i=1}^n{(y_i-(\beta_0 - \beta_1x_i))^2} \]</span></p>
<p>As you can see, the right part of the function is the negative of the cost function
defined above. If we wanted to optimize this function we would need to take derivative of
the function with respect to <span class="math inline">\(\beta\)</span> parameters. That means we can ignore the
first part since there is no <span class="math inline">\(\beta\)</span> terms there. This simply reduces to the
negative of the cost function. Hence, this approach produces exactly the same
result as the cost function approach. The difference is that we defined our
problem
within the domain of statistics. This particular function has still to be optimized. This can be done with some calculus without the need for an
iterative approach.</p>
</div>
<div id="linear-algebra-and-closed-form-solution-to-linear-regression" class="section level4">
<h4><span class="header-section-number">4.3.1.3</span> Linear algebra and closed-form solution to linear regression</h4>
<p>The last approach we will describe is the minimization process using linear
algebra. If you find this concept challenging, feel free to skip it but scientific publications and other books frequently use matrix notation and linear algebra to define and solve regression problems. In this case, we do not use an iterative approach. Instead, we will
minimize cost function by explicitly taking its derivatives with respect to
<span class="math inline">\(\beta\)</span>’s and setting them to zero. This is doable by employing linear algebra
and matrix calculus. This approach is also called “ordinary least squares”. We
will not
show the whole derivation here but the following expression
is what we are trying to minimize in matrix notation, this is basically a
different notation of the same minimization problem defined above. Remember
<span class="math inline">\(\epsilon_i=Y_i-(\beta_0+\beta_1x_i)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\sum\epsilon_{i}^2=\epsilon^T\epsilon=(Y-{\beta}{X})^T(Y-{\beta}{X}) \\
=Y^T{Y}-2{\beta}^T{Y}+{\beta}^TX^TX{\beta}
\end{aligned}
\]</span>
After rearranging the terms, we take the derivative of <span class="math inline">\(\epsilon^T\epsilon\)</span>
with respect to <span class="math inline">\(\beta\)</span>, and equalize that to zero. We then arrive at
the following for estimated <span class="math inline">\(\beta\)</span> values, <span class="math inline">\(\hat{\beta}\)</span>:</p>
<p><span class="math display">\[\hat{\beta}=(X^TX)^{-1}X^TY\]</span></p>
<p>This requires for you to calculate the inverse of the <span class="math inline">\(X^TX\)</span> term, which could
be slow for large matrices. Iterative approach over the cost function
derivatives will be faster for larger problems.
The linear algebra notation is something you will see in the papers
or other resources often. If you input the data matrix X and solve the <span class="math inline">\((X^TX)^{-1}\)</span>
,
you get the following values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> for simple regression
. However, we should note that this simple linear regression case can easily
be solved algebraically without the need for matrix operations. This can be done
by taking the derivative of <span class="math inline">\(\sum{(y_i-(\beta_0+\beta_1x_i))^2}\)</span> with respect to
<span class="math inline">\(\beta_1\)</span>, rearranging the terms and equalizing the derivative to zero.</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum{(x_i-\overline{X})(y_i-\overline{Y})}}{ \sum{(x_i-\overline{X})^2} }\]</span>
<span class="math display">\[\hat{\beta_0}=\overline{Y}-\hat{\beta_1}\overline{X}\]</span></p>
</div>
<div id="fitting-lines-in-r" class="section level4">
<h4><span class="header-section-number">4.3.1.4</span> Fitting lines in R</h4>
<p>After all this theory, you will be surprised how easy it is to fit lines in R.
This is achieved just by <code>lm()</code> command, stands for linear models. Let’s do this
for a simulated data set and plot the fit. First step is to simulate the
data, we will decide on <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> values. The we will decide
on the variance parameter,<span class="math inline">\(\sigma\)</span> to be used in simulation of error terms,
<span class="math inline">\(\epsilon\)</span>. We will first find <span class="math inline">\(Y\)</span> values, just using the linear equation
<span class="math inline">\(Y=\beta0+\beta_1X\)</span>, for
a set of <span class="math inline">\(X\)</span> values. Then, we will add the error terms get our simulated values.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" title="1"><span class="co"># set random number seed, so that the random numbers from the text</span></a>
<a class="sourceLine" id="cb43-2" title="2"><span class="co"># is the same when you run the code.</span></a>
<a class="sourceLine" id="cb43-3" title="3"><span class="kw">set.seed</span>(<span class="dv">32</span>)</a>
<a class="sourceLine" id="cb43-4" title="4"></a>
<a class="sourceLine" id="cb43-5" title="5"><span class="co"># get 50 X values between 1 and 100</span></a>
<a class="sourceLine" id="cb43-6" title="6">x =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">50</span>,<span class="dv">1</span>,<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb43-7" title="7"></a>
<a class="sourceLine" id="cb43-8" title="8"><span class="co"># set b0,b1 and varience (sigma)</span></a>
<a class="sourceLine" id="cb43-9" title="9">b0 =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb43-10" title="10">b1 =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb43-11" title="11">sigma =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb43-12" title="12"><span class="co"># simulate error terms from normal distribution</span></a>
<a class="sourceLine" id="cb43-13" title="13">eps =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dv">0</span>,sigma)</a>
<a class="sourceLine" id="cb43-14" title="14"><span class="co"># get y values from the linear equation and addition of error terms</span></a>
<a class="sourceLine" id="cb43-15" title="15">y =<span class="st"> </span>b0 <span class="op">+</span><span class="st"> </span>b1<span class="op">*</span>x<span class="op">+</span><span class="st"> </span>eps</a></code></pre></div>
<p>Now let us fit a line using lm() function. The function requires a formula, and
optionally a data frame. We need the pass the following expression within the
lm function, <code>y~x</code>, where <code>y</code> is the simulated <span class="math inline">\(Y\)</span> values and <code>x</code> is the explanatory variables <span class="math inline">\(X\)</span>. We will then use <code>abline()</code> function to draw the fit.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" title="1">mod1=<span class="kw">lm</span>(y<span class="op">~</span>x)</a>
<a class="sourceLine" id="cb44-2" title="2"></a>
<a class="sourceLine" id="cb44-3" title="3"><span class="co"># plot the data points</span></a>
<a class="sourceLine" id="cb44-4" title="4"><span class="kw">plot</span>(x,y,<span class="dt">pch=</span><span class="dv">20</span>,</a>
<a class="sourceLine" id="cb44-5" title="5">     <span class="dt">ylab=</span><span class="st">&quot;Gene Expression&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Histone modification score&quot;</span>)</a>
<a class="sourceLine" id="cb44-6" title="6"><span class="co"># plot the linear fit</span></a>
<a class="sourceLine" id="cb44-7" title="7"><span class="kw">abline</span>(mod1,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-26"></span>
<img src="compgenomrReloaded_files/figure-html/unnamed-chunk-26-1.png" alt="Gene expression and histone modification score modelled by linear regression" width="60%" />
<p class="caption">
FIGURE 4.15: Gene expression and histone modification score modelled by linear regression
</p>
</div>
</div>
</div>
<div id="how-to-estimate-the-error-of-the-coefficients" class="section level3">
<h3><span class="header-section-number">4.3.2</span> How to estimate the error of the coefficients</h3>
<p>Since we are using a sample to estimate the coefficients they are
not exact, with every random sample they will vary. Below, we
are taking multiple samples from the population and fitting lines to each
sample, with each sample the lines slightly change.We are overlaying the
points and the lines for each sample on top of the other samples
.When we take 200 samples and fit lines for each of them,the lines fit are
variable. And,
we get a normal-like distribution of <span class="math inline">\(\beta\)</span> values with a defined mean
and standard deviation a, which is called standard error of the
coefficients.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-27"></span>
<img src="compgenomrReloaded_files/figure-html/unnamed-chunk-27-1.png" alt="Regression coefficients vary with every random sample. The figure illustrates the variability of regression coefficients when regression is done using a sample of data points. Histograms depict this variability for $b_0$ and $b_1$ coefficients." width="672" />
<p class="caption">
FIGURE 4.16: Regression coefficients vary with every random sample. The figure illustrates the variability of regression coefficients when regression is done using a sample of data points. Histograms depict this variability for <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> coefficients.
</p>
</div>
<p>As usually we will not have access to the population to do repeated sampling,
model fitting and estimation of the standard error for the coefficients. But
there is statistical theory that helps us infer the population properties from
the sample. When we assume that error terms have constant variance and mean zero
, we can model the uncertainty in the regression coefficients, <span class="math inline">\(\beta\)</span>s.
The estimates for standard errors of <span class="math inline">\(\beta\)</span>s for simple regression are as
follows and shown without derivation.</p>
<p><span class="math display">\[
\begin{aligned}
s=RSE=\sqrt{\frac{\sum{(y_i-(\beta_0+\beta_1x_i))^2}}{n-2}  } =\sqrt{\frac{\sum{\epsilon^2}}{n-2}  } \\
SE(\hat{\beta_1})=\frac{s}{\sqrt{\sum{(x_i-\overline{X})^2}}} \\
SE(\hat{\beta_0})=s\sqrt{ \frac{1}{n} + \frac{\overline{X}^2}{\sum{(x_i-\overline{X})^2} }  }
\end{aligned}
\]</span></p>
<p>Notice that that <span class="math inline">\(SE(\beta_1)\)</span> depends on the estimate of variance of
residuals shown as <span class="math inline">\(s\)</span> or <strong>Residual Standard Error (RSE)</strong>.
Notice alsos standard error depends on the spread of <span class="math inline">\(X\)</span>. If <span class="math inline">\(X\)</span> values have more
variation, the standard error will be lower. This intuitively makes sense since if the
spread of the <span class="math inline">\(X\)</span> is low, the regression line will be able to wiggle more
compared to a regression line that is fit to the same number of points but
covers a greater range on the X-axis.</p>
<p>The standard error estimates can also be used to calculate confidence intervals and test
hypotheses, since the following quantity called t-score approximately follows a
t-distribution with <span class="math inline">\(n-p\)</span> degrees of freedom, where <span class="math inline">\(n\)</span> is the number
of data points and <span class="math inline">\(p\)</span> is the number of coefficients estimated.</p>
<p><span class="math display">\[ \frac{\hat{\beta_i}-\beta_test}{SE(\hat{\beta_i})}\]</span></p>
<p>Often, we would like to test the null hypothesis if a coefficient is equal to
zero or not. For simple regression this could mean if there is a relationship
between explanatory variable and response variable. We would calculate the
t-score as follows <span class="math inline">\(\frac{\hat{\beta_i}-0}{SE(\hat{\beta_i})}\)</span>, and compare it
t-distribution with <span class="math inline">\(d.f.=n-p\)</span> to get the p-value.</p>
<p>We can also
calculate the uncertainty of the regression coefficients using confidence
intervals, the range of values that are likely to contain <span class="math inline">\(\beta_i\)</span>. The 95%
confidence interval for <span class="math inline">\(\hat{\beta_i}\)</span> is
<span class="math inline">\(\hat{\beta_i}\)</span> ± <span class="math inline">\(t_{0.975}SE(\hat{\beta_i})\)</span>.
<span class="math inline">\(t_{0.975}\)</span> is the 97.5% percentile of
the t-distribution with <span class="math inline">\(d.f. = n – p\)</span>.</p>
<p>In R, <code>summary()</code> function will test all the coefficients for the null hypothesis
<span class="math inline">\(\beta_i=0\)</span>. The function takes the model output obtained from the <code>lm()</code>
function. To demonstrate this, let us first get some data. The procedure below
simulates data to be used in a regression setting and it is useful to examine
what the linear model expect to model the data.</p>
<p>Since we have the data, we can build our model and call the <code>summary</code> function.
We will then use <code>confint()</code> function to get the confidence intervals on the
coefficients and <code>coef()</code> function to pull out the estimated coefficients from
the model.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" title="1">mod1=<span class="kw">lm</span>(y<span class="op">~</span>x)</a>
<a class="sourceLine" id="cb45-2" title="2"><span class="kw">summary</span>(mod1)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -77.11 -18.44   0.33  16.06  57.23 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  13.2454     6.2887    2.11    0.038 *  
## x             0.4995     0.0513    9.74  4.5e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 28.8 on 98 degrees of freedom
## Multiple R-squared:  0.492,  Adjusted R-squared:  0.486 
## F-statistic: 94.8 on 1 and 98 DF,  p-value: 4.54e-16</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" title="1"><span class="co"># get confidence intervals </span></a>
<a class="sourceLine" id="cb47-2" title="2"><span class="kw">confint</span>(mod1)</a></code></pre></div>
<pre><code>##              2.5 %  97.5 %
## (Intercept) 0.7657 25.7251
## x           0.3977  0.6014</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" title="1"><span class="co"># pull out coefficients from the model</span></a>
<a class="sourceLine" id="cb49-2" title="2"><span class="kw">coef</span>(mod1)</a></code></pre></div>
<pre><code>## (Intercept)           x 
##     13.2454      0.4995</code></pre>
<p>The <code>summary()</code> function prints out an extensive list of values.
The “Coefficients” section has the estimates, their standard error, t score
and the p-value from the hypothesis test <span class="math inline">\(H_0:\beta_i=0\)</span>. As you can see, the
estimate we get for the coefficients and their standard errors are close to
the ones we get from the repeatedly sampling and getting a distribution of
coefficients. This is statistical inference at work, we can estimate the
population properties within a certain error using just a sample.</p>
</div>
<div id="accuracy-of-the-model" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Accuracy of the model</h3>
<p>If you have observed the table output by <code>summary()</code> function, you must have noticed there are some other outputs, such as “Residual standard error”,
“Multiple R-squared” and “F-statistic”. These are metrics that are useful
for assessing the accuracy of the model. We will explain them one by one.</p>
<p>_ (RSE)_ simply is the square-root of the
the sum of squared error terms, divided by degrees of freedom, <span class="math inline">\(n-p\)</span>, for simple
linear regression case, <span class="math inline">\(n-2\)</span>. Sum of of the squares of the error terms is also
called <strong>“Residual sum of squares”</strong>, RSS. So RSE is
calculated as follows:</p>
<p><span class="math display">\[ s=RSE=\sqrt{\frac{\sum{(y_i-\hat{Y_i})^2 }}{n-p}}=\sqrt{\frac{RSS}{n-p}}\]</span></p>
<p>RSE is a way of assessing the model fit. The larger the RSE the worse the
model is. However, this is an absolute measure in the units of <span class="math inline">\(Y\)</span> and we have nothing to
compare against. One idea is that we divide it by RSS of a simpler model
for comparative purposes. That simpler model is in this case is the model
with the intercept,<span class="math inline">\(\beta_0\)</span>. A very bad model will have close zero
coefficients for explanatory variables, and the RSS of that model
will be close to the RSS of the model with only the intercept. In such
a model intercept will be equal to <span class="math inline">\(\overline{Y}\)</span>. As it turns out, RSS of
the the model with
just the intercept is called <em>“Total Sum of Squares” or TSS</em>. A good model will have a low <span class="math inline">\(RSS/TSS\)</span>. The metric <span class="math inline">\(R^2\)</span> uses these quantities to calculate a score between 0 and 1, and closer to 1 the better the model. Here is how
it is calculated:</p>
<p><span class="math display">\[R^2=1-\frac{RSS}{TSS}=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}\]</span></p>
<p><span class="math inline">\(TSS-RSS\)</span> part of the formula often referred to as “explained variability” in
the model. The bottom part is for “total variability”. With this interpretation, higher
the “explained variability” better the model. For simple linear regression
with one explanatory variable, the square root of <span class="math inline">\(R^2\)</span> is a quantity known
as absolute value of the correlation coefficient, which can be calculated for any pair of variables, not only
the
response and the explanatory variables. <em>Correlation</em> is a general measure of
linear
relationship between two variables. One
of the most popular flavors of correlation is the Pearson correlation coefficient. Formally, It is the
<em>covariance</em> of X and Y divided by multiplication of standard deviations of
X and Y. In R, it can be calculated with <code>cor()</code> function.</p>
<p><span class="math display">\[ 
r_{xy}=\frac{cov(X,Y)}{\sigma_x\sigma_y}
      =\frac{\sum\limits_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}
            {\sqrt{\sum\limits_{i=1}^n (x_i-\bar{x})^2 \sum\limits_{i=1}^n (y_i-\bar{y})^2}}
\]</span>
In the equation above, cov is the covariance, this is again a measure of
how much two variables change together, like correlation. If two variables
show similar behavior they will usually have positive covariance value, if they have opposite behavior, the covariance will have negative value.
However, these values are boundless. A normalized way of looking at
covariance is to divide covariance by the multiplication of standard
errors of X and Y. This bounds the values to -1 and 1, and as mentioned
above called Pearson correlation coefficient. The values that change in a similar manner will have a positive coefficient, the values that change in
opposite manner will have negative coefficient, and pairs do not have
a linear relationship will have 0 or near 0 correlation. In
the figure below, we are showing <span class="math inline">\(R^2\)</span>, correlation
coefficient and covariance for different scatter plots.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-30"></span>
<img src="compgenomrReloaded_files/figure-html/unnamed-chunk-30-1.png" alt="Correlation and covariance for different scatter plots" width="864" />
<p class="caption">
FIGURE 4.17: Correlation and covariance for different scatter plots
</p>
</div>
<p>For simple linear regression, correlation can be used to asses the model. However, this becomes useless as a measure of general accuracy
if the there are more than one explanatory
variable as in multiple linear regression. In that case, <span class="math inline">\(R^2\)</span> is a measure
of accuracy for the model. Interestingly, square of the
correlation of predicted values
and original response variables (<span class="math inline">\((cor(Y,\hat{Y}))^2\)</span> ) equals to <span class="math inline">\(R^2\)</span> for
multiple linear regression.</p>
<p>The last accuracy measure or the model fit in general we are going to explain is <em>F-statistic</em>. This is a quantity that depends on RSS and TSS again. It can also answer one important question that other metrics can
not easily answer. That question is whether or not any of the explanatory
variables have predictive value or in other words if all the explanatory variables are zero. We can write the null hypothesis as follows:</p>
<p><span class="math display">\[H_0: \beta_1=\beta_2=\beta_3=...=\beta_p=0 \]</span></p>
<p>where the alternative is:</p>
<p><span class="math display">\[H_1: \text{at least one } \beta_i \neq 0 \]</span></p>
<p>Remember <span class="math inline">\(TSS-RSS\)</span> is analogous to “explained variability” and the RSS is
analogous to “unexplained variability”. For the F-statistic, we divide explained variance to
unexplained variance. Explained variance is just the <span class="math inline">\(TSS-RSS\)</span> divided
by degrees of freedom, and unexplained variance is the RSE.
The ratio will follow the F-distribution
with two parameters, the degrees of freedom for the explained variance and
the degrees of freedom for the the unexplained variance.F-statistic for a linear model is calculated as follows.</p>
<p><span class="math display">\[F=\frac{(TSS-RSS)/(p-1)}{RSS/(n-p)}=\frac{(TSS-RSS)/(p-1)}{RSE} \sim F(p-1,n-p)\]</span></p>
<p>If the variances are the same, the ratio will be 1, and when <span class="math inline">\(H_0\)</span> is true, then
it can be shown that expected value of <span class="math inline">\((TSS-RSS)/(p-1)\)</span> will be <span class="math inline">\(\sigma^2\)</span>
which is estimated by RSE. So, if the variances are significantly different,
the ratio will need to be significantly bigger than 1.
If the ratio is large enough we can reject the null hypothesis. To asses that
we need to use software or look up the tables for F statistics with calculated
parameters. In R, function <code>qf()</code> can be used to calculate critical value of the
ratio. Benefit of the F-test over
looking at significance of coefficients one by one is that we circumvent
multiple testing problem. If there are lots of explanatory variables
at least 5% of the time (assuming we use 0.05 as P-value significance
cutoff), p-values from coefficient t-tests will be wrong. In summary,
F-test is a better choice for testing if there is any association
between the explanatory variables and the response variable.</p>
</div>
<div id="regression-with-categorical-variables" class="section level3">
<h3><span class="header-section-number">4.3.4</span> Regression with categorical variables</h3>
<p>An important feature of linear regression is that categorical variables can
be used as explanatory variables, this feature is very useful in genomics
where explanatory variables often could be categorical. To put it in
context, in our histone modification example we can also include if
promoters have CpG islands or not as a variable. In addition, in
differential gene expression, we usually test the difference between
different condition which can be encoded as categorical variables in
a linear regression. We can sure use t-test for that as well if there
are only 2 conditions, but if there are more conditions and other variables
to control for such as Age or sex of the samples, we need to take those
into account for our statistics, and t-test alone can not handle such
complexity. In addition, when we have categorical variables we can also
have numeric variables in the model and we certainly do not have to include
only one type of variable in a model.</p>
<p>The simplest model with categorical variables include two levels that
can be encoded in 0 and 1.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" title="1"><span class="kw">set.seed</span>(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb51-2" title="2">gene1=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">4</span>,<span class="dt">sd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb51-3" title="3">gene2=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">2</span>,<span class="dt">sd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb51-4" title="4">gene.df=<span class="kw">data.frame</span>(<span class="dt">exp=</span><span class="kw">c</span>(gene1,gene2),</a>
<a class="sourceLine" id="cb51-5" title="5">                  <span class="dt">group=</span><span class="kw">c</span>( <span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">30</span>),<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">30</span>) ) )</a>
<a class="sourceLine" id="cb51-6" title="6"></a>
<a class="sourceLine" id="cb51-7" title="7">mod2=<span class="kw">lm</span>(exp<span class="op">~</span>group,<span class="dt">data=</span>gene.df)</a>
<a class="sourceLine" id="cb51-8" title="8"><span class="kw">summary</span>(mod2)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = exp ~ group, data = gene.df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.729 -1.066  0.012  1.384  4.563 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    2.185      0.352    6.21    6e-08 ***
## group          1.873      0.497    3.77  0.00039 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.93 on 58 degrees of freedom
## Multiple R-squared:  0.196,  Adjusted R-squared:  0.183 
## F-statistic: 14.2 on 1 and 58 DF,  p-value: 0.000391</code></pre>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" title="1"><span class="kw">require</span>(mosaic)</a>
<a class="sourceLine" id="cb53-2" title="2"><span class="kw">plotModel</span>(mod2)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-31"></span>
<img src="compgenomrReloaded_files/figure-html/unnamed-chunk-31-1.png" alt="Linear model with a categorical variable coded as 0 and 1" width="672" />
<p class="caption">
FIGURE 4.18: Linear model with a categorical variable coded as 0 and 1
</p>
</div>
<p>we can even compare more levels, we do not even have to encode them
ourselves. We can pass categorical variables to <code>lm()</code> function.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb54-1" title="1">gene.df=<span class="kw">data.frame</span>(<span class="dt">exp=</span><span class="kw">c</span>(gene1,gene2,gene2),</a>
<a class="sourceLine" id="cb54-2" title="2">                  <span class="dt">group=</span><span class="kw">c</span>( <span class="kw">rep</span>(<span class="st">&quot;A&quot;</span>,<span class="dv">30</span>),<span class="kw">rep</span>(<span class="st">&quot;B&quot;</span>,<span class="dv">30</span>),<span class="kw">rep</span>(<span class="st">&quot;C&quot;</span>,<span class="dv">30</span>) ) </a>
<a class="sourceLine" id="cb54-3" title="3">                  )</a>
<a class="sourceLine" id="cb54-4" title="4"></a>
<a class="sourceLine" id="cb54-5" title="5">mod3=<span class="kw">lm</span>(exp<span class="op">~</span>group,<span class="dt">data=</span>gene.df)</a>
<a class="sourceLine" id="cb54-6" title="6"><span class="kw">summary</span>(mod3)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = exp ~ group, data = gene.df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.729 -1.079 -0.098  1.484  4.563 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    4.058      0.378    10.7  &lt; 2e-16 ***
## groupB        -1.873      0.535    -3.5  0.00073 ***
## groupC        -1.873      0.535    -3.5  0.00073 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.07 on 87 degrees of freedom
## Multiple R-squared:  0.158,  Adjusted R-squared:  0.139 
## F-statistic: 8.17 on 2 and 87 DF,  p-value: 0.000558</code></pre>
</div>
<div id="regression-pitfalls" class="section level3">
<h3><span class="header-section-number">4.3.5</span> Regression pitfalls</h3>
<p>In most cases one should look at the error terms (residuals) vs fitted
values plot. Any structure in this plot indicates problems such as
non-linearity, correlation of error terms, non-constant variance or
unusual values driving the fit. Below we briefly explain the potential
issues with the linear regression.</p>
<div id="non-linearity" class="section level5">
<h5><span class="header-section-number">4.3.5.0.1</span> non-linearity</h5>
<p>If the true relationship is far from linearity, prediction accuracy
is reduced and all the other conclusions are questionable. In some cases,
transforming the data with <span class="math inline">\(logX\)</span>, <span class="math inline">\(\sqrt{X}\)</span> and <span class="math inline">\(X^2\)</span> could resolve
the issue.</p>
</div>
<div id="correlation-of-explanatory-variables" class="section level5">
<h5><span class="header-section-number">4.3.5.0.2</span> correlation of explanatory variables</h5>
<p>If the explanatory variables are correlated that could lead to something
known as multicolinearity. When this happens SE estimates of the coefficients will be too large. This is usually observed in time-course
data.</p>
</div>
<div id="correlation-of-error-terms" class="section level5">
<h5><span class="header-section-number">4.3.5.0.3</span> correlation of error terms</h5>
<p>This assumes that the errors of the response variables are uncorrelated with each other. If they are confidence intervals in the coefficients
might too narrow.</p>
</div>
<div id="non-constant-variance-of-error-terms" class="section level5">
<h5><span class="header-section-number">4.3.5.0.4</span> Non-constant variance of error terms</h5>
<p>This means that different response variables have the same variance in their errors, regardless of the values of the predictor variables. If
the errors are not constant, if for the errors grow as X grows this
will result in unreliable estimates in standard errors as the model
assumes constant variance. Transformation of data, such as
<span class="math inline">\(logX\)</span> and <span class="math inline">\(\sqrt{X}\)</span> could help in some cases.</p>
</div>
<div id="outliers-and-high-leverage-points" class="section level5">
<h5><span class="header-section-number">4.3.5.0.5</span> outliers and high leverage points</h5>
<p>Outliers are extreme values for Y and high leverage points are unusual
X values. Both of these extremes have power to affect the fitted line
and the standard errors. In some cases (measurement error), they can be
removed from the data for a better fit.</p>

<div class="rmdtip">
<p><strong>Want to know more ?</strong></p>
<ul>
<li>linear models and derivations of equations including matrix notation
<ul>
<li>Applied Linear Statistical Models by Kutner, Nachtsheim, et al.</li>
<li>Elements of statistical learning by Hastie &amp; Tibshirani</li>
<li>An Introduction to statistical learning by James, Witten, et al.</li>
</ul></li>
</ul>
</div>

</div>
</div>
</div>
<p style="text-align: center;">
<a href="4-2-how-to-test-for-differences-between-samples.html"><button class="btn btn-default">Previous</button></a>
<a href="4-4-clustering-grouping-samples-based-on-their-similarity.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
