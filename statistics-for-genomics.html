<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Genomics with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="book files for Computational Genomics with R">
  <meta name="generator" content="bookdown 0.1.7 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Genomics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="book files for Computational Genomics with R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Genomics with R" />
  
  <meta name="twitter:description" content="book files for Computational Genomics with R" />
  

<meta name="author" content="Altuna Akalin">

<meta name="date" content="2016-09-05">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction-to-r.html">
<link rel="next" href="genomic-intervals-and-r.html">

<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#why-r"><i class="fa fa-check"></i><b>1.1</b> Why R ?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#data-munging-pre-processing"><i class="fa fa-check"></i><b>1.1.1</b> Data munging (pre-processing)</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#general-data-anaylsis-and-exploration"><i class="fa fa-check"></i><b>1.1.2</b> General data anaylsis and exploration</a></li>
<li class="chapter" data-level="1.1.3" data-path="index.html"><a href="index.html#visualization"><i class="fa fa-check"></i><b>1.1.3</b> Visualization</a></li>
<li class="chapter" data-level="1.1.4" data-path="index.html"><a href="index.html#dealing-with-genomic-intervals"><i class="fa fa-check"></i><b>1.1.4</b> Dealing with genomic intervals</a></li>
<li class="chapter" data-level="1.1.5" data-path="index.html"><a href="index.html#application-of-other-bioinformatics-specific-algorithms"><i class="fa fa-check"></i><b>1.1.5</b> Application of other bioinformatics specific algorithms</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i><b>1.2</b> Who is this book for?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#what-will-you-get-out-of-this"><i class="fa fa-check"></i><b>1.2.1</b> What will you get out of this?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.3</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html"><i class="fa fa-check"></i><b>2</b> Introduction to Genomics</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#genes-dna-and-central-dogma"><i class="fa fa-check"></i><b>2.1</b> Genes, DNA and central dogma</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#what-is-a-genome"><i class="fa fa-check"></i><b>2.1.1</b> What is a genome?</a></li>
<li class="chapter" data-level="2.1.2" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#what-is-a-gene"><i class="fa fa-check"></i><b>2.1.2</b> What is a gene?</a></li>
<li class="chapter" data-level="2.1.3" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#how-genes-are-controlled-the-transcriptional-and-the-post-transcriptional-regulation"><i class="fa fa-check"></i><b>2.1.3</b> How genes are controlled ? The transcriptional and the post-transcriptional regulation</a></li>
<li class="chapter" data-level="2.1.4" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#what-does-a-gene-look-like"><i class="fa fa-check"></i><b>2.1.4</b> What does a gene look like?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#elements-of-gene-regulation"><i class="fa fa-check"></i><b>2.2</b> Elements of gene regulation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#transcriptional-regulation"><i class="fa fa-check"></i><b>2.2.1</b> Transcriptional regulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#post-transcriptional-regulation"><i class="fa fa-check"></i><b>2.2.2</b> Post-transcriptional regulation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#shaping-the-genome-dna-mutation"><i class="fa fa-check"></i><b>2.3</b> Shaping the genome: DNA mutation</a></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#high-throughput-experimental-methods-in-genomics"><i class="fa fa-check"></i><b>2.4</b> High-throughput experimental methods in genomics</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#the-general-idea-behind-high-throughput-techniques"><i class="fa fa-check"></i><b>2.4.1</b> The general idea behind high-throughput techniques</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#high-throughput-sequencing"><i class="fa fa-check"></i><b>2.4.2</b> High-throughput sequencing</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-genomics.html"><a href="introduction-to-genomics.html#visualization-and-data-repositories-for-genomics"><i class="fa fa-check"></i><b>2.5</b> Visualization and data repositories for genomics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>3</b> Introduction to R</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#the-setup"><i class="fa fa-check"></i><b>3.1</b> The Setup</a><ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#installing-packages"><i class="fa fa-check"></i><b>3.1.1</b> Installing packages</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#installing-packages-in-custom-locations"><i class="fa fa-check"></i><b>3.1.2</b> Installing packages in custom locations</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-help-on-functions-and-packages"><i class="fa fa-check"></i><b>3.1.3</b> Getting help on functions and packages</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#computations-in-r"><i class="fa fa-check"></i><b>3.2</b> Computations in R</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-structures"><i class="fa fa-check"></i><b>3.3</b> Data structures</a><ul>
<li class="chapter" data-level="3.3.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#vectors"><i class="fa fa-check"></i><b>3.3.1</b> Vectors</a></li>
<li class="chapter" data-level="3.3.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#matrices"><i class="fa fa-check"></i><b>3.3.2</b> Matrices</a></li>
<li class="chapter" data-level="3.3.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-frames"><i class="fa fa-check"></i><b>3.3.3</b> Data Frames</a></li>
<li class="chapter" data-level="3.3.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#lists"><i class="fa fa-check"></i><b>3.3.4</b> Lists</a></li>
<li class="chapter" data-level="3.3.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#factors"><i class="fa fa-check"></i><b>3.3.5</b> Factors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-types"><i class="fa fa-check"></i><b>3.4</b> Data types</a></li>
<li class="chapter" data-level="3.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#reading-and-writing-data"><i class="fa fa-check"></i><b>3.5</b> Reading and writing data</a></li>
<li class="chapter" data-level="3.6" data-path="introduction-to-r.html"><a href="introduction-to-r.html#plotting-in-r"><i class="fa fa-check"></i><b>3.6</b> Plotting in R</a></li>
<li class="chapter" data-level="3.7" data-path="introduction-to-r.html"><a href="introduction-to-r.html#functions-and-control-structures-for-ifelse-etc."><i class="fa fa-check"></i><b>3.7</b> Functions and control structures (for, if/else etc.)</a><ul>
<li class="chapter" data-level="3.7.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#user-defined-functions"><i class="fa fa-check"></i><b>3.7.1</b> User defined functions</a></li>
<li class="chapter" data-level="3.7.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#loops-and-looping-structures-in-r"><i class="fa fa-check"></i><b>3.7.2</b> Loops and looping structures in R</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="introduction-to-r.html"><a href="introduction-to-r.html#session-info"><i class="fa fa-check"></i><b>3.8</b> Session info</a></li>
<li class="chapter" data-level="3.9" data-path="introduction-to-r.html"><a href="introduction-to-r.html#exercises"><i class="fa fa-check"></i><b>3.9</b> Exercises</a><ul>
<li class="chapter" data-level="3.9.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#computations-in-r-1"><i class="fa fa-check"></i><b>3.9.1</b> Computations in R</a></li>
<li class="chapter" data-level="3.9.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-structures-in-r"><i class="fa fa-check"></i><b>3.9.2</b> Data structures in R</a></li>
<li class="chapter" data-level="3.9.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#reading-in-and-writing-data-out-in-r"><i class="fa fa-check"></i><b>3.9.3</b> Reading in and writing data out in R</a></li>
<li class="chapter" data-level="3.9.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#plotting-in-r-1"><i class="fa fa-check"></i><b>3.9.4</b> Plotting in R</a></li>
<li class="chapter" data-level="3.9.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#functions-and-control-structures-for-ifelse-etc.-1"><i class="fa fa-check"></i><b>3.9.5</b> Functions and control structures (for, if/else etc.)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html"><i class="fa fa-check"></i><b>4</b> Statistics for Genomics</a><ul>
<li class="chapter" data-level="4.1" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions"><i class="fa fa-check"></i><b>4.1</b> How to summarize collection of data points: The idea behind statistical distributions</a><ul>
<li class="chapter" data-level="4.1.1" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#describing-the-central-tendency-mean-and-median"><i class="fa fa-check"></i><b>4.1.1</b> Describing the central tendency: mean and median</a></li>
<li class="chapter" data-level="4.1.2" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#describing-the-spread-measurements-of-variation"><i class="fa fa-check"></i><b>4.1.2</b> Describing the spread: measurements of variation</a></li>
<li class="chapter" data-level="4.1.3" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#precision-of-estimates-confidence-intervals"><i class="fa fa-check"></i><b>4.1.3</b> Precision of estimates: Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#how-to-test-for-differences-in-samples"><i class="fa fa-check"></i><b>4.2</b> How to test for differences in samples</a><ul>
<li class="chapter" data-level="4.2.1" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#randomization-based-testing-for-difference-of-the-means"><i class="fa fa-check"></i><b>4.2.1</b> randomization based testing for difference of the means</a></li>
<li class="chapter" data-level="4.2.2" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#using-t-test-for-difference-of-the-means-between-two-samples"><i class="fa fa-check"></i><b>4.2.2</b> Using t-test for difference of the means between two samples</a></li>
<li class="chapter" data-level="4.2.3" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#multiple-testing-correction"><i class="fa fa-check"></i><b>4.2.3</b> multiple testing correction</a></li>
<li class="chapter" data-level="4.2.4" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#moderated-t-tests-using-information-from-multiple-comparisons"><i class="fa fa-check"></i><b>4.2.4</b> moderated t-tests: using information from multiple comparisons</a></li>
<li class="chapter" data-level="4.2.5" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#want-to-know-more"><i class="fa fa-check"></i><b>4.2.5</b> Want to know more…</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#relationship-between-variables-linear-models-and-correlation"><i class="fa fa-check"></i><b>4.3</b> Relationship between variables: linear models and correlation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#how-to-fit-a-line"><i class="fa fa-check"></i><b>4.3.1</b> How to fit a line</a></li>
<li class="chapter" data-level="4.3.2" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#how-to-estimate-the-error-of-the-coefficients"><i class="fa fa-check"></i><b>4.3.2</b> How to estimate the error of the coefficients</a></li>
<li class="chapter" data-level="4.3.3" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#accuracy-of-the-model"><i class="fa fa-check"></i><b>4.3.3</b> Accuracy of the model</a></li>
<li class="chapter" data-level="4.3.4" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#regression-with-categorical-variables"><i class="fa fa-check"></i><b>4.3.4</b> Regression with categorical variables</a></li>
<li class="chapter" data-level="4.3.5" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#regression-pitfalls"><i class="fa fa-check"></i><b>4.3.5</b> Regression pitfalls</a></li>
<li class="chapter" data-level="4.3.6" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#want-to-know-more-1"><i class="fa fa-check"></i><b>4.3.6</b> Want to know more…</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#roadmap-for-future"><i class="fa fa-check"></i><b>4.4</b> Roadmap for future</a><ul>
<li class="chapter" data-level="4.4.1" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#clustering"><i class="fa fa-check"></i><b>4.4.1</b> Clustering</a></li>
<li class="chapter" data-level="4.4.2" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#dimension-reduction-with-pca"><i class="fa fa-check"></i><b>4.4.2</b> Dimension reduction with PCA</a></li>
<li class="chapter" data-level="4.4.3" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#classification"><i class="fa fa-check"></i><b>4.4.3</b> Classification</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#exercises-1"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="4.5.1" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1"><i class="fa fa-check"></i><b>4.5.1</b> How to summarize collection of data points: The idea behind statistical distributions</a></li>
<li class="chapter" data-level="4.5.2" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#how-to-test-for-differences-in-samples-1"><i class="fa fa-check"></i><b>4.5.2</b> How to test for differences in samples</a></li>
<li class="chapter" data-level="4.5.3" data-path="statistics-for-genomics.html"><a href="statistics-for-genomics.html#relationship-between-variables-linear-models-and-correlation-1"><i class="fa fa-check"></i><b>4.5.3</b> Relationship between variables: linear models and correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html"><i class="fa fa-check"></i><b>5</b> Genomic Intervals and R</a><ul>
<li class="chapter" data-level="5.1" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#operations-on-genomic-intervals-with-genomicranges-package"><i class="fa fa-check"></i><b>5.1</b> Operations on Genomic Intervals with GenomicRanges package</a><ul>
<li class="chapter" data-level="5.1.1" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#how-to-create-and-manipulate-a-granges-object"><i class="fa fa-check"></i><b>5.1.1</b> How to create and manipulate a GRanges object</a></li>
<li class="chapter" data-level="5.1.2" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#getting-genomic-regions-into-r-as-granges-objects"><i class="fa fa-check"></i><b>5.1.2</b> Getting genomic regions into R as GRanges objects</a></li>
<li class="chapter" data-level="5.1.3" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#finding-regions-that-dodo-not-overlap-with-another-set-of-regions"><i class="fa fa-check"></i><b>5.1.3</b> Finding regions that do/do not overlap with another set of regions</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#dealing-with-high-throughput-sequencing-reads"><i class="fa fa-check"></i><b>5.2</b> Dealing with high-throughput sequencing reads</a><ul>
<li class="chapter" data-level="5.2.1" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#quality-check-on-sequencing-reads-and-mapping-reads-to-the-genome"><i class="fa fa-check"></i><b>5.2.1</b> Quality check on sequencing reads and mapping reads to the genome</a></li>
<li class="chapter" data-level="5.2.2" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#counting-mapped-reads-for-a-set-of-regions"><i class="fa fa-check"></i><b>5.2.2</b> Counting mapped reads for a set of regions</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#dealing-with-continuous-scores-over-the-genome"><i class="fa fa-check"></i><b>5.3</b> Dealing with continuous scores over the genome</a><ul>
<li class="chapter" data-level="5.3.1" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#extracting-subsections-of-rle-and-rlelist-objects"><i class="fa fa-check"></i><b>5.3.1</b> Extracting subsections of Rle and RleList objects</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#visualizing-and-summarizing-genomic-intervals"><i class="fa fa-check"></i><b>5.4</b> Visualizing and summarizing genomic intervals</a><ul>
<li class="chapter" data-level="5.4.1" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#visualizing-intervals-in-locus-of-interest"><i class="fa fa-check"></i><b>5.4.1</b> Visualizing intervals in locus of interest</a></li>
<li class="chapter" data-level="5.4.2" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#summaries-of-genomic-intervals"><i class="fa fa-check"></i><b>5.4.2</b> Summaries of genomic intervals</a></li>
<li class="chapter" data-level="5.4.3" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#making-karyograms-and-circos-plots"><i class="fa fa-check"></i><b>5.4.3</b> Making karyograms and circos plots</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#session-info-1"><i class="fa fa-check"></i><b>5.5</b> Session info</a></li>
<li class="chapter" data-level="5.6" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#exercises-2"><i class="fa fa-check"></i><b>5.6</b> Exercises</a><ul>
<li class="chapter" data-level="5.6.1" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#the-setup-1"><i class="fa fa-check"></i><b>5.6.1</b> The setup</a></li>
<li class="chapter" data-level="5.6.2" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#operations-on-genomic-intervals-with-genomicranges-package-1"><i class="fa fa-check"></i><b>5.6.2</b> Operations on Genomic Intervals with GenomicRanges package</a></li>
<li class="chapter" data-level="5.6.3" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#dealing-with-mapped-high-throughput-sequencing-reads"><i class="fa fa-check"></i><b>5.6.3</b> Dealing with mapped high-throughput sequencing reads</a></li>
<li class="chapter" data-level="5.6.4" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#dealing-with-contiguous-scores-over-the-genome"><i class="fa fa-check"></i><b>5.6.4</b> Dealing with contiguous scores over the genome</a></li>
<li class="chapter" data-level="5.6.5" data-path="genomic-intervals-and-r.html"><a href="genomic-intervals-and-r.html#visualizing-and-summarizing-genomic-intervals-1"><i class="fa fa-check"></i><b>5.6.5</b> Visualizing and summarizing genomic intervals</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Genomics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistics-for-genomics" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Statistics for Genomics</h1>
<p>This chapter will summarize statistics and machine-learning methods frequently used in computational genomics. As these fields are continuously evolving, the techniques introduced here do not form an exhaustive list but mostly corner stone methods that are often and still being used. In addition, we focused on giving intuitive and practical understanding of the methods with relevant examples from the field.<br />
If you want to dig deeper into statistics and math, beyond what is described here, we included appropriate references with annotation after each major section.</p>
<div id="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions" class="section level2">
<h2><span class="header-section-number">4.1</span> How to summarize collection of data points: The idea behind statistical distributions</h2>
<p>In biology and many other fields data is collected via experimentation. The nature of the experiments and natural variation in biology makes it impossible to get the same exact measurements every time you measure something. For example, if you are measuring gene expression values for a certain gene, say PAX6, and let’s assume you are measuring expression per sample and cell with any method( microarrays, rt-qPCR, etc.). You will not get the same expression value even if your samples are homogeneous. Due to technical bias in experiments or natural variation in the samples. Instead, we would like to describe this collection of data some other way that represents the general properties of the data. The figure shows a sample of 20 expression values from PAX6 gene.</p>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-128-1.png" width="672" /></p>
<div id="describing-the-central-tendency-mean-and-median" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Describing the central tendency: mean and median</h3>
<p>As seen in the figure above, the points from this sample are distributed around a central value and the histogram below the dot plot shows number of points in each bin. Another observation is that there are some bins that have more points than others. If we want to summarize what we observe, we can try to represent the collection of data points with an expression value that is typical to get, something that represents the general tendency we observe on the dot plot and the histogram. This value is sometimes called central value or central tendency, and there are different ways to calculate such a value. In the figure above, we see that all the values are spread around 6.13 (red line), and that is indeed what we call mean value of this sample of expression values. It can be calculated with the following formula <span class="math inline">\(\overline{X}=\sum_{i=1}^n x_i/n\)</span>, where <span class="math inline">\(x_i\)</span> is the expression value of an experiment and <span class="math inline">\(n\)</span> is the number of expression value obtained from the experiments. In R, <code>mean()</code> function will calculate the mean of a provided vector of numbers. This is called a “sample mean”. In reality the possible values of PAX6 expression for all cells (provided each cell is of the identical cell type and is in identical conditions) are much much more than 20. If we had the time and the funding to sample all cells and measure PAX6 expression we would get a collection values that would be called, in statistics, a “population”. In our case the population will look like the left hand side of the figure below. What we have done with our 20 data points is that we took a sample of PAX6 expression values from this population, and calculated the sample mean.</p>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-129-1.png" width="672" /></p>
<p>The mean of the population is calculated the same way but traditionally Greek letter <span class="math inline">\(\mu\)</span> is used to denote the population mean. Normally, we would not have access to the population and we will use sample mean and other quantities derived from the sample to estimate the population properties. This is the basic idea behind statistical inference which we will see this in action in later sections as well. We estimate the population parameters from the sample parameters and there is some uncertainty associated with those estimates. We will be trying to assess those uncertainties and make decisions in the presence of those uncertainties.</p>
<p>We are not yet done with measuring central tendency. There are other ways to describe it, such as the median value. Mean can be affected by outliers easily. If certain values are very high or low from the bulk of the sample this will shift mean towards those outliers. However, median is not affected by outliers. It is simply the value in a distribution where half of the values are above and the other half is below. In R, <code>median()</code> function will calculate the mean of a provided vector of numbers.</p>
<p>Let’s create a set of random numbers and calculate their mean and median using R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#create 10 random numbers from uniform distribution </span>
x=<span class="kw">runif</span>(<span class="dv">10</span>)
<span class="co"># calculate mean</span>
<span class="kw">mean</span>(x)</code></pre></div>
<pre><code>## [1] 0.3738963</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate median</span>
<span class="kw">median</span>(x)</code></pre></div>
<pre><code>## [1] 0.3277896</code></pre>
</div>
<div id="describing-the-spread-measurements-of-variation" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Describing the spread: measurements of variation</h3>
<p>Another useful way to summarize a collection of data points is to measure how variable the values are. You can simply describe the range of the values , such as minimum and maximum values. You can easily do that in R with <code>range()</code> function. A more common way to calculate variation is by calculating something called “standard deviation” or the related quantity called “variance”. This is a quantity that shows how variable the values are, a value around zero indicates there is not much variation in the values of the data points, and a high value indicates high variation in the values. The variance is the squared distance of data points from the mean. Population variance is again a quantity we usually do not have access to and is simply calculate as follows <span class="math inline">\(\sigma^2=\sum_{i=1}^n \frac{(x_i-\mu)^2}{n}\)</span>, where <span class="math inline">\(\mu\)</span> is the population mean, <span class="math inline">\(x_i\)</span> is the ith data point in the population and <span class="math inline">\(n\)</span> is the population size. However, when the we have only access to a sample this formulation is biased. It means that it underestimates the population variance, so we make a small adjustment when we calculate the sample variance, denoted as <span class="math inline">\(s^2\)</span>: <span class="math display">\[
\begin{align}
s^2=\sum_{i=1}^n \frac{(x_i-\overline{X})^2}{n-1} &amp;&amp; \text{ where $x_i$ is the ith data point and
$\overline{X}$ is the sample mean.}
\end{align}
\]</span></p>
<p>The sample standard deviation is simply the square-root of the sample variance. The good thing about standard deviation is that it has the same unit as the mean so it is more intuitive.<br />
<span class="math display">\[s=\sqrt{\sum_{i=1}^n \frac{(x_i-\overline{X})^2}{n-1}}\]</span></p>
<p>We can calculate sample standard deviation and variation with <code>sd()</code> and <code>var()</code> functions in R. These functions take vector of numeric values as input and calculate the desired quantities. Below we use those functions on a randomly generated vector of numbers.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x=<span class="kw">rnorm</span>(<span class="dv">20</span>,<span class="dt">mean=</span><span class="dv">6</span>,<span class="dt">sd=</span><span class="fl">0.7</span>)
<span class="kw">var</span>(x)</code></pre></div>
<pre><code>## [1] 0.2531495</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(x)</code></pre></div>
<pre><code>## [1] 0.5031397</code></pre>
<p>One potential problem with the variance is that it could be affected by outliers. The points that are too far away from the mean will have a large affect on the variance even though there might be few of them. A way to measure variance that could be less affected by outliers is looking at where bulk of the distribution is. How do we define where the bulk is? One common way is to look at the the difference between 75th percentile and 25th percentile, this effectively removes a lot of potential outliers which will be towards the edges of the range of values. This is called interquartile range , and can be easily calculated using R via <code>IQR()</code> function and the quantiles of a vector is calculated with <code>quantile()</code> function.</p>
<p>Let us plot the boxplot for a random vector and also calculate IQR using R. In the boxplot below, 25th and 75th percentiles are the edges of the box, and the median is marked with a thick line going through roughly middle the box.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x=<span class="kw">rnorm</span>(<span class="dv">20</span>,<span class="dt">mean=</span><span class="dv">6</span>,<span class="dt">sd=</span><span class="fl">0.7</span>)
<span class="kw">IQR</span>(x)</code></pre></div>
<pre><code>## [1] 0.5010954</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quantile</span>(x)</code></pre></div>
<pre><code>##       0%      25%      50%      75%     100% 
## 5.437119 5.742895 5.860302 6.243991 6.558112</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(x,<span class="dt">horizontal =</span> T)</code></pre></div>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-134-1.png" width="672" /></p>
<div id="frequently-used-statistical-distributions" class="section level4">
<h4><span class="header-section-number">4.1.2.1</span> Frequently used statistical distributions</h4>
<p>The distributions have parameters (such as mean and variance) that summarizes them but also they are functions that assigns each outcome of a statistical experiment to its probability of occurrence. One distribution that you will frequently encounter is the normal distribution or Gaussian distribution. The normal distribution has a typical “bell-curve” shape and, characterized by mean and standard deviation. A set of data points that follow normal distribution mostly will be close to the mean but spread around it controlled by the standard deviation parameter. That means if we sample data points from a normal distribution we are more likely to sample nearby the mean and sometimes away from the mean. Probability of an event occurring is higher if it is nearby the mean. The effect of the parameters for normal distribution can be observed in the following plot.</p>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-135-1.png" width="672" /></p>
<p>The normal distribution is often denoted by <span class="math inline">\(\mathcal{N}(\mu,\,\sigma^2)\)</span> When a random variable <span class="math inline">\(X\)</span> is distributed normally with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, we write:</p>
<p><span class="math display">\[X\ \sim\ \mathcal{N}(\mu,\,\sigma^2).\]</span></p>
<p>The probability density function of Normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> is as follows</p>
<p><span class="math display">\[P(x)=\frac{1}{\sigma\sqrt{2\pi} } \; e^{ -\frac{(x-\mu)^2}{2\sigma^2} } \]</span></p>
<p>The probability density function gives the probability of observing a value on a normal distribution defined by <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> parameters.</p>
<p>Often times, we do not need the exact probability of a value but we need the probability of observing a value larger or smaller than a critical value or reference point. For example, we might want to know the probability of <span class="math inline">\(X\)</span> being smaller than or equal to -2 for a normal distribution with mean 0 and standard deviation 2. ,<span class="math inline">\(P(X &lt;= -2 \; | \; \mu=0,\sigma=2)\)</span>. In this case, what we want is the are under the curve shaded in blue. To be able to that we need to integrate the probability density function but we will usually let software do that. Traditional, you calculate a Z-score which is simply <span class="math inline">\((X-\mu)/\sigma=(-2-0)/2=1\)</span>, and corresponds to how many standard deviations you are away from the mean. This is also called “standardization”, the corresponding value is distributed in “standard normal distribution” where <span class="math inline">\(\mathcal{N}(0,\,1)\)</span>.</p>
<p>After calculating the Z-score, we can go look up in a table, that contains the area under the curve for the left and right side of the Z-score, but again use software for that tables are old school.</p>
<p>Below we are showing the Z-score and the associated probabilities derived from the calculation above for <span class="math inline">\(P(X &lt;= -2 \; | \; \mu=0,\sigma=2)\)</span>. <img src="compgenomr_files/figure-html/unnamed-chunk-136-1.png" width="672" /></p>
<p>In R, family of <code>*norm</code> functions (<code>rnorm</code>,<code>dnorm</code>,<code>qnorm</code> and <code>pnorm</code>) can be used to operate with normal distribution, such as calculating probabilities and generating random numbers drawn from normal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get the probability of P(X= -2) where mean=0 and sd=2</span>
<span class="kw">dnorm</span>(-<span class="dv">2</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.1209854</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get the probability of P(X =&lt; -2) where mean=0 and sd=2</span>
<span class="kw">pnorm</span>(-<span class="dv">2</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.1586553</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get the probability of P(X &gt; -2) where mean=0 and sd=2</span>
<span class="kw">pnorm</span>(-<span class="dv">2</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">2</span>,<span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>## [1] 0.8413447</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get 5 random numbers from normal dist with  mean=0 and sd=2</span>
<span class="kw">rnorm</span>(<span class="dv">5</span>, <span class="dt">mean=</span><span class="dv">0</span> , <span class="dt">sd=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] -1.8109030 -1.9220710 -0.5146717  0.8216728 -0.7900804</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get y value corresponding to P(X &gt; y) = 0.15 with  mean=0 and sd=2</span>
<span class="kw">qnorm</span>( <span class="fl">0.15</span>, <span class="dt">mean=</span><span class="dv">0</span> , <span class="dt">sd=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] -2.072867</code></pre>
<p>There are many other distribution functions in R that can be used the same way. You have to enter the distribution specific parameters along with your critical value, quantiles or number of random numbers depending on which function you are using in the family.We will list some of those functions below.</p>
<ul>
<li><p><code>dbinom</code> is for binomial distribution. This distribution is usually used to model fractional data and binary data. Examples from genomics includes methylation data.</p></li>
<li><p><code>dpois</code> is used for Poisson distribution and <code>dnbinom</code> is used for negative binomial distribution. These distributions are used to model count data such as sequencing read counts.</p></li>
<li><p><code>df</code> (F distribution) and <code>dchisq</code> (Chi-Squared distribution) are used in relation to distribution of variation. F distribution is used to model ratios of variation and Chi-Squared distribution is used to model distribution of variations. You will frequently encounter these in linear models and generalized linear models.</p></li>
</ul>
</div>
</div>
<div id="precision-of-estimates-confidence-intervals" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Precision of estimates: Confidence intervals</h3>
<p>When we take a random sample from a population and compute a statistic, such as the mean, we are trying to approximate the mean of the population. How well this sample statistic estimates the population value will always be a concern. A confidence interval addresses this concern because it provides a range of values which is plausible to contain the population parameter of interest. Normally, we would not have access to a population. If we did, we would not have to estimate the population parameters and its precision.</p>
<p>When we do not have access to the population, one way to estimate intervals is to repeatedly take samples from the original sample with replacement, that is we take a data point from the sample we replace, and we take another data point until we have sample size of the original sample. Then, we calculate the parameter of interest, in this case mean, and repeat this step a large number of times, such as 1000. At this point, we would have a distribution of re-sampled means, we can then calculate the 2.5th and 97.5th percentiles and these will be our so-called 95% confidence interval. This procedure, resampling with replacement to estimate the precision of population parameter estimates, is known as the <strong>bootstrap</strong>.</p>
<p>Let’s see how we can do this in practice. We simulate a sample coming from a normal distribution (but we pretend we don’t know the population parameters). We will try to estimate the precision of the mean of the sample using bootstrap to build confidence intervals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">21</span>)
<span class="kw">require</span>(mosaic)
sample1=<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dv">20</span>,<span class="dv">5</span>) <span class="co"># simulate a sample</span>

<span class="co"># do bootstrap resampling, sampling with replacement</span>
boot.means=<span class="kw">do</span>(<span class="dv">1000</span>) *<span class="st"> </span><span class="kw">mean</span>(<span class="kw">resample</span>(sample1))

<span class="co"># get percentiles from the bootstrap means</span>
q=<span class="kw">quantile</span>(boot.means[,<span class="dv">1</span>],<span class="dt">p=</span><span class="kw">c</span>(<span class="fl">0.025</span>,<span class="fl">0.975</span>))

<span class="co"># plot the histogram</span>
<span class="kw">hist</span>(boot.means[,<span class="dv">1</span>],<span class="dt">col=</span><span class="st">&quot;cornflowerblue&quot;</span>,<span class="dt">border=</span><span class="st">&quot;white&quot;</span>,
                    <span class="dt">xlab=</span><span class="st">&quot;sample means&quot;</span>,<span class="dt">main=</span><span class="st">&quot;1000 bootstrap means&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(q[<span class="dv">1</span>], q[<span class="dv">2</span>] ),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">text</span>(<span class="dt">x=</span>q[<span class="dv">1</span>],<span class="dt">y=</span><span class="dv">200</span>,<span class="kw">round</span>(q[<span class="dv">1</span>],<span class="dv">3</span>),<span class="dt">adj=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>))
<span class="kw">text</span>(<span class="dt">x=</span>q[<span class="dv">2</span>],<span class="dt">y=</span><span class="dv">200</span>,<span class="kw">round</span>(q[<span class="dv">2</span>],<span class="dv">3</span>),<span class="dt">adj=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>))</code></pre></div>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-138-1.png" width="672" /></p>
<p>If we had a convenient mathematical method to calculate confidence interval we could also do without resampling methods. It turns out that if we take repeated samples from a population of with sample size <span class="math inline">\(n\)</span>, the distribution of means ( <span class="math inline">\(\overline{X}\)</span>) of those samples will be approximately normal with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma/\sqrt{n}\)</span>. This is also known as <strong>Central Limit Theorem(CLT)</strong> and is one of the most important theorems in statistics. This also means that <span class="math inline">\(\frac{\overline{X}-\mu}{\sigma\sqrt{n}}\)</span> has a standard normal distribution and we can calculate the Z-score and then we can get the percentiles associated with the Z-score. Below, we are showing the Z-score calculation for the distribution of <span class="math inline">\(\overline{X}\)</span>, and then we are deriving the confidence intervals starting with the fact that probability of Z being between -1.96 and 1.96 is 0.95. We then use algebra to show that the probability that unknown <span class="math inline">\(\mu\)</span> is captured between <span class="math inline">\(\overline{X}-1.96\sigma\sqrt{n}\)</span> and <span class="math inline">\(\overline{X}+1.96\sigma\sqrt{n}\)</span> is 0.95, which is commonly known as 95% confidence interval.</p>
<p><span class="math display">\[
\begin{equation} 
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\\
P(-1.96 &lt; Z &lt; 1.96)=0.95 \\
P(-1.96 &lt; \frac{\overline{X}-\mu}{\sigma/\sqrt{n}} &lt; 1.96)=0.95\\
P(\mu-1.96\sigma\sqrt{n} &lt; \overline{X} &lt; \mu+1.96\sigma\sqrt{n})=0.95\\
P(\overline{X}-1.96\sigma\sqrt{n} &lt; \mu &lt; \overline{X}+1.96\sigma\sqrt{n})=0.95\\
confint=[\overline{X}-1.96\sigma\sqrt{n},\overline{X}+1.96\sigma\sqrt{n}]
\end{equation}
\]</span></p>
<p>A 95% confidence interval for population mean is the most common common interval to use, and would mean that we would expect 95% of the interval estimates to include the population parameter, in this case mean. However, we can pick any value such as 99% or 90%. We can generalize the confidence interval for <span class="math inline">\((1-\alpha)100%\)</span> as follows:</p>
<p><span class="math display">\[\overline{X} \pm Z_{\alpha/2}\sigma\sqrt{n}\]</span></p>
<p>In R, we can do this using <code>qnorm()</code> function to get Z-scores associated with <span class="math inline">\({\alpha/2}\)</span> and <span class="math inline">\({1-\alpha/2}\)</span>. As you can see, the confidence intervals we calculated using CLT are very similar to the ones we got from bootstrap for the same sample. For bootstrap we got <span class="math inline">\([19.21, 21.989]\)</span> and for the CLT based estimate we got <span class="math inline">\([19.23638, 22.00819]\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha=<span class="fl">0.05</span>
sd=<span class="dv">5</span>
n=<span class="dv">50</span>
<span class="kw">mean</span>(sample1)+<span class="kw">qnorm</span>(<span class="kw">c</span>(alpha/<span class="dv">2</span>,<span class="dv">1</span>-alpha/<span class="dv">2</span>))*sd/<span class="kw">sqrt</span>(n)</code></pre></div>
<pre><code>## [1] 19.23638 22.00819</code></pre>
<p>The good thing about CLT as long as the sample size is large regardless of the population distribution, the distribution of sample means drawn from that population will always be normal. Here we are repeatedly drawing samples 1000 times with sample size <span class="math inline">\(n\)</span>=10,30, and 100 from a bimodal, exponential and a uniform distribution and we are getting sample mean distributions following normal distribution. <img src="compgenomr_files/figure-html/unnamed-chunk-140-1.png" width="672" /></p>
<p>However, we should note that how we constructed the confidence interval using standard normal distribution, <span class="math inline">\(N(0,1)\)</span>, only works when the when we know the population standard deviation. In reality, we usually have only access to a sample and have no idea about the population standard deviation. If this is the case we should use estimate the standard deviation using sample standard deviation and use something called <em>t distribution</em> instead of standard normal distribution in our interval calculation. Our confidence interval becomes <span class="math inline">\(\overline{X} \pm t_{\alpha/2}s\sqrt{n}\)</span>, with t distribution parameter <span class="math inline">\(d.f=n-1\)</span>, since now the following quantity is t distributed <span class="math inline">\(\frac{\overline{X}-\mu}{s/\sqrt{n}}\)</span> instead of standard normal distribution.</p>
<p>The t distribution is similar to standard normal distribution has mean 0 but its spread is larger than the normal distribution especially when sample size is small, and has one parameter <span class="math inline">\(v\)</span> for the degrees of freedom, which is <span class="math inline">\(n-1\)</span> in this case. Degrees of freedom is simply number of data points minus number of parameters estimated. Here we are estimating the mean from the data and the distribution is for the means, therefore degrees of freedom is <span class="math inline">\(n-1\)</span>. <img src="compgenomr_files/figure-html/unnamed-chunk-141-1.png" width="672" /></p>
</div>
</div>
<div id="how-to-test-for-differences-in-samples" class="section level2">
<h2><span class="header-section-number">4.2</span> How to test for differences in samples</h2>
<p>Often times we would want to compare sets of samples. Such comparisons include if wild-type samples have different expression compared to mutants or if healthy samples are different from disease samples in some measurable feature (blood count, gene expression, methylation of certain loci). Since there is variability in our measurements, we need to take that into account when comparing the sets of samples. We can simply subtract the means of two samples, but given the variability of sampling, at the very least we need to decide a cutoff value for differences of means, small differences of means can be explained by random chance due to sampling. That means we need to compare the difference we get to a value that is typical to get if the difference between two group means were only due to sampling. If you followed the logic above, here we actually introduced two core ideas of something called “hypothesis testing”, this is simply using statistics to determine the probability that a given hypothesis (if two sample sets are from the same population or not) is true. Formally, those two core ideas are as follows:</p>
<ol style="list-style-type: decimal">
<li>Decide on a hypothesis to test, often called “null hypothesis” (<span class="math inline">\(H_0\)</span>). In our case, the hypothesis is there is no difference between sets of samples. An the “Alternative hypothesis” (<span class="math inline">\(H_1\)</span>) is there is a difference between the samples.</li>
<li>Decide on a statistic to test the truth of the null hypothesis.</li>
<li>Calculate the statistic</li>
<li>Compare it to a reference value to establish significance, the P-value. Based on that either accept or reject the null hypothesis, <span class="math inline">\(H_0\)</span></li>
</ol>
<div id="randomization-based-testing-for-difference-of-the-means" class="section level3">
<h3><span class="header-section-number">4.2.1</span> randomization based testing for difference of the means</h3>
<p>There is one intuitive way to go about this. If we believe there are no differences between samples that means the sample labels (test-control or healthy-disease) has no meaning. So, if we randomly assign labels to the samples that and calculate the difference of the mean, this creates a null distribution for the <span class="math inline">\(H_0\)</span> where we can compare the real difference and measure how unlikely it is to get such a value under the expectation of the null hypothesis. We can calculate all possible permutations to calculate the null distribution. However, sometimes that is not very feasible and equivalent approach would be generating the null distribution by taking a smaller number of random samples with shuffled group membership.</p>
<p>Below, we are doing this process in R. We are first simulating two samples from two different distributions. These would be equivalent to gene expression measurements obtained under different conditions. Then, we calculate the differences in the means and do the randomization procedure to get a null distribution when we assume there is no difference between samples, <span class="math inline">\(H_0\)</span>. We than calculate how often we would get the original difference we calculated under the assumption that <span class="math inline">\(H_0\)</span> is true.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>)
gene1=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">4</span>,<span class="dt">sd=</span><span class="dv">2</span>)
gene2=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">2</span>,<span class="dt">sd=</span><span class="dv">2</span>)
org.diff=<span class="kw">mean</span>(gene1)-<span class="kw">mean</span>(gene2)
gene.df=<span class="kw">data.frame</span>(<span class="dt">exp=</span><span class="kw">c</span>(gene1,gene2),
                  <span class="dt">group=</span><span class="kw">c</span>( <span class="kw">rep</span>(<span class="st">&quot;test&quot;</span>,<span class="dv">30</span>),<span class="kw">rep</span>(<span class="st">&quot;control&quot;</span>,<span class="dv">30</span>) ) )


exp.null &lt;-<span class="st"> </span><span class="kw">do</span>(<span class="dv">1000</span>) *<span class="st"> </span><span class="kw">diff</span>(<span class="kw">mean</span>(exp ~<span class="st"> </span><span class="kw">shuffle</span>(group), <span class="dt">data=</span>gene.df))
<span class="kw">hist</span>(exp.null[,<span class="dv">1</span>],<span class="dt">xlab=</span><span class="st">&quot;null distribution | no difference in samples&quot;</span>,
     <span class="dt">main=</span><span class="kw">expression</span>(<span class="kw">paste</span>(H[<span class="dv">0</span>],<span class="st">&quot; :no difference in means&quot;</span>) ),
     <span class="dt">xlim=</span><span class="kw">c</span>(-<span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">col=</span><span class="st">&quot;cornflowerblue&quot;</span>,<span class="dt">border=</span><span class="st">&quot;white&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">quantile</span>(exp.null[,<span class="dv">1</span>],<span class="fl">0.95</span>),<span class="dt">col=</span><span class="st">&quot;red&quot;</span> )
<span class="kw">abline</span>(<span class="dt">v=</span>org.diff,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span> )
<span class="kw">text</span>(<span class="dt">x=</span><span class="kw">quantile</span>(exp.null[,<span class="dv">1</span>],<span class="fl">0.95</span>),<span class="dt">y=</span><span class="dv">200</span>,<span class="st">&quot;0.05&quot;</span>,<span class="dt">adj=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">text</span>(<span class="dt">x=</span>org.diff,<span class="dt">y=</span><span class="dv">200</span>,<span class="st">&quot;org. diff.&quot;</span>,<span class="dt">adj=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>),<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</code></pre></div>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-142-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p.val=<span class="kw">sum</span>(exp.null[,<span class="dv">1</span>]&gt;org.diff)/<span class="kw">length</span>(exp.null[,<span class="dv">1</span>])
p.val</code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>After doing random permutations and getting a null distribution, it is possible to get a confidence interval for the distribution of difference in means. This is simply the 2.5th and 97.5th percentiles of the null distribution, and directly related to the P-value calculation above.</p>
</div>
<div id="using-t-test-for-difference-of-the-means-between-two-samples" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Using t-test for difference of the means between two samples</h3>
<p>We can also calculate the difference between means using a t-test. Sometimes we will have too few data points in a sample to do meaningful randomization test, also randomization takes more time than doing a t-test. This is a test that depends on t distribution. The line of thought follows from the CLT and we can show differences in means are t distributed. There are couple of variants of the t-test for this purpose. If we assume the variances are equal we can use the following version</p>
<p><span class="math display">\[t = \frac{\bar {X}_1 - \bar{X}_2}{s_{X_1X_2} \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\]</span> where <span class="math display">\[s_{X_1X_2} = \sqrt{\frac{(n_1-1)s_{X_1}^2+(n_2-1)s_{X_2}^2}{n_1+n_2-2}}\]</span> In the first equation above the quantity is t distributed with <span class="math inline">\(n_1+n_2-2\)</span> degrees of freedom. We can calculate the quantity then use software to look for the percentile of that value in that t distribution, which is our P-value. When we can not assume equal variances we use “Welch’s t-test” which is the default t-test in R and also works well when variances and the sample sizes are the same. For this test we calculate the following quantity:</p>
<p><span class="math display">\[t = {\overline{X}_1 - \overline{X}_2 \over s_{\overline{X}_1 - \overline{X}_2}}\]</span> where <span class="math display">\[s_{\overline{X}_1 - \overline{X}_2} = \sqrt{{s_1^2 \over n_1} + {s_2^2  \over n_2}}
\]</span> and the degrees of freedom equals to: <span class="math display">\[\mathrm{d.f.} = \frac{(s_1^2/n_1 + s_2^2/n_2)^2}{(s_1^2/n_1)^2/(n_1-1) + (s_2^2/n_2)^2/(n_2-1)}
\]</span></p>
<p>Luckily, R does all those calculations for us. Below we will show the use of <code>t.test()</code> function in R. We will use it on the samples we simulated above.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Welch&#39;s t-test</span>
stats::<span class="kw">t.test</span>(gene1,gene2)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  gene1 and gene2
## t = 3.7653, df = 47.552, p-value = 0.0004575
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.872397 2.872761
## sample estimates:
## mean of x mean of y 
##  4.057728  2.185149</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># t-test with equal varience assumption</span>
stats::<span class="kw">t.test</span>(gene1,gene2,<span class="dt">var.equal=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  gene1 and gene2
## t = 3.7653, df = 58, p-value = 0.0003905
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.8770753 2.8680832
## sample estimates:
## mean of x mean of y 
##  4.057728  2.185149</code></pre>
<p>A final word on t-tests: they generally assume population where samples coming from have normal distribution, however it is been shown t-test can tolerate deviations from normality. Especially, when two distributions are moderately skewed in the same direction. This is due to central limit theorem which says means of samples will be distributed normally no matter the population distribution if sample sizes are large.</p>
</div>
<div id="multiple-testing-correction" class="section level3">
<h3><span class="header-section-number">4.2.3</span> multiple testing correction</h3>
<p>We should think of hypothesis testing as a non-error-free method of making decisions. There will be times when we declare something significant and accept <span class="math inline">\(H_1\)</span> but we will be wrong. These decisions are also called “false positives” or “false discoveries”, this is also known as “type I error”. Similarly, we can fail to reject a hypothesis when we actually should. These cases are known as “false negatives”, also known as “type II error”.</p>
<p>The ratio of true negatives to the sum of true negatives and false positives (<span class="math inline">\(\frac{TN}{FP+TN}\)</span>) is known as specificity. And we usually want to decrease the FP and get higher specificity. The ratio of true positives to the sum of true positives and false negatives (<span class="math inline">\(\frac{TP}{TP+FN}\)</span>) is known as sensitivity. And, again we usually want to decrease the FN and get higher sensitivity. Sensitivity is also known as “power of a test” in the context of hypothesis testing. More powerful tests will be highly sensitive and will do less type II errors. For the t-test the power is positively associated with sample size and the effect size. Higher the sample size, smaller the standard error and looking for the larger effect sizes will similarly increase the power.</p>
<p>The general summary of these the different combination of the decisions are included in the table below.</p>
<table>
<colgroup>
<col width="19%" />
<col width="25%" />
<col width="25%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center"><span class="math inline">\(H_0\)</span> is TRUE, [Gene is NOT differentially expressed]</th>
<th align="center"><span class="math inline">\(H_1\)</span> is TRUE, [Gene is differentially expressed]</th>
<th align="left"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Accept <span class="math inline">\(H_0\)</span> (claim that the gene is not differentially expressed)</td>
<td align="center">True Negatives (TN)</td>
<td align="center">False Negatives (FN) ,type II error</td>
<td align="left"><span class="math inline">\(m_0\)</span>: number of truly null hypotheses</td>
</tr>
<tr class="even">
<td align="left">reject <span class="math inline">\(H_0\)</span> (claim that the gene is differentially expressed)</td>
<td align="center">False Positives (FP) ,type I error</td>
<td align="center">True Positives (TP)</td>
<td align="left"><span class="math inline">\(m-m_0\)</span>: number of truly alternative hypotheses</td>
</tr>
</tbody>
</table>
<p>We expect to make more type I errors as the number of tests increase, that means we will reject the null hypothesis by mistake. For example, if we perform a test the 5% significance level, there is a 5% chance of incorrectly rejecting the null hypothesis if the null hypothesis is true. However, if we make 1000 tests where all null hypotheses are true for each of them, the average number of incorrect rejections is 50. And if we apply the rules of probability, there are is almost a 100% chance that we will have at least one incorrect rejection. There are multiple statistical techniques to prevent this from happening. These techniques generally shrink the P-values obtained from multiple tests to higher values, if the individual P-value is low enough it survives this process. The most simple method is just to multiply the individual, P-value (<span class="math inline">\(p_i\)</span>) with the number of tests (<span class="math inline">\(m\)</span>): <span class="math inline">\(m \cdot p_i\)</span>, this is called “Bonferroni correction”. However, this is too harsh if you have thousands of tests. Other methods are developed to remedy this. Those methods rely on ranking the P-values and dividing <span class="math inline">\(m \cdot p_i\)</span> by the rank,<span class="math inline">\(i\)</span>, :<span class="math inline">\(\frac{m \cdot p_i }{i}\)</span>, this is derived from Benjamini–Hochberg procedure. This procedure is developed to control for “False Discovery Rate (FDR)” , which is proportion of false positives among all significant tests. And in practical terms, we get the “FDR adjusted P-value” from the procedure described above. This gives us an estimate of proportion of false discoveries for a given test. To elaborate, p-value of 0.05 implies that 5% of all tests will be false positives. An FDR adjusted p-value of 0.05 implies that 5% of significant tests will be false positives. The FDR adjusted P-values will result in a lower number of false positives.</p>
<p>One final method that is also popular is called the “q-value” method and related to the method above. This procedure relies on estimating the proportion of true null hypotheses from the distribution of raw p-values and using that quantity to come up with what is called a “q-value”, which is also an FDR adjusted P-value . That can be practically defined as “the proportion of significant features that turn out to be false leads.” A q-value 0.01 would mean 1% of the tests called significant at this level will be truly null on average. Within the genomics community q-value and FDR adjusted P-value are synonymous although they can be calculated differently.</p>
<p>In R, the base function <code>p.adjust()</code> implements most of the p-value correction methods described above. For the q-value, we can use the <code>qvalue</code> package from Bioconductor. Below we are demonstrating how to use them on a set of simulated p-values.The plot shows that Bonferroni correction does a terrible job. FDR(BH) and q-value approach are better but q-value approach is more permissive than FDR(BH).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(qvalue)
<span class="kw">data</span>(hedenfalk)

qvalues &lt;-<span class="st"> </span><span class="kw">qvalue</span>(hedenfalk$p)$q
bonf.pval=<span class="kw">p.adjust</span>(hedenfalk$p,<span class="dt">method =</span><span class="st">&quot;bonferroni&quot;</span>)
fdr.adj.pval=<span class="kw">p.adjust</span>(hedenfalk$p,<span class="dt">method =</span><span class="st">&quot;fdr&quot;</span>)

<span class="kw">plot</span>(hedenfalk$p,qvalues,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),
     <span class="dt">xlab=</span><span class="st">&quot;raw P-values&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;adjusted P-values&quot;</span>)
<span class="kw">points</span>(hedenfalk$p,bonf.pval,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">points</span>(hedenfalk$p,fdr.adj.pval,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>,<span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;q-value&quot;</span>,<span class="st">&quot;FDR (BH)&quot;</span>,<span class="st">&quot;Bonferroni&quot;</span>),
       <span class="dt">fill=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>))</code></pre></div>
<p><img src="compgenomr_files/figure-html/multtest-1.png" width="672" /></p>
</div>
<div id="moderated-t-tests-using-information-from-multiple-comparisons" class="section level3">
<h3><span class="header-section-number">4.2.4</span> moderated t-tests: using information from multiple comparisons</h3>
<p>In genomics, we usually do not do one test but many, as described above. That means we may be able to use the information from the parameters obtained from all comparisons to influence the individual parameters. For example, if you have many variances calculated for thousands of genes across samples, you can force individual variance estimates to shrunk towards the mean or the median of the distribution of variances. This usually creates better performance in individual variance estimates and therefore better performance in significance testing which depends on variance estimates. How much the values be shrunk towards a common value comes in many flavors. These tests in general are called moderated t-tests or shrinkage t-tests. One approach popularized by Limma software is to use so-called “Empirical Bayesian methods”. The main formulation in these methods is <span class="math inline">\(\hat{V_g} = aV_0 + bV_g\)</span>, where <span class="math inline">\(V_0\)</span> is the background variability<br />
and <span class="math inline">\(V_g\)</span> is the individual variability. Then, these methods estimate <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> in various ways to come up with shrunk version of variability, <span class="math inline">\(\hat{V_g}\)</span>. In a Bayesian viewpoint, the prior knowledge is used to calculate the variability of an individual gene. In this case, <span class="math inline">\(V_0\)</span> would be the prior knowledge we have on variability of the genes and we use that knowledge to influence our estimate for the individual genes.</p>
<p>Below we are simulating a gene expression matrix with 1000 genes, and 3 test and 3 control groups. Each row is a gene and in normal circumstances we would like to find out differentially expressed genes. In this case, we are simulating them from the same distribution so in reality we do not expect any differences. We then use the adjusted standard error estimates in empirical Bayesian spirit but in a very crude way. We just shrink the gene-wise standard error estimates towards the median with equal <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> weights. That is to say, we add individual estimate to the median of standard error distribution from all genes and divide that quantity by 2. So if we plug that in the to the above formula what we do is:</p>
<p><span class="math display">\[ \hat{V_g} = (V_0 + V_g)/2 \]</span></p>
<p>In the code below, we are avoiding for loops or apply family functions by using vectorized operations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>)

<span class="co">#sample data matrix from normal distribution</span>

gset=<span class="kw">rnorm</span>(<span class="dv">3000</span>,<span class="dt">mean=</span><span class="dv">200</span>,<span class="dt">sd=</span><span class="dv">70</span>)
data=<span class="kw">matrix</span>(gset,<span class="dt">ncol=</span><span class="dv">6</span>)

<span class="co"># set groups</span>
group1=<span class="dv">1</span>:<span class="dv">3</span>
group2=<span class="dv">4</span>:<span class="dv">6</span>
n1=<span class="dv">3</span>
n2=<span class="dv">3</span>
dx=<span class="kw">rowMeans</span>(data[,group1])-<span class="kw">rowMeans</span>(data[,group2])
  
<span class="kw">require</span>(matrixStats)

<span class="co"># get the esimate of pooled variance </span>
stderr &lt;-<span class="st"> </span><span class="kw">sqrt</span>( (<span class="kw">rowVars</span>(data[,group1])*(n1<span class="dv">-1</span>) +<span class="st"> </span><span class="kw">rowVars</span>(data[,group2])*(n2<span class="dv">-1</span>)) /<span class="st"> </span>(n1+n2<span class="dv">-2</span>) *<span class="st"> </span>( <span class="dv">1</span>/n1 +<span class="st"> </span><span class="dv">1</span>/n2 ))

<span class="co"># do the shrinking towards median</span>
mod.stderr &lt;-<span class="st"> </span>(stderr +<span class="st"> </span><span class="kw">median</span>(stderr)) /<span class="st"> </span><span class="dv">2</span> <span class="co"># moderation in variation</span>

<span class="co"># esimate t statistic with moderated variance</span>
t.mod =<span class="st"> </span>dx /<span class="st"> </span>mod.stderr

<span class="co"># calculate P-value of rejecting null </span>
p.mod =<span class="st"> </span><span class="dv">2</span>*<span class="kw">pt</span>( -<span class="kw">abs</span>(t.mod), n1+n2<span class="dv">-2</span> )

<span class="co"># esimate t statistic without moderated variance</span>
t =<span class="st"> </span>dx /<span class="st"> </span>stderr

<span class="co"># calculate P-value of rejecting null </span>
p =<span class="st"> </span><span class="dv">2</span>*<span class="kw">pt</span>( -<span class="kw">abs</span>(t), n1+n2<span class="dv">-2</span> )

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">hist</span>(p,<span class="dt">col=</span><span class="st">&quot;cornflowerblue&quot;</span>,<span class="dt">border=</span><span class="st">&quot;white&quot;</span>,<span class="dt">main=</span><span class="st">&quot;&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;P-values t-test&quot;</span>)
<span class="kw">mtext</span>(<span class="kw">paste</span>(<span class="st">&quot;signifcant tests:&quot;</span>,<span class="kw">sum</span>(p&lt;<span class="fl">0.05</span>))  )
<span class="kw">hist</span>(p.mod,<span class="dt">col=</span><span class="st">&quot;cornflowerblue&quot;</span>,<span class="dt">border=</span><span class="st">&quot;white&quot;</span>,<span class="dt">main=</span><span class="st">&quot;&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;P-values mod. t-test&quot;</span>)
<span class="kw">mtext</span>(<span class="kw">paste</span>(<span class="st">&quot;signifcant tests:&quot;</span>,<span class="kw">sum</span>(p.mod&lt;<span class="fl">0.05</span>))  )</code></pre></div>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-143-1.png" width="672" /></p>
</div>
<div id="want-to-know-more" class="section level3">
<h3><span class="header-section-number">4.2.5</span> Want to know more…</h3>
<ul>
<li>basic statistical concepts
<ul>
<li>“Cartoon guide to statistics” by Gonick &amp; Smith</li>
<li>“Introduction to statistics” by Mine Rundel, et al. (Free e-book)</li>
</ul></li>
<li>Hands-on statistics recipes with R
<ul>
<li>“The R book” by Crawley</li>
</ul></li>
<li>moderated tests
<ul>
<li>comparison of moderated tests for differential expression <a href="http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-17" class="uri">http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-17</a></li>
<li>limma method: Smyth, G. K. (2004). Linear models and empirical Bayes methods for assessing differential expression in microarray experiments. Statistical Applications in Genetics and Molecular Biology, 3, No. 1, Article 3. <a href="http://www.statsci.org/smyth/pubs/ebayes.pdf" class="uri">http://www.statsci.org/smyth/pubs/ebayes.pdf</a></li>
</ul></li>
</ul>
</div>
</div>
<div id="relationship-between-variables-linear-models-and-correlation" class="section level2">
<h2><span class="header-section-number">4.3</span> Relationship between variables: linear models and correlation</h2>
<p>In genomics, we would often need to measure or model the relationship between variables. We might want to know about expression of a particular gene in liver in relation to the dosage of a drug that patient receives. Or, we may want to know DNA methylation of certain locus in the genome in relation to age of the sample donor’s. Or, we might be interested in the relationship between histone modifications and gene expression. Is there a linear relationship, the more histone modification the more the gene is expressed ?</p>
<p>In these situations and many more, linear regression or linear models can be used to model the relationship with a “dependent” or “response” variable (expression or methylation in the above examples) and one or more “independent”&quot; or “explanatory” variables (age, drug dosage or histone modification in the above examples). Our simple linear model has the following components.</p>
<p><span class="math display">\[  Y= \beta_0+\beta_1X + \epsilon \]</span></p>
<p>In the equation above, <span class="math inline">\(Y\)</span> is the response variable and <span class="math inline">\(X\)</span> is the explanatory variable. <span class="math inline">\(\epsilon\)</span> is the mean-zero error term. Since, the line fit will not be able to precisely predict the <span class="math inline">\(Y\)</span> values, there will be some error associated with each prediction when we compare it to the original <span class="math inline">\(Y\)</span> values. This error is captured in <span class="math inline">\(\epsilon\)</span> term. We can alternatively write the model as follows to emphasize that the model approximates <span class="math inline">\(Y\)</span>, in this case notice that we removed the <span class="math inline">\(\epsilon\)</span> term: <span class="math inline">\(Y \sim \beta_0+\beta_1X\)</span></p>
<p>The graph below shows the relationship between histone modification (trimethylated forms of histone H3 at lysine 4, aka H3K4me3) and gene expression for 100 genes. The blue line is our model with estimated coefficients (<span class="math inline">\(\hat{y}=\hat{\beta}_0 + \hat{\beta}_1X\)</span>, where <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> the estimated values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\hat{y}\)</span> indicates the prediction). The red lines indicate the individual errors per data point, indicated as <span class="math inline">\(\epsilon\)</span> in the formula above.</p>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-144-1.png" width="672" /></p>
<p>There could be more than one explanatory variable, we then simply add more <span class="math inline">\(X\)</span> and <span class="math inline">\(\beta\)</span> to our model. If there are two explanatory variables our model will look like this:</p>
<p><span class="math display">\[  Y= \beta_0+\beta_1X_1 +\beta_2X_2 + \epsilon \]</span></p>
<p>In this case, we will be fitting a plane rather than a line. However, the fitting process which we will describe in the later sections will not change. For our gene expression problem. We can introduce one more histone modification, H3K27me3. We will then have a linear model with 2 explanatory variables and the fitted plane will look like the one below. The gene expression values are shown as dots below and above the fitted plane.</p>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-145-1.png" width="672" /></p>
<div id="matrix-notation-for-linear-models" class="section level4">
<h4><span class="header-section-number">4.3.0.1</span> Matrix notation for linear models</h4>
<p>We can naturally have more explanatory variables than just two.The formula below has <span class="math inline">\(n\)</span> explanatory variables.</p>
<p><span class="math display">\[Y= \beta_0+\beta_1X_1+\beta_2X_2 +  \beta_3X_3 + .. + \beta_nX_n +\epsilon\]</span></p>
<p>If there are many variables, it would be easier to write the model in matrix notation. The matrix form of linear model with two explanatory variables will look like the one below. First matrix would be our data matrix. This contains our explanatory variables and a column of 1s. The second term is a column vector of <span class="math inline">\(\beta\)</span> values. We add a vector of error terms,<span class="math inline">\(\epsilon\)</span>s to the matrix multiplication.</p>
<p><span class="math display">\[\mathbf{Y} = \left[\begin{array}
{r,r,r}
1 &amp; X_{1,1} &amp; X_{1,2} \\
1 &amp; X_{2,1} &amp; X_{2,2} \\
1 &amp; X_{3,1} &amp; X_{3,2} \\
1 &amp; X_{4,1} &amp; X_{4,2}
\end{array}\right]
%
\left[\begin{array}
{r,r,r}
\beta_0 \\
\beta_1 \\
\beta_2 
\end{array}\right]
% 
+
\left[\begin{array}
{r,r,r}
\epsilon_1 \\
\epsilon_2 \\ 
\epsilon_3 \\ 
\epsilon_0
\end{array}\right]
\]</span></p>
<p>The multiplication of data matrix and <span class="math inline">\(\beta\)</span> vector and addition of the error terms simply results in the the following set of equations per data point:</p>
<p><span class="math display">\[
\begin{align}
Y_1= \beta_0+\beta_1X_{1,1}+\beta_2X_{1,2} +\epsilon_1 \\
Y_2= \beta_0+\beta_1X_{2,1}+\beta_2X_{2,2} +\epsilon_2 \\
Y_3= \beta_0+\beta_1X_{3,1}+\beta_2X_{3,2} +\epsilon_3 \\
Y_4= \beta_0+\beta_1X_{4,1}+\beta_2X_{4,2} +\epsilon_4 
\end{align}
\]</span></p>
<p>This expression involving the multiplication of the data matrix, the <span class="math inline">\(\beta\)</span> vector and vector of error terms (<span class="math inline">\(\epsilon\)</span>) could be simply written as follows.</p>
<p><span class="math display">\[Y=X\beta + \epsilon\]</span></p>
<p>In the equation above <span class="math inline">\(Y\)</span> is the vector of response variables and <span class="math inline">\(X\)</span> is the data matrix and <span class="math inline">\(\beta\)</span> is the vector of coefficients. This notation is more concise and often used in scientific papers. However, this also means you need some understanding of linear algebra to follow the math laid out in such resources.</p>
</div>
<div id="how-to-fit-a-line" class="section level3">
<h3><span class="header-section-number">4.3.1</span> How to fit a line</h3>
<p>At this point a major questions is left unanswered: How did we fit this line? We basically need to define <span class="math inline">\(\beta\)</span> values in a structured way. There are multiple ways or understanding how to do this, all of which converges to the same end point. We will describe them one by one.</p>
<div id="the-cost-or-loss-function-approach" class="section level4">
<h4><span class="header-section-number">4.3.1.1</span> The cost or loss function approach</h4>
<p>This is the first approach and in my opinion is easiest to understand. We try to optimize a function, often called “cost function” or “loss function”. The cost function is the sum of squared differences between the predicted <span class="math inline">\(\hat{Y}\)</span> values from our model and the original <span class="math inline">\(Y\)</span> values. The optimization procedure tries to find <span class="math inline">\(\beta\)</span> values that minimizes this difference between reality and the predicted values.</p>
<p><span class="math display">\[min \sum{(y_i-(\beta_0+\beta_1x_i))^2}\]</span></p>
<p>Note that this is related to the the error term, <span class="math inline">\(\epsilon\)</span>, we already mentioned above, we are trying to minimize the squared sum of <span class="math inline">\(\epsilon_i\)</span> for each data point. We can do this minimization by a bit of calculus. The rough algorithm is as follows:</p>
<ol style="list-style-type: decimal">
<li>Pick a random starting point, random <span class="math inline">\(\beta\)</span> values</li>
<li>Take the partial derivatives of the cost function to see which direction is the way to go in the cost function.</li>
<li>Take a step toward the direction that minimizes the cost function.
<ul>
<li>step size is parameter to choose, there are many variants.</li>
</ul></li>
<li>repeat step 2,3 until convergence.</li>
</ol>
<p>This is the basis of “gradient descent” algorithm. With the help of partial derivatives we define a “gradient” on the cost function and follow that through multiple iterations and until convergence, meaning until the results do not improve defined by a margin. The algorithm usually converges to optimum <span class="math inline">\(\beta\)</span> values. Below, we show the cost function over various <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> values for the histone modification and gene expression data set. The algorithm will pick a point on this graph and traverse it incrementally based on the derivatives and converge on the bottom of the cost function “well”.</p>
<p>-://..com/watch?v=5Q</p>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-146-1.png" width="672" /></p>
</div>
<div id="not-cost-function-but-maximum-likelihood-function" class="section level4">
<h4><span class="header-section-number">4.3.1.2</span> Not cost function but maximum likelihood function</h4>
<p>We can also think of this problem from more a statistical point of view. In essence, we are looking for best statistical parameters, in this case <span class="math inline">\(\beta\)</span> values, for our model that are most likely to produce such a scatter of data points given the explanatory variables.This is called “Maximum likelihood” approach. Probability of observing a <span class="math inline">\(Y\)</span> value, given that the distribution of it on a given <span class="math inline">\(X\)</span> value follows a normal distribution with mean <span class="math inline">\(\beta_0+\beta_1x_i\)</span> and variance <span class="math inline">\(s^2\)</span> , and is shown below. Note that this assumes variance is constant and <span class="math inline">\(s^2=\frac{\sum{\epsilon_i}}{n-2}\)</span> is an unbiased estimation for population variance, <span class="math inline">\(\sigma^2\)</span>.</p>
<p><span class="math display">\[P(y_{i})=\frac{1}{s\sqrt{2\pi} }e^{-\frac{1}{2}\left(\frac{y_i-(\beta_0 + \beta_1x_i)}{s}\right)^2}\]</span></p>
<p>Following from this, then the likelihood function ,shown as <span class="math inline">\(L\)</span> below, for linear regression is multiplication of <span class="math inline">\(P(y_{i})\)</span> for all data points.</p>
<p><span class="math display">\[L=P(y_1)P(y_2)P(y_3)..P(y_n)=\prod\limits_{i=1}^n{P_i}\]</span></p>
<p>This can be simplified to this by some algebra and taking logs (since it is easier to add than multiply)</p>
<p><span class="math display">\[ln(L) = -nln(s\sqrt{2\pi}) - \frac{1}{2s^2} \sum\limits_{i=1}^n{(y_i-(\beta_0 - \beta_1x_i))^2} \]</span></p>
<p>As you can see, the right part of the function is the negative of the cost function defined above. If we wanted to optimize this function we would need to take derivative of the function with respect to <span class="math inline">\(\beta\)</span> parameters. That means we can ignore the first part since there is no <span class="math inline">\(\beta\)</span> terms there. This simply reduces to the negative of the cost function. Hence, this approach produces exactly the same result as the cost function approach. The difference is that we defined our problem within the domain of statistics. This particular function has still to be optimized. This can be done with some calculus without the need for an iterative approach.</p>
</div>
<div id="linear-algebra-and-closed-form-solution-to-linear-regression" class="section level4">
<h4><span class="header-section-number">4.3.1.3</span> Linear algebra and closed-form solution to linear regression</h4>
<p>The last approach we will describe is the minimization process using linear algebra. But in this case, we do not use an iterative approach. Instead, we will minimize cost function by explicitly taking its derivatives with respect to <span class="math inline">\(\beta\)</span>’s and setting them to zero. This is doable by employing linear algebra and matrix calculus. This approach is also called “ordinary least squares”. We will not show the whole derivation here but the following expression is what we are trying to minimize in matrix notation, this is basically a different notation of the same minimization problem defined above. Remember <span class="math inline">\(\epsilon_i=Y_i-(\beta_0+\beta_1x_i)\)</span></p>
<p><span class="math display">\[
\begin{align}
\sum\epsilon_{i}^2=\epsilon^T\epsilon=(Y-{\beta}{X})^T(Y-{\beta}{X}) \\
=Y^T{Y}-2{\beta}^T{Y}+{\beta}^TX^TX{\beta}
\end{align}
\]</span> After rearranging the terms, we take the derivative of <span class="math inline">\(\epsilon^T\epsilon\)</span> with respect to <span class="math inline">\(\beta\)</span>, and equalize that to zero. We then arrive at the following for estimated <span class="math inline">\(\beta\)</span> values, <span class="math inline">\(\hat{\beta}\)</span>:</p>
<p><span class="math display">\[\hat{\beta}=(X^TX)^{-1}X^TY\]</span></p>
<p>This requires for you to calculate the inverse of the <span class="math inline">\(X^TX\)</span> term, which could be slow for large matrices. Iterative approach over the cost function derivatives will be faster for larger problems. The linear algebra notation is something you will see in the papers or other resources often. If you input the data matrix X and solve the <span class="math inline">\((X^TX)^{-1}\)</span> , you get the following values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> for simple regression . However, we should note that this simple linear regression case can easily be solved algebraically without the need for matrix operations. This can be done by taking the derivative of <span class="math inline">\(\sum{(y_i-(\beta_0+\beta_1x_i))^2}\)</span> with respect to <span class="math inline">\(\beta_1\)</span>, rearranging the terms and equalizing the derivative to zero.</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum{(x_i-\overline{X})(y_i-\overline{Y})}}{ \sum{(x_i-\overline{X})^2} }\]</span> <span class="math display">\[\hat{\beta_0}=\overline{Y}-\hat{\beta_1}\overline{X}\]</span></p>
</div>
<div id="fitting-lines-in-r" class="section level4">
<h4><span class="header-section-number">4.3.1.4</span> Fitting lines in R</h4>
<p>After all this theory, you will be surprised how easy it is to fit lines in R. This is achieved just by <code>lm()</code> command, stands for linear models. Let’s do this for a simulated data set and plot the fit. First step is to simulate the data, we will decide on <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> values. The we will decide on the variance parameter,<span class="math inline">\(\sigma\)</span> to be used in simulation of error terms, <span class="math inline">\(\epsilon\)</span>. We will first find <span class="math inline">\(Y\)</span> values, just using the linear equation <span class="math inline">\(Y=\beta0+\beta_1X\)</span>, for a set of <span class="math inline">\(X\)</span> values. Then, we will add the error terms get our simulated values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random number seed, so that the random numbers from the text</span>
<span class="co"># is the same when you run the code.</span>
<span class="kw">set.seed</span>(<span class="dv">32</span>)

<span class="co"># get 50 X values between 1 and 100</span>
x =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">50</span>,<span class="dv">1</span>,<span class="dv">100</span>)

<span class="co"># set b0,b1 and varience (sigma)</span>
b0 =<span class="st"> </span><span class="dv">10</span>
b1 =<span class="st"> </span><span class="dv">2</span>
sigma =<span class="st"> </span><span class="dv">20</span>
<span class="co"># simulate error terms from normal distribution</span>
eps =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dv">0</span>,sigma)
<span class="co"># get y values from the linear equation and addition of error terms</span>
y =<span class="st"> </span>b0 +<span class="st"> </span>b1*x+<span class="st"> </span>eps</code></pre></div>
<p>Now let us fit a line using lm() function. The function requires a formula, and optionally a data frame. We need the pass the following expression within the lm function, <code>y~x</code>, where <code>y</code> is the simulated <span class="math inline">\(Y\)</span> values and <code>x</code> is the explanatory variables <span class="math inline">\(X\)</span>.We will then use <code>abline()</code> function to draw the fit.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1=<span class="kw">lm</span>(y~x)

<span class="co"># plot the data points</span>
<span class="kw">plot</span>(x,y,<span class="dt">pch=</span><span class="dv">20</span>,
     <span class="dt">ylab=</span><span class="st">&quot;Gene Expression&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Histone modification score&quot;</span>)
<span class="co"># plot the linear fit</span>
<span class="kw">abline</span>(mod1,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</code></pre></div>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-148-1.png" width="672" /></p>
</div>
</div>
<div id="how-to-estimate-the-error-of-the-coefficients" class="section level3">
<h3><span class="header-section-number">4.3.2</span> How to estimate the error of the coefficients</h3>
<p>Since we are using a sample to estimate the coefficients they are not exact, with every random sample they will vary. Below, we are taking multiple samples from the population and fitting lines to each sample, with each sample the lines slightly change.We are overlaying the points and the lines for each sample on top of the other samples .When we take 200 samples and fit lines for each of them,the lines fit are variable. And, we get a normal-like distribution of <span class="math inline">\(\beta\)</span> values with a defined mean and standard deviation a, which is called standard error of the coefficients.</p>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-149-1.png" width="672" /></p>
<p>As usually we will not have access to the population to do repeated sampling, model fitting and estimation of the standard error for the coefficients. But there is statistical theory that helps us infer the population properties from the sample. When we assume that error terms have constant variance and mean zero , we can model the uncertainty in the regression coefficients, <span class="math inline">\(\beta\)</span>s. The estimates for standard errors of <span class="math inline">\(\beta\)</span>s for simple regression are as follows and shown without derivation.</p>
<p><span class="math display">\[
\begin{align}
s=RSE=\sqrt{\frac{\sum{(y_i-(\beta_0+\beta_1x_i))^2}}{n-2}  } =\sqrt{\frac{\sum{\epsilon^2}}{n-2}  } \\
SE(\hat{\beta_1})=\frac{s}{\sqrt{\sum{(x_i-\overline{X})^2}}} \\
SE(\hat{\beta_0})=s\sqrt{ \frac{1}{n} + \frac{\overline{X}^2}{\sum{(x_i-\overline{X})^2} }  }
\end{align}
\]</span></p>
<p>Notice that that <span class="math inline">\(SE(\beta_1)\)</span> depends on the estimate of variance of residuals shown as <span class="math inline">\(s\)</span> or <strong>Residual Standard Error (RSE)</strong>. Notice alsos standard error depends on the spread of <span class="math inline">\(X\)</span>. If <span class="math inline">\(X\)</span> values have more variation, the standard error will be lower. This intuitively makes sense since if the spread of the <span class="math inline">\(X\)</span> is low, the regression line will be able to wiggle more compared to a regression line that is fit to the same number of points but covers a greater range on the X-axis.</p>
<p>The standard error estimates can also be used to calculate confidence intervals and test hypotheses, since the following quantity called t-score approximately follows a t-distribution with <span class="math inline">\(n-p\)</span> degrees of freedom, where <span class="math inline">\(n\)</span> is the number of data points and <span class="math inline">\(p\)</span> is the number of coefficients estimated.</p>
<p><span class="math display">\[ \frac{\hat{\beta_i}-\beta_test}{SE(\hat{\beta_i})}\]</span></p>
<p>Often, we would like to test the null hypothesis if a coefficient is equal to zero or not. For simple regression this could mean if there is a relationship between explanatory variable and response variable. We would calculate the t-score as follows <span class="math inline">\(\frac{\hat{\beta_i}-0}{SE(\hat{\beta_i})}\)</span>, and compare it t-distribution with <span class="math inline">\(d.f.=n-p\)</span> to get the p-value.</p>
<p>We can also calculate the uncertainty of the regression coefficients using confidence intervals, the range of values that are likely to contain <span class="math inline">\(\beta_i\)</span>. The 95% confidence interval for <span class="math inline">\(\hat{\beta_i}\)</span> is <span class="math inline">\(\hat{\beta_i}\)</span> ± <span class="math inline">\(t_{0.975}SE(\hat{\beta_i})\)</span>. <span class="math inline">\(t_{0.975}\)</span> is the 97.5% percentile of the t-distribution with <span class="math inline">\(d.f. = n – p\)</span>.</p>
<p>In R, <code>summary()</code> function will test all the coefficients for the null hypothesis <span class="math inline">\(\beta_i=0\)</span>. The function takes the model output obtained from the <code>lm()</code> function. To demonstrate this, let us first get some data. The procedure below simulates data to be used in a regression setting and it is useful to examine what the linear model expect to model the data.</p>
<p>Since we have the data, we can build our model and call the <code>summary</code> function. We will then use <code>confint()</code> function to get the confidence intervals on the coefficients and <code>coef()</code> function to pull out the estimated coefficients from the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1=<span class="kw">lm</span>(y~x)
<span class="kw">summary</span>(mod1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -77.11 -18.44   0.33  16.06  57.23 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 13.24538    6.28869   2.106   0.0377 *  
## x            0.49954    0.05131   9.736 4.54e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 28.77 on 98 degrees of freedom
## Multiple R-squared:  0.4917, Adjusted R-squared:  0.4865 
## F-statistic: 94.78 on 1 and 98 DF,  p-value: 4.537e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get confidence intervals </span>
<span class="kw">confint</span>(mod1)</code></pre></div>
<pre><code>##                 2.5 %     97.5 %
## (Intercept) 0.7656777 25.7250883
## x           0.3977129  0.6013594</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># pull out coefficients from the model</span>
<span class="kw">coef</span>(mod1)</code></pre></div>
<pre><code>## (Intercept)           x 
##  13.2453830   0.4995361</code></pre>
<p>The <code>summary()</code> function prints out an extensive list of values. The “Coefficients” section has the estimates, their standard error, t score and the p-value from the hypothesis test <span class="math inline">\(H_0:\beta_i=0\)</span>. As you can see, the estimate we get for the coefficients and their standard errors are close to the ones we get from the repeatedly sampling and getting a distribution of coefficients. This is statistical inference at work, we can estimate the population properties within a certain error using just a sample.</p>
</div>
<div id="accuracy-of-the-model" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Accuracy of the model</h3>
<p>If you have observed the table output by <code>summary()</code> function, you must have noticed there are some other outputs, such as “Residual standard error”, “Multiple R-squared” and “F-statistic”. These are metrics that are useful for assessing the accuracy of the model. We will explain them one by one.</p>
<p>_ (RSE)_ simply is the square-root of the the sum of squared error terms, divided by degrees of freedom, <span class="math inline">\(n-p\)</span>, for simple linear regression case, <span class="math inline">\(n-2\)</span>. Sum of of the squares of the error terms is also called <strong>“Residual sum of squares”</strong>, RSS. So RSE is calculated as follows:</p>
<p><span class="math display">\[ s=RSE=\sqrt{\frac{\sum{(y_i-\hat{Y_i})^2 }}{n-p}}=\sqrt{\frac{RSS}{n-p}}\]</span></p>
<p>RSE is a way of assessing the model fit. The larger the RSE the worse the model is. However, this is an absolute measure in the units of <span class="math inline">\(Y\)</span> and we have nothing to compare against. One idea is that we divide it by RSS of a simpler model for comparative purposes. That simpler model is in this case is the model with the intercept,<span class="math inline">\(\beta_0\)</span>. A very bad model will have close zero coefficients for explanatory variables, and the RSS of that model will be close to the RSS of the model with only the intercept. In such a model intercept will be equal to <span class="math inline">\(\overline{Y}\)</span>. As it turns out, RSS of the the model with just the intercept is called <em>“Total Sum of Squares” or TSS</em>. A good model will have a low <span class="math inline">\(RSS/TSS\)</span>. The metric <span class="math inline">\(R^2\)</span> uses these quantities to calculate a score between 0 and 1, and closer to 1 the better the model. Here is how it is calculated:</p>
<p><span class="math display">\[R^2=1-\frac{RSS}{TSS}=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}\]</span></p>
<p><span class="math inline">\(TSS-RSS\)</span> part of the formula often referred to as “explained variability” in the model. The bottom part is for “total variability”. With this interpretation, higher the “explained variability” better the model. For simple linear regression with one explanatory variable, the square root of <span class="math inline">\(R^2\)</span> is a quantity known as absolute value of the correlation coefficient, which can be calculated for any pair of variables, not only the response and the explanatory variables. <em>Correlation</em> is a general measure of linear relationship between two variables. One of the most popular flavors of correlation is the Pearson correlation coefficient. Formally, It is the <em>covariance</em> of X and Y divided by multiplication of standard deviations of X and Y. In R, it can be calculated with <code>cor()</code> function.</p>
<p><span class="math display">\[ r_{xy}=\frac{cov(X,Y)}{\sigma_x\sigma_y}
      =\frac{\sum\limits_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}
            {\sqrt{\sum\limits_{i=1}^n (x_i-\bar{x})^2 \sum\limits_{i=1}^n (y_i-\bar{y})^2}}
\]</span> In the equation above, cov is the covariance, this is again a measure of how much two variables change together, like correlation. If two variables show similar behavior they will usually have positive covariance value, if they have opposite behavior, the covariance will have negative value. However, these values are boundless. A normalized way of looking at covariance is to divide covariance by the multiplication of standard errors of X and Y. This bounds the values to -1 and 1, and as mentioned above called Pearson correlation coefficient. The values that change in a similar manner will have a positive coefficient, the values that change in opposite manner will have negative coefficient, and pairs do not have a linear relationship will have 0 or near 0 correlation. In the figure below, we are showing <span class="math inline">\(R^2\)</span>, correlation coefficient and covariance for different scatter plots.</p>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-152-1.png" width="864" /></p>
<p>For simple linear regression, correlation can be used to asses the model. However, this becomes useless as a measure of general accuracy if the there are more than one explanatory variable as in multiple linear regression. In that case, <span class="math inline">\(R^2\)</span> is a measure of accuracy for the model. Interestingly, square of the correlation of predicted values and original response variables (<span class="math inline">\((cor(Y,\hat{Y}))^2\)</span> ) equals to <span class="math inline">\(R^2\)</span> for multiple linear regression.</p>
<p>The last accuracy measure or the model fit in general we are going to explain is <em>F-statistic</em>. This is a quantity that depends on RSS and TSS again. It can also answer one important question that other metrics can not easily answer. That question is whether or not any of the explanatory variables have predictive value or in other words if all the explanatory variables are zero. We can write the null hypothesis as follows:</p>
<p><span class="math display">\[H_0: \beta_1=\beta_2=\beta_3=...=\beta_p=0 \]</span></p>
<p>where the alternative is:</p>
<p><span class="math display">\[H_1: \text{at least one } \beta_i \neq 0 \]</span></p>
<p>Remember <span class="math inline">\(TSS-RSS\)</span> is analogous to “explained variability” and the RSS is analogous to “unexplained variability”. For the F-statistic, we divide explained variance to unexplained variance. Explained variance is just the <span class="math inline">\(TSS-RSS\)</span> divided by degrees of freedom, and unexplained variance is the RSE. The ratio will follow the F-distribution with two parameters, the degrees of freedom for the explained variance and the degrees of freedom for the the unexplained variance.F-statistic for a linear model is calculated as follows.</p>
<p><span class="math display">\[F=\frac{(TSS-RSS)/(p-1)}{RSS/(n-p)}=\frac{(TSS-RSS)/(p-1)}{RSE} \sim F(p-1,n-p)\]</span></p>
<p>If the variances are the same, the ratio will be 1, and when <span class="math inline">\(H_0\)</span> is true, then it can be shown that expected value of <span class="math inline">\((TSS-RSS)/(p-1)\)</span> will be <span class="math inline">\(\sigma^2\)</span> which is estimated by RSE. So, if the variances are significantly different, the ratio will need to be significantly bigger than 1. If the ratio is large enough we can reject the null hypothesis. To asses that we need to use software or look up the tables for F statistics with calculated parameters. In R, function <code>qf()</code> can be used to calculate critical value of the ratio. Benefit of the F-test over looking at significance of coefficients one by one is that we circumvent multiple testing problem. If there are lots of explanatory variables at least 5% of the time (assuming we use 0.05 as P-value significance cutoff), p-values from coefficient t-tests will be wrong. In summary, F-test is a better choice for testing if there is any association between the explanatory variables and the response variable.</p>
</div>
<div id="regression-with-categorical-variables" class="section level3">
<h3><span class="header-section-number">4.3.4</span> Regression with categorical variables</h3>
<p>An important feature of linear regression is that categorical variables can be used as explanatory variables, this feature is very useful in genomics where explanatory variables often could be categorical. To put it in context, in our histone modification example we can also include if promoters have CpG islands or not as a variable. In addition, in differential gene expression, we usually test the difference between different condition which can be encoded as categorical variables in a linear regression. We can sure use t-test for that as well if there are only 2 conditions, but if there are more conditions and other variables to control for such as Age or sex of the samples, we need to take those into account for our statistics, and t-test alone can not handle such complexity. In addition, when we have categorical variables we can also have numeric variables in the model and we certainly do not have to include only one type of variable in a model.</p>
<p>The simplest model with categorical variables include two levels that can be encoded in 0 and 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>)
gene1=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">4</span>,<span class="dt">sd=</span><span class="dv">2</span>)
gene2=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">2</span>,<span class="dt">sd=</span><span class="dv">2</span>)
gene.df=<span class="kw">data.frame</span>(<span class="dt">exp=</span><span class="kw">c</span>(gene1,gene2),
                  <span class="dt">group=</span><span class="kw">c</span>( <span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">30</span>),<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">30</span>) ) )

mod2=<span class="kw">lm</span>(exp~group,<span class="dt">data=</span>gene.df)
<span class="kw">summary</span>(mod2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = exp ~ group, data = gene.df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7290 -1.0664  0.0122  1.3840  4.5629 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   2.1851     0.3517   6.214 6.04e-08 ***
## group         1.8726     0.4973   3.765 0.000391 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.926 on 58 degrees of freedom
## Multiple R-squared:  0.1964, Adjusted R-squared:  0.1826 
## F-statistic: 14.18 on 1 and 58 DF,  p-value: 0.0003905</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(mosaic)
<span class="kw">plotModel</span>(mod2)</code></pre></div>
<p><img src="compgenomr_files/figure-html/unnamed-chunk-153-1.png" width="672" /></p>
<p>we can even compare more levels, we do not even have to encode them ourselves. We can pass categorical variables to <code>lm()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gene.df=<span class="kw">data.frame</span>(<span class="dt">exp=</span><span class="kw">c</span>(gene1,gene2,gene2),
                  <span class="dt">group=</span><span class="kw">c</span>( <span class="kw">rep</span>(<span class="st">&quot;A&quot;</span>,<span class="dv">30</span>),<span class="kw">rep</span>(<span class="st">&quot;B&quot;</span>,<span class="dv">30</span>),<span class="kw">rep</span>(<span class="st">&quot;C&quot;</span>,<span class="dv">30</span>) ) 
                  )

mod3=<span class="kw">lm</span>(exp~group,<span class="dt">data=</span>gene.df)
<span class="kw">summary</span>(mod3)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = exp ~ group, data = gene.df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7290 -1.0793 -0.0976  1.4844  4.5629 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.0577     0.3781  10.731  &lt; 2e-16 ***
## groupB       -1.8726     0.5348  -3.502 0.000732 ***
## groupC       -1.8726     0.5348  -3.502 0.000732 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.071 on 87 degrees of freedom
## Multiple R-squared:  0.1582, Adjusted R-squared:  0.1388 
## F-statistic: 8.174 on 2 and 87 DF,  p-value: 0.0005582</code></pre>
</div>
<div id="regression-pitfalls" class="section level3">
<h3><span class="header-section-number">4.3.5</span> Regression pitfalls</h3>
<p>In most cases one should look at the error terms (residuals) vs fitted values plot. Any structure in this plot indicates problems such as non-linearity, correlation of error terms, non-constant variance or unusual values driving the fit. Below we briefly explain the potential issues with the linear regression.</p>
<div id="non-linearity" class="section level5">
<h5><span class="header-section-number">4.3.5.0.1</span> non-linearity</h5>
<p>If the true relationship is far from linearity, prediction accuracy is reduced and all the other conclusions are questionable. In some cases, transforming the data with <span class="math inline">\(logX\)</span>, <span class="math inline">\(\sqrt{X}\)</span> and <span class="math inline">\(X^2\)</span> could resolve the issue.</p>
</div>
<div id="correlation-of-explanatory-variables" class="section level5">
<h5><span class="header-section-number">4.3.5.0.2</span> correlation of explanatory variables</h5>
<p>If the explanatory variables are correlated that could lead to something known as multicolinearity. When this happens SE estimates of the coefficients will be too large. This is usually observed in time-course data.</p>
</div>
<div id="correlation-of-error-terms" class="section level5">
<h5><span class="header-section-number">4.3.5.0.3</span> correlation of error terms</h5>
<p>This assumes that the errors of the response variables are uncorrelated with each other. If they are confidence intervals in the coefficients might too narrow.</p>
</div>
<div id="non-constant-variance-of-error-terms" class="section level5">
<h5><span class="header-section-number">4.3.5.0.4</span> Non-constant variance of error terms</h5>
<p>This means that different response variables have the same variance in their errors, regardless of the values of the predictor variables. If the errors are not constant, if for the errors grow as X grows this will result in unreliable estimates in standard errors as the model assumes constant variance. Transformation of data, such as <span class="math inline">\(logX\)</span> and <span class="math inline">\(\sqrt{X}\)</span> could help in some cases.</p>
</div>
<div id="outliers-and-high-leverage-points" class="section level5">
<h5><span class="header-section-number">4.3.5.0.5</span> outliers and high leverage points</h5>
<p>Outliers are extreme values for Y and high leverage points are unusual X values. Both of these extremes have power to affect the fitted line and the standard errors. In some cases (measurement error), they can be removed from the data for a better fit.</p>
</div>
</div>
<div id="want-to-know-more-1" class="section level3">
<h3><span class="header-section-number">4.3.6</span> Want to know more…</h3>
<ul>
<li>linear models and derivations of equations including matrix notation
<ul>
<li>Applied Linear Statistical Models by Kutner, Nachtsheim, et al.</li>
<li>Elements of statistical learning by Hastie &amp; Tibshirani</li>
<li>An Introduction to statistical learning by James, Witten, et al.</li>
</ul></li>
</ul>
</div>
</div>
<div id="roadmap-for-future" class="section level2">
<h2><span class="header-section-number">4.4</span> Roadmap for future</h2>
<div id="clustering" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Clustering</h3>
<p>Clustering is the task of grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups. Thise groupings are called clusters.</p>
<p>Resource: Free book chapter on practical clustering with R <a href="https://manning-content.s3.amazonaws.com/download/e/dc31390-3cb7-49dd-ab02-937c1af1c2e1/PDSwR_CH08.pdf" class="uri">https://manning-content.s3.amazonaws.com/download/e/dc31390-3cb7-49dd-ab02-937c1af1c2e1/PDSwR_CH08.pdf</a></p>
<div id="learning-objectives" class="section level5">
<h5><span class="header-section-number">4.4.1.0.1</span> Learning objectives</h5>
<ul>
<li>distance metrics</li>
<li>household clustering algorithms</li>
<li>How to decide best number of clusters</li>
</ul>
</div>
</div>
<div id="dimension-reduction-with-pca" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Dimension reduction with PCA</h3>
<div id="learning-objectives-1" class="section level5">
<h5><span class="header-section-number">4.4.2.0.1</span> Learning objectives</h5>
<ul>
<li>why do we need it?</li>
<li>What is Eigen vectors and values ?</li>
<li>Matrix operations in 2D geometry</li>
</ul>
<p>Resource: - Easy intro: <a href="http://www.nature.com/nbt/journal/v26/n3/abs/nbt0308-303.html" class="uri">http://www.nature.com/nbt/journal/v26/n3/abs/nbt0308-303.html</a></p>
<ul>
<li>More involved with R code: <a href="https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/" class="uri">https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/</a></li>
</ul>
</div>
</div>
<div id="classification" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Classification</h3>
<div id="learning-objectives-2" class="section level5">
<h5><span class="header-section-number">4.4.3.0.1</span> learning objectives</h5>
<ul>
<li>penalized logistic and linear regression (remedies multicolinearity problem)</li>
<li>random forests</li>
<li>Support Vector Machines</li>
</ul>
<p>resource: Introduction to statistical learning by James, Witten, et al.</p>
</div>
</div>
</div>
<div id="exercises-1" class="section level2">
<h2><span class="header-section-number">4.5</span> Exercises</h2>
<div id="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1" class="section level3">
<h3><span class="header-section-number">4.5.1</span> How to summarize collection of data points: The idea behind statistical distributions</h3>
<div id="section-86" class="section level4">
<h4><span class="header-section-number">4.5.1.1</span> </h4>
<p>Calculate the means and variances of the rows of the following simulated data set, plot the distributions of means and variances using <code>hist()</code> and <code>boxplot()</code> functions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>)

<span class="co">#sample data matrix from normal distribution</span>
gset=<span class="kw">rnorm</span>(<span class="dv">600</span>,<span class="dt">mean=</span><span class="dv">200</span>,<span class="dt">sd=</span><span class="dv">70</span>)
data=<span class="kw">matrix</span>(gset,<span class="dt">ncol=</span><span class="dv">6</span>)</code></pre></div>
</div>
<div id="section-87" class="section level4">
<h4><span class="header-section-number">4.5.1.2</span> </h4>
<p>Using the data generated above, calculate the standard deviation of the distribution of the means using <code>sd()</code> function. Compare that to the expected standard error obtained from central limit theorem keeping in mind the population parameters were <span class="math inline">\(\sigma=70\)</span> and <span class="math inline">\(n=6\)</span>. How does the estimate from the random samples change if we simulate more data with <code>data=matrix(rnorm(6000,mean=200,sd=70),ncol=6)</code></p>
</div>
<div id="section-88" class="section level4">
<h4><span class="header-section-number">4.5.1.3</span> </h4>
<ol start="0" style="list-style-type: decimal">
<li>simulate 30 random variables using <code>rpois()</code> function, do this 1000 times and calculate means of sample. Plot the sampling distributions of the means using a histogram. Get the 2.5th and 97.5th percentiles of the distribution.</li>
<li>Use <code>t.test</code> function to calculate confidence intervals of the first random sample <code>pois1</code> simulated from<code>rpois()</code> function below.</li>
<li>Use bootstrap confidence interval for the mean on <code>pois1</code></li>
<li>compare all the estimates</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>)

<span class="co">#sample 30 values from poisson dist with lamda paramater =30</span>
pois1=<span class="kw">rpois</span>(<span class="dv">30</span>,<span class="dt">lambda=</span><span class="dv">5</span>)</code></pre></div>
</div>
<div id="section-89" class="section level4">
<h4><span class="header-section-number">4.5.1.4</span> </h4>
<p>Optional exercise: Try to recreate the following figure, which demonstrates the CLT concept. <img src="compgenomr_files/figure-html/unnamed-chunk-159-1.png" width="672" /></p>
</div>
</div>
<div id="how-to-test-for-differences-in-samples-1" class="section level3">
<h3><span class="header-section-number">4.5.2</span> How to test for differences in samples</h3>
<div id="section-90" class="section level4">
<h4><span class="header-section-number">4.5.2.1</span> </h4>
<p>Test the difference of means of the following simulated genes using the randomization, t-test and <code>wilcox.test()</code> functions. Plot the distributions using histograms and boxplots.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">101</span>)
gene1=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">4</span>,<span class="dt">sd=</span><span class="dv">3</span>)
gene2=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">3</span>,<span class="dt">sd=</span><span class="dv">3</span>)</code></pre></div>
</div>
<div id="section-91" class="section level4">
<h4><span class="header-section-number">4.5.2.2</span> </h4>
<p>Test the difference of means of the following simulated genes using the randomization, t-test and <code>wilcox.test()</code> functions. Plot the distributions using histograms and boxplots.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>)
gene1=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">4</span>,<span class="dt">sd=</span><span class="dv">2</span>)
gene2=<span class="kw">rnorm</span>(<span class="dv">30</span>,<span class="dt">mean=</span><span class="dv">2</span>,<span class="dt">sd=</span><span class="dv">2</span>)</code></pre></div>
</div>
<div id="section-92" class="section level4">
<h4><span class="header-section-number">4.5.2.3</span> </h4>
<p>read the gene expression data set with <code>data=readRDS(&quot;StatisticsForGenomics/geneExpMat.rds&quot;)</code>. The data has 100 differentially expressed genes.First 3 columns are the test samples, and the last 3 are the control samples. Do a t-test for each gene (each row is a gene), record the p-values. Then, do a moderated t-test, as shown in the lecture notes and record the p-values. Do a p-value histogram and compare two approaches in terms of the number of significant tests with 0.05 threshold. On the p-values use FDR (BH), bonferroni and q-value adjustment methods. Calculate how many adjusted p-values are below 0.05 for each approach.</p>
</div>
</div>
<div id="relationship-between-variables-linear-models-and-correlation-1" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Relationship between variables: linear models and correlation</h3>
<div id="section-93" class="section level4">
<h4><span class="header-section-number">4.5.3.1</span> </h4>
<p>Below we are going to simulate X and Y values.</p>
<ol style="list-style-type: decimal">
<li>Run the code then fit a line to predict Y based on X.</li>
<li>Plot the scatter plot and the fitted line.</li>
<li>Calculate correlation and R^2.</li>
<li>Run the <code>summary()</code> function and try to extract P-values for the model from the object returned by <code>summary</code>. see <code>?summary.lm</code></li>
<li>Plot the residuals vs fitted values plot, by calling <code>plot</code> function with <code>which=1</code> as the second argument. First argument is the model returned by <code>lm</code>.</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random number seed, so that the random numbers from the text</span>
<span class="co"># is the same when you run the code.</span>
<span class="kw">set.seed</span>(<span class="dv">32</span>)

<span class="co"># get 50 X values between 1 and 100</span>
x =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">50</span>,<span class="dv">1</span>,<span class="dv">100</span>)

<span class="co"># set b0,b1 and varience (sigma)</span>
b0 =<span class="st"> </span><span class="dv">10</span>
b1 =<span class="st"> </span><span class="dv">2</span>
sigma =<span class="st"> </span><span class="dv">20</span>
<span class="co"># simulate error terms from normal distribution</span>
eps =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dv">0</span>,sigma)
<span class="co"># get y values from the linear equation and addition of error terms</span>
y =<span class="st"> </span>b0 +<span class="st"> </span>b1*x+<span class="st"> </span>eps</code></pre></div>
</div>
<div id="section-94" class="section level4">
<h4><span class="header-section-number">4.5.3.2</span> </h4>
<p>Read the data set histone modification data set with using a variation of: <code>df=readRDS(&quot;StatisticsForGenomics_data/HistoneModeVSgeneExp.rds&quot;)</code>. There are 3 columns in the data set these are measured levels of H3K4me3, H3K27me3 and gene expression per gene.</p>
<ol style="list-style-type: decimal">
<li>plot the scatter plot for H3K4me3 vs expression</li>
<li>plot the scatter plot for H3K27me3 vs expression</li>
<li>fit the model model for prediction of expression data using:
<ul>
<li>only H3K4me3 as explanatory variable</li>
<li>only H3K27me3 as explanatory variable</li>
<li>using both H3K4me3 and H3K27me3 as explanatory variables</li>
</ul></li>
<li>inspect summary() function output in each case, which terms are significant</li>
<li>Is using H3K4me3 and H3K27me3 better than the model with only H3K4me3.</li>
<li>Plot H3k4me3 vs H3k27me3. Inspect the points that does not follow a linear trend. Are they clustered at certain segments of the plot. Bonus: Is there any biological or technical interpretation for those points ?</li>
</ol>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="genomic-intervals-and-r.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
