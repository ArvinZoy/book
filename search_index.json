[
["index.html", "Computational Genomics with R Chapter 1 Preface 1.1 Why R ? 1.2 Who is this book for? 1.3 Acknowledgements", " Computational Genomics with R Altuna Akalin 2016-09-05 Chapter 1 Preface The aim of computational genomics is to do biological interpretation of high dimensional genomics data. Generally speaking, it is similar to any other kind of data analysis but often times doing computational genomics will require domain specific knowledge and tools. As new high-throughout experimental techniques on the rise, data analysis capabilities are sought-after features for researchers. R, with its statistical heritage, plotting features and rich user-contributed packages is one of the best languages for the task of analyzing data. The book gives a brief introduction on basics of R and later divided to chapters that represent subsets of genomics data analysis. 1.1 Why R ? The most of the computational genomics tasks can be accomplished by R. R is not only a powerful statistical programming language but also go-to data analysis tool for many computational genomics experts. High-dimensional genomics datasets are usually suitable to be analyzed with core R packages and functions. On top of that, Bioconductor and CRAN have an array of specialized tools for doing genomics specific analysis. Here is a list of computational genomics tasks that can be completed using R. 1.1.1 Data munging (pre-processing) Often times, the data will not come in ready to analyze format. You may need to convert it to other formats by transforming data points (such as log transforming, normalizing etc), or remove columns/rows that , or remove data points with empty values and, and subset the data set with some arbitrary condition. Most of these tasks can be achieved using R. In addition, with the help of packages R can con- nect to databases in various formats such as mySQL, mongoDB, etc., and query and get the data to R environment using database specific tools. Unfortunately, not all data muging and processing tasks can be accomplished only by R. At times, you may need to use domain specific software or software dealing better with specific type of data sets. For example, R is not great at dealing with character strings, if you are trying to filter a large dataset based on some regular expres- sion you may be better of with perl or awk. 1.1.2 General data anaylsis and exploration Most genomics data sets are suitable for application of general data analysis tools. In some cases, you may need to preprocess the data to get it to a state that is suitable for application such tools. unsupervised data analysis: clustering (k-means, hierarchical), matrix factorization (PCA, ICA etc) supervised data analysis: generalized linear models, support vector machines, randomForests 1.1.3 Visualization Visualization is an important part of all data analysis techniques including computational genomics. Again, you can use core visu- alization technniques in R and also genomics specific ones with the help of specific packages. Basic plots: Histograms, scatter plots, bar plots, box plots ideograms and circus plots for genomics heatmaps meta-profiles of genomic features, read enrichment over all pro- moters genomic track visualization for given locus 1.1.4 Dealing with genomic intervals Most of the genomics data come in a tabular format that contains the location in the genome and some other relevant values, such as scores for those genomic features and/or names. R/Bioconductor has dedicated methods to deal with such data. Here are a couple of example tasks that you can achieve using R. Overlapping CpG islands with transcription start sites, and filter- ing based on overlaps Aligning reads and making read enrichment profiles Overlapping aligned reads with exons and counting aligned reads per gene 1.1.5 Application of other bioinformatics specific algorithms In addition to genomic interval centered methods, R/Bioconductor gives you access to multitude of other bioinformatics specific algo- rithms. Here are some of the things you can do. Sequence analysis: TF binding motifs, GC content and CpG counts of a given DNA sequence Differential expression (or arrays and sequencing based measure- ments) Gene set/Pathway analysis: What kind of genes are enriched in my gene set 1.2 Who is this book for? The book contains practical and theoretical aspects for computational genomics. Biology and medicine generate more data than ever before and we need to educate more people with data analysis skills and understanding of computational genomics. Since computational genomics is interdisciplinary; this book aims to be accessible for both biologists and computer scientists. We wrote this book for the following audiences: Biologists and medical scientists who generate the data and are keen on analyzing it themselves. Students and researchers who are formally starting to do research on or using computational genomics but do not have extensive domain specific knowledge but has at least a beginner level in a quantitative field: math, stats Experienced researchers looking for recipes or quick how-tos to get started in specific data analysis tasks relating to computational genomics. 1.2.1 What will you get out of this? This resource describes the skills and provides how-tos that will help readers analyze their own genomics data. After reading: If you are not familiar with R, you will get the basics of R and divide right in to specialized uses of R for computational genomics. you will understand genomic intervals and operations on them, such as overlap You will be able to use R and its vast package library to do sequence analysis: Such as calculating GC content for given segments of a genome or find transcription factor binding sites You will be familiar with visualization techniques used in genomics, such as heatmaps,meta-gene plots and genomic track visualization You will be familiar with supervised and unsupervised learning techniques which are important in data modelling and exploratory analysis of high-dimensional data You will be familiar with analysis of different high-throughput sequencing data sets mostly using R based tools. 1.3 Acknowledgements Here goes the acknowledgements "],
["introduction-to-genomics.html", "Chapter 2 Introduction to Genomics 2.1 Genes, DNA and central dogma 2.2 Elements of gene regulation 2.3 Shaping the genome: DNA mutation 2.4 High-throughput experimental methods in genomics 2.5 Visualization and data repositories for genomics", " Chapter 2 Introduction to Genomics The aim of this chapter is to provide the reader with some of the fundamentals required for understanding genome biology. By no means, this is a complete overview of the subject but just a summary that will help the non-biologist reader understand the recurring biological concepts in computational genomics. Readers that are well-versed in genome biology and modern genome-wide quantitative assays should feel free to skip this chapter or skim it through. 2.1 Genes, DNA and central dogma A central concept that will come up again and again is “the gene”. Before we can explain that we need to introduce a few other concepts that are important to understand the gene concept. Human body is made up of billions of cells. These cells specialize in different tasks. For example, in the liver there are cells that help produce enzymes to break toxins. In the heart, there are specialized muscle cells that make the heart beat. Yet, all these different kinds of cells come from a single celled embryo. All the instructions to make different kinds of cells are contained within that single cell and with every division of that cell, those instructions are transmitted to new cells. These instructions can be coded into a string - a molecule of DNA, a polymer made of recurring units called nucleotides. The four nucleotides in DNA molecules, Adenine, Guanine, Cytosine and Thymine (coded as four letters: A, C, G, and T) in a specific sequence, store the information for life. DNA is organized in a double-helix form where two complementary polymers interlace with each other and twist into the familiar helical shape. 2.1.1 What is a genome? The full DNA sequence of an organism, which contains all the hereditary information, is called a genome. The genome contains all the information to build and maintain an organism. Genomes come in different sizes and structures. Our genome is not only a naked strech of DNA. In eukaryotic cells, DNA is wrapped around proteins (histones) forming higher-order structures like nucleosomes which make up chromatins and chromosomes (see Figure 2.1). Figure 2.1: Chromosome structure in animals There might be several chromosomes depending on the organism. However, in some species (such as most prokaryotes) DNA is stored in a circular form. The size of genome between species differs too. Human genome has 46 chromosomes and over 3 billion base-pairs, whereas wheat genome has 42 chromosomes and 17 billion base-pairs, both genome size and chromosome numbers are variable between different organisms. Genome sequences of organisms are obtained using sequencing technology. With this technology, fragments of the DNA sequence from the genome, called reads, are obtained. Larger chunks of the genome sequence is later obtained by stitching the initial fragments to larger ones by using the overlapping reads. Latest, sequencing technologies made genome sequencing cheaper and faster. These technologies output more reads, longer reads and more accurate reads. Estimated cost of the first human genome is $300 million in 1999-2000, today a high-quality human genome can be obtained for $1500. Since the costs are going down, researchers and clinicians can generate more data. This drives up to costs for data storage and also drives up the demand for qualified people to analyze genomic data. This was one of the motivations behind writing this book. 2.1.2 What is a gene? In the genome, there are specific regions containing the precise information that encodes for physical products of genetic information. A region in the genome with this information is traditionally called a “gene”. However, the precise definition of the gene is still developing. According to the classical textbooks in molecular biology, a gene is a segment of a DNA sequence corresponding to a single protein or to a single catalytic and structural RNA molecule [1]. A modern definition is: “A region (or regions) that includes all of the sequence elements necessary to encode a functional transcript” [2]. No matter how variable the definitions are, all agree on the fact that genes are basic units of heredity in all living organisms. All cells use their hereditary information in the same way most of the time; the DNA is replicated to transfer the information to new cells. If activated, the genes are transcribed into messenger RNAs (mRNAs) in nucleus (in eukaryotes), followed by mRNAs (if the gene is protein coding) getting translated into proteins in the cytoplasm. This is essentially a process of information transfer between information carrying polymers; DNA, RNA and proteins, known as the “central dogma” of molecular biology (see Figure 2.2 for a summary). Proteins are essential elements for life. The growth and repair, functioning and structure of all living cells depends on them. This is why the gene is a central concept in genome biology, because a gene can encode information for proteins and other functional molecules. How genes are controled and activated dictates everything about an organism. From the identity of a cell to response to an infection, how cells develop and behave against certain stimuli is governed by activity of the genes and functional molecules they encode. The liver cell becomes a liver cell because certain genes are activated and their functional products are produced to help liver cell achieve its tasks. Figure 2.2: Central Dogma: replication, transcription, translation 2.1.3 How genes are controlled ? The transcriptional and the post-transcriptional regulation In order to answer this question, we have to dig a little deeper on the transcription concept we introduced via the central dogma. The first step in a process of information transfer - a production of an RNA copy of a part of the DNA sequence - is called transcription. This task is carried out by the RNA polymerase enzyme. RNA polymerase-dependent initiation of transcription is enabled by the existence of a specific region in the sequence of DNA - a core promoter. Core promoters are regions of DNA that promote transcription and are found upstream from the start site of transcription. In eukaryotes, several proteins, called general transcription factors recognize and bind to core promoters and form a pre-initiation complex. RNA polymerases recognize these complexes and initiate synthesis of RNAs, the polymerase travels along the template DNA and making an RNA copy[3]. After mRNA is produced it is often spliced by splicesosome. The sections called ‘introns’ are removed and sections called ‘exons’ left in. Then, the remaining mRNA translated into proteins. Which exons will be part of the final mature transcript can also be regulated and creates diversity in protein structure and function (See Figure 2.3`). Figure 2.3: Transcription could be followed by splicing, which creates different transcript isoforms. This will in return create different protein isoforms since the information required to produce the protein is encoded in the transcripts. Differences in transcript of the same gene can give rise to different protein isoforms On the contrary to protein coding genes, non-coding RNA (ncRNAs) genes are processed and assume their functional structures after transcription and without going into translation, hence the name: non-coding RNAs. certain ncRNAs can also be spliced but still not translated. ncRNAs and other RNAs in general can form complementary base-pairs within the RNA molecule which gives them additional complexity. This self-complementarity based structure, termed RNA secondary structure, is often necessary for functions of many ncRNA species. In summary, the set of processes, from transcription initiation to production of the functional product, is referred to as gene expression. Gene expression quantification and regulation is a fundamental topic in genome biology. 2.1.4 What does a gene look like? Before we move forward, it will be good to discuss how we can visualize genes. As someone interested in computational genomics, you will frequently encounter a gene on a computer screen, and how it is represented on the computer will be equivalent to what you imagine when you hear the word “gene”. In the online databases, the genes will appear as a sequence of letters or as a series of connected boxes showing exon-intron structure which may include the direction of transcription as well (see Figure 2.4). direction. You will encounter more with the latter so this is likely what will pop into your mind when you think of genes. As we have mentioned DNA has two strands, and a gene can be located on either of them, and direction of transcription will depend on that. In the Figure you can see arrows on introns (lines connecting boxes) indicating the direction of the gene. Figure 2.4: A) Representation of a gene at UCSC browser. Boxes indicate exons, and lines indicate introns. B) Partial sequence of FATE1 gene as shown in NCBI GenBank database. 2.2 Elements of gene regulation The mechanisms regulating gene expression are essential for all living organisms as they dictate where and how much of a gene product (may it be protein or ncRNA) should be manufactured. This regulation could occur at the pre- and co-transcriptional level by controlling how many transcripts should be produced and/or which version of the transcript should be produced by regulating splicing. Different versions of the same gene could encode for proteins by regulating splicing the process can decide which parts will go into the final mRNA that will code for the protein. In addition, gene products can be regulated post-transcriptionally where certain molecules bind to RNA and mark them for degradation even before they can be used in protein production. Gene regulation drives cellular differentiation; a process during which different tissues and cell types are produced. It also helps cells maintain differentiated states of cells/tissues. As a product of this process, at the final stage of differentiation, different kinds of cells maintain different expression profiles although they contain the same genetic material. As mentioned above there are two main types of regulation and next we will provide information on those. 2.2.1 Transcriptional regulation The rate of transcription initiation is the primary regulatory element in gene expression regulation. The rate is controlled by core promoter elements as well as distant-acting regulatory elements such as enhancers. On top of that, processes like histone modifications and/or DNA methylation have a crucial regulatory impact on transcription. If a region is not accessible for the transcriptional machinery, e.g. in the case when chromatin structure is compacted due to the presence of specific histone modifications, or if the promoter DNA is methylated, transcription may not start at all. Last but the not least, gene activity is also controlled post-transcriptionally by ncRNAs such as microRNAs (miRNAs), as well as by cell signaling resulting in protein modification or altered protein-protein interactions. 2.2.1.1 Regulation by transcription factors through regulatory regions Transcripton factors are proteins that recognize a specific DNA motif to bind on a regulatory region and regulate the transcription rate of the gene associated with that regulatory region (See Figure 2.5)` for an illustration). These factors bind to a variety of regulatory regions summarized in Figure 2.5, and their concerted action controls the transcription rate. Apart from their binding preference, their concentration, the availability of synergistic or competing transcription factors will also affect the transcription rate. Figure 2.5: Representation of regulatory regions in animal genomes 2.2.1.1.1 Core and proximal promoters Core promoters are the immediate neighboring regions around the transcription start site (TSS) that serves as a docking site for the transcriptional machinery and pre-initiation complex (PIC) assembly. The textbook model for transcription initiation is as follows: The core promoter has a TATA motif (referred as TATA-box) 30 bp upstream of an initiator sequence (Inr), which also contains TSS. Firstly, transcription factor TFIID binds to the TATA-box. Next, general transcription factors are recruited and transcription is initiated on the initiator sequence. Apart from the TATA-box and Inr, there are a number of sequence elements on the animal core promoters that are associated with transcription initiation and PIC assembly, such as downstream promoter elements (DPEs), the BRE elements and CpG islands. DPEs are found 28-32 bp downstream of the TSS in TATA-less promoters of Drosophila melanogaster, it generally co-occurs with the Inr element, and is thought to have a similar function to the TATA-box. The BRE element is recognized by TFIIB protein and lies upstream of the TATA-box. CpG islands are CG dinucleotide-enriched segments of vertebrate genomes, despite the general depletion of CG dinucleotides in those genomes. 50-70% of promoters in human genome are associated with CpG islands. Proximal promoter elements are typically right upstream of the core promoters and usually contain binding sites for activator transcription factors and they provide additional control over gene expression. 2.2.1.1.2 Enhancers: Proximal regulation is not the only, nor the most important mode of gene regulation. Most of the transcription factor binding sites in the human genome are found in intergenic regions or in introns . This indicates the widespread usage of distal regulatory elements in animal genomes. On a molecular function level, enhancers are similar to proximal promoters; they contain binding sites for the same transcriptional activators and they basically enhance the gene expression. However, they are often highly modular and several of them can affect the same promoter at the same time or in different time-points or tissues. In addition, their activity is independent of their orientation and their distance to the promoter they interact with. A number of studies showed that enhancers can act upon their target genes over several kilobases away. According to a popular model, enhancers achieve this by looping the DNA and coming to contact with their target genes. 2.2.1.1.3 Silencers: Silencers are similar to enhancers; however their effect is opposite of enhancers on the transcription of the target gene, and results in decreasing their level of transcription. They contain binding sites for repressive transcription factors. Repressor transcription factors can either block the binding of an activator , directly compete for the same binding site, or induce a repressive chromatin state in which no activator binding is possible. Silencer effects, similar to those of enhancers, are independent of orientation and distance to target genes. In contradiction to this general view, in Drosophila there are two types of silencers, long-range and short-range. Short-range silencers are close to promoters and long-range silencers can silence multiple promoters or enhancers over kilobases away. Like enhancers, silencers bound by repressors may also induce changes in DNA structure by looping and creating higher order structures. One class of such repressor proteins, which is thought to initiate higher-order structures by looping, is Polycomb group proteins (PcGs). 2.2.1.1.4 Insulators: Insulator regions limit the effect of other regulatory elements to certain chromosomal boundaries; in other words, they create regulatory domains untainted by the regulatory elements in regions outside that domain. Insulators can block enhancer-promoter communication and/or prevent spreading of repressive chromatin domains. In vertebrates and insects, some of the well-studied insulators are bound by CTCF (CCCTC-binding factor). Genome-wide studies from different mammalian tissues confirm that CTCF binding is largely invariant of cell type, and CTCF motif locations are conserved in vertebrates. At present, there are two models of explaining the insulator function; the most prevalent model claims insulators create physically separate domains by modifying chromosome structure. This is thought to be achieved by CTCF-driven chromatin looping and recent evidence shows that CTCF can induce a higher-order chromosome structure through creating loops of chromatins. According to the second model, an insulator-bound activator cannot bind an enhancer; thus enhancer-blocking activity is achieved and insulators can also recruit active histone domain, creating an active domain for enhancers to function. 2.2.1.1.5 Locus control regions: Locus control regions (LCRs) are clusters of different regulatory elements that control entire set of genes on a locus. LCRs help genes achieve their temporal and/or tissue-specific expression programs. LCRs may be composed of multiple cis-regulatory elements, such as insulators, enhancers and they act upon their targets even from long distances. However LCRs function with an orientation dependent manner, for example the activity of beta-globin LCR is lost if inverted. The mechanism of LCR function otherwise seems similar to other long-range regulators described above. The evidence is mounting in the direction of a model where DNA-looping creates a chromosomal structure in which target genes are clustered together, which seems to be essential for maintaining open chromatin domain. 2.2.1.2 Epigenetic regulation Epigenetics in biology usually refers to constructions (chromatin structure, DNA methylation etc.) other than DNA sequence that influence gene regulation. In essence, epigenetic regulation is the regulation of DNA packing and structure, the consequence of which is gene expression regulation. A typical example is that DNA packing inside the nucleus can directly influence gene expression by creating accessible regions for transcription factors to bind. There are two main mechanisms in epigenetic regulation: i) DNA modifications ii) histone modifications. Below, we will introduce these two mechanisms. 2.2.1.2.1 DNA modifications such as methylation: DNA methylation is usually associated with gene silencing. DNA methyltransferase enzyme catalyzes the addition of a methyl group to cytosine of CpG dinucleotides (while in mammals the addition of methyl group is largely restricted to CpG dinucleotides, methylation can occur in other bases as well) . This covalent modification either interferes with transcription factor binding on the region, or methyl-CpG binding proteins induce the spread of repressive chromatin domains, thus the gene is silenced if its promoter has methylated CG dinucleotides. DNA methylation usually occurs in repeat sequences to repress transposable elements, these elements when active can jump around and insert them to random parts of the genome, potentially distrupting the genomic functions. .DNA methylation is also related to a key core and proximal promoter element: CpG islands. CpG islands are usually unmethylated, however for some genes CpG island methylation accompanies their silenced expression. For example, during X-chromosome inactivation many CpG islands are heavily methylated and the associated genes are silenced. In addition, in embryonic stem cell differentiation pluripotency-associated genes are silenced due to DNA methylation. Apart from methylation, there are other kinds of DNA modifications present in mamalian genomes, such as hydroxy-methylation and formylcytosine. These are other modifications under current research that are either intermediate or stable modifications with distinct functional associations. There are as many as 12 distinct DNA modifications observed when we look accross all studied species. 2.2.1.2.2 Histone Modifications: Histones are proteins that constitute nucleosome. In eukaryotes, eight histones nucleosomes are wrapped around by DNA and build nucleosome. They help super-coiling of DNA and inducing high-order structure called chromatin. In chromatin, DNA is either densely packed (called heterochromatin or closed chromatin), or it is loosely packed (called euchromatin or open chromatin) [60, 61]. Heterochromatin is thought to harbor inactive genes since DNA is densely packed and transcriptional machinery cannot access it. On the other hand, euchromatin is more accessible for transcriptional machinery and might therefore harbor active genes. Histones have long and unstructured N-terminal tails which can be covalently modified. The most studied modifications include acetylation, methylation and phosphorylation [60]. Using their tails, histones interact with neighboring nucleosomes and the modifications on the tail affect the nucleosomes affinity to bind DNA and therefore influence DNA packaging around nucleosomes. Different modifications on histones are used in different combinations to program the activity of the genes during differentiation. Histone modifications have a distinct nomenclature, for example: H3K4me3 means the lysine (K) on the 4th position of histone H3 is tri-methylated. Table 1 Histone modifications and their effects. If more than one histone modification has the same effect, they are separated by commas. Modifications Effect H3K9ac Active promoters and enhancers H3K14ac Active transcription H3K4me3/me2/me1,H3K27ac Active promoters and enhancers, H3K4me1 and H3K27ac is enhancer-specific H3K36me3 Active transcribed regions H3K27me3/me2/me1 Silent promoters H3K9me3/me2/me1 Silent promoters Histone modifications are associated with a number of different transcription-related conditions; some of them are summarized in Table 1. Histone modifications can indicate where the regulatory regions are and they can also indicate activity of the genes. From a gene regulatory perspective, maybe the most important modifications are the ones associated with enhancers and promoters. Certain genes in mouse embryonic stem cells have both active H3K4me3 and inactive H3K27me3 modifications in their promoters. Surprisingly, most of these genes have high CpG content, are important for development and are shown to have paused RNA polymerase II [66, 67]. In addition, Heintzman et al. showed that H3K4me1 could predict tissue-specific active enhancers in human cells [68, 69]. The examples above demonstrate the capability of histone modifications in predicting regulatory potential. Furthermore, certain proteins can influence chromatin structure by interacting with histones. Some of these proteins, like those of the Polycomb Group (PcG) and CTCF, are discussed above in the insulators and silencer sections. In vertebrates and insects, PcGs are responsible for maintaining the silent state of developmental genes, and trithorax group proteins (trxG) for maintaining their active state [70, 71]. PcGs and trxGs induce repressed or active states by catalyzing histone modifications or DNA methylation. Both the proteins bind PREs that can be on promoters or several kilobases away [30, 31, 71]. Another protein that induces histone modifications is CTCF. In b-globin locus, CTCF binding is shown to be associated with repressive H3K9/K27me2 modifications [72]. Want to know more on transcriptional regulation ? Transcriptional regulatory elements in the human genome: http://www.ncbi.nlm.nih.gov/pubmed/16719718 On metazoan promoters: types and transcriptional properties: http://www.ncbi.nlm.nih.gov/pubmed/22392219 General principles of regulatory sequence function http://www.nature.com/nrg/journal/v15/n7/abs/nrg3684.html DNA methylation: roles in mammalian development http://www.nature.com/doifinder/10.1038/nrg3354 Histone modifications and organization of the genome http://www.nature.com/nrg/journal/v12/n1/full/nrg2905.html DNA methylation and histone modifications are linked http://www.nature.com/nrg/journal/v10/n5/abs/nrg2540.html 2.2.2 Post-transcriptional regulation 2.2.2.1 Regulation by non-coding RNAs Recent years have witnessed an explosion in noncoding RNA (ncRNA)-related research. Many publications implicated ncRNAs as important regulatory elements. Plants and animals produce many different types of ncRNAs such as long non-coding RNAs (lncRNAs), small-interferring RNAs (siRNAs), microRNAs (miRNAs), promoter-associated RNAs (PARs) and small nucleolar RNAs (snoRNAs). lncRNAs are typically &gt;200 bp long, they are involved in epigenetic regulation by interacting with chromatin remodeling factors and they function in gene regulation. siRNAs are short double-stranded RNAs which are involved in gene-regulation and transposon control, they silence their target genes by cooperating with Argonaute proteins . miRNAs are short single-stranded RNA molecules that interact with their target genes by using their complementary sequence and mark them for quicker degradation. PARs may regulate gene expression as well: they are ~18-200bp long ncRNAs originating from promoters of coding genes. snoRNAs also shown to play roles in gene regulation, although they are mostly believed to guide ribosomal RNA modifications. 2.2.2.2 Splicing regulation Splicing is regulated by regulatory elements on the pre-mRNA and proteins binding to those elements . Regulatory elements are categorized as splicing enhancers and repressors. They can be located either in exons or introns. Depending of their activity and their locations there are four types of regulatory elements: - exonic splicing enhancers (ESEs) - exonic splicing silencers (ESSs) - intronic splicing enhancers (ISEs) - intronic splicing silencers (ISSs). The majority of splicing repressors are heterogeneous nuclear ribonucleoproteins (hnRNPs). If splicing repressor protein bind silencer elements they reduce the change of nearby site to be used as splice junction. On the contrary, splicing enhancers are sites to which splicing activator proteins bind and binding on that region increases the probability that a nearby site will be used as a splice junction. Most of the activator proteins that bind to splicing enhancers are members of the SR protein family. Such proteins can recognize specific RNA recognition motifs. By regulating splicing exons can be skipped or included which creates protein diversity. Want to know more on post-transcriptional regulation… On miRNAs: Their genesis and modes of regulation http://www.sciencedirect.com/science/article/pii/S0092867404000455 Functions of small RNAs http://www.nature.com/nrg/journal/v15/n9/abs/nrg3765.html Functions of non coding RNAs http://www.nature.com/nrg/journal/v15/n6/abs/nrg3722.html on splicing: Wang, Zefeng; Christopher B. Burge (May 2008). “Splicing regulation: From a parts list of regulatory elements to an integrated splicing code”. RNA 14 (5): 802–813. doi:10.1261/rna.876308. ISSN 1355-8382. PMC 2327353. PMID 18369186. Retrieved 2013-08-15. 2.3 Shaping the genome: DNA mutation Human and chimpanzee genomes are 98.8% similar. The 1.2% difference is what separetes us from chimpanzees. The further you move away from human in terms of evolutionary distance the higher the difference gets. However, even between the members of the same species differences in genome sequences exists. These differences are due to a process called mutation which drives differences between individuals but also the provides the fuel for evolution as the source of the genetic variation. Individuals with beneficial mutations can adapt to their surroundings better than others and in time these mutations which are beneficial for survival spreads in the population due to a process called “natural selection”. Selection acts upon individuals with beneficial features which gives them an edge for survival in a given environment. Genetic variation created by the mutations in individuals provide the material on which selection can act upon. If the selection process goes for a long time in a relatively isolated environment that requires adaptation, this population can evolve into a different species given enough time.This is the basic idea behind evolution in a nutshell, and without mutations providing the genetic variation there will be no evolution. Mutations in the genome occur due to multiple reasons. First, DNA replication is not an error-free process. Before a cell division, the DNA is replicated with 1 mistake per 10^8 to 10^10 base-pairs. Second, mutagens such as UV light can induce mutations on the genome. Third factor that contributes to mutation is imperfect DNA repair. Every day any human cell suffers multiple instances DNA damage. DNA repair enzymes are there to cope with this damage but they are also not error-free, depending on which DNA repair mechanism is used (there are multiple) mistakes will be made at varying rates. Mutations are classified by how many bases they effect, their effect on DNA structure and gene function. By their effect on DNA structure the mutations are classified as follows: Base substitution: A base is changed with another. Deletion: One or more bases is deleted. Insertion: New base or bases inserted into the genome. Microsatellite mutation: Small insertions or deletions of small tandemly repeating DNA segments. Inversion: A DNA fragment changes its orientation 180 degrees. Translocation: A DNA fragment moves to another location in the genome. Mutations can also be classified by their size as follows: Point mutations: mutations that involve one base. Substitutions, deletions and insertions are point mutations. They are also termed as single nucleotide polymorphisms (SNPs). small-scale mutations: mutations that involve several bases. Large-scale mutations: mutations which involve larger chromosomal regions. Transposable element insertions (where a segment of the genome jumps to another region in the genome) and segmental duplications ( a large region is copied multiple times in tandem) are typical large scale mutations. Aneuploidies: Insertions or deletions of whole chromosomes. Whole-genome polyploidies: duplications involving whole genome. Mutations by their effect on gene function can be classified as follows: gain-of-function mutations: A type of mutation in which the altered gene product possesses a new molecular function or a new pattern of gene expression. loss-of-function mutations: A mutation that results in reduced or abolished protein function. This is the more common type of mutation. 2.4 High-throughput experimental methods in genomics Most of the biological phenomena described above relating to transcription , gene regulation or DNA mutation can be measured over the entire genome using high-throughput experimental techniques, which are quickly becoming the standard for studying genome biology. In addition, their applications in the clinic are also gaining momemntum: there are already diagnostic tests that are based on these techniques. Some of the things that can be measured by high-throughput assays are as follows: Which genes are expressed and how much ? Where does a transcription factor bind ? Which bases are methylated in the genome ? Which transcripts are translated ? Where does RNA-binding proteins bind ? Which microRNAs are expressed ? Which parts of the genome are in contact with each other ? Where are the mutations in the genome located ? Which parts of the genome are nucleosome-free ? There are many more questions one can answer using modern genome-wide techniques and every other day a new variant of the existing techniques comes along to answer a new question. However, One has to keep in mind that these methods are at varying degrees of maturity and they all come with technical limitations and are not noise-free. Despite this, they are extremely useful for research and clinical purposes. And, thanks to these methods we are able to sequence and annotate genomes at a massive scale. 2.4.1 The general idea behind high-throughput techniques High-throughput methods aim to quantify or locate all or most of the genome that harbours the biological feature (expressed genes, binding sites, etc.) of interest. Most of the methods rely on some sort of enrichment of the the targeted biological feature. For example, if you want to measure expression of protein coding genes you need to be able to extract mRNA molecules with special post-transcriptional alterations that protein-coding genes acquire. If you are looking for transcription factor binding, you need to enrich for the DNA fragments that are bound by the protein if interest.This parts depends on available molecular biology and chemistry technques, and the final product of this part is RNA or DNA fragments. Next, you need to be able to tell where these fragments are coming from in the genome and how many of them are there. Microarrays were the standard tool for the quantification step until spread of sequencing techniques. In microarrays, one had to design complementary bases ,called “oligos” or “probes”, to the genetic material enriched via the experimental protocol. If the enriched material is complementary to the genetic material, a light signal will be produced and the intensity of the signal will be proportional to the amount of the genetic material pairing with that oligo. There will be more probes available for hybrdization (process of complementary bases forming bonds ), so the more fragments available stronger the signal. For this to be able to work, you need to know at least part of your genome sequence, and design probes. If you want to measure gene expression, your probes should overlap with genes and should be unique enough to not to bind sequences from other genes. This technology is now being replaced with sequencing technology, where you directly sequence your genetic material. If you have the sequence of your fragments, you can align them back to genome, see where they are coming from and count them. This is a better technology where the quantification is based on the real identiy of fragments rather than based on hybridization to designed probes. In summary HT techniques has the following steps, and this also summarized in Figure 2.6: Extraction: This is the step where you extract the genetic material of interest, RNA or DNA. and nrichment Enrichment: In this step, you enrich for the event you are interested in. For example, protein binding sites. In some cases such as whole-genome DNA sequencing there is no need for enrichment step. You just get fragments of genomic DNA and sequence them. Quantification: This is where you quantify your enriched material. Depending on the protocol you may need to quantify a control set as well, where you should see no enrichment or only background enrichment. Figure 2.6: Common steps of High-throughput assays in genome biology 2.4.2 High-throughput sequencing High-throughput sequencing, or massively parallel sequencing, is a collection of methods and technologies that can sequence DNA thousands/millons of fragments at a time. This is in contrast to older technologies that can produce a limited number of fragments at a time. Here, throughput refers to number of sequenced bases per hour. The older low-throughput sequencing methods have ~ 100 times less throughput compared to modern high-throughput methods. The increased throughput gives the ability to measure biological features on a genome-wide scale in a shorter time frame. Similar to other high-throughput methods, sequencing based methods also require an enrichment step. This step enriches for the features we are interested in. The main difference of the sequencing based methods is the quantification step. In high-throughput sequencing, enriched fragments are put through the sequencer which outputs the sequences for the fragments. Due to limitations in current leading technologies, only limited number of bases can be sequenced using from the input fragments. However, the length is usually enough to uniquely map the reads to the genome and quantify the input fragments. 2.4.2.1 High-throughput sequencing data If there is a genome available, the reads are aligned to the genome and based on the sequencing protocol different strategies are applied for analysis. Some of the potential analysis strategies and processed output of read alignments are depicted in Figure 2.7. For example, we maybe interested to quantify the gene expression. The experimental protocol, called RNA sequencing- RNA-seq, enriches for fragments of RNA that are coming from protein coding genes. Upon alignment, we can calculate the coverage profile which gives us a read count per base along the genome. This information can be stored in a text file or specialized file formats to be used in subsequent analysis or visualization. We can also just count how many reads overlap with exons of each gene and record read counts per gene for further analysis. This essentially produces a table with gene names and read counts for different samples. As we will see in later chapters, this is an essential information for statistical models that model RNA-seq data. Furthermore, we can stack up the reads and count how many times we see a base position in a read mismatches the base in the genome. Read aligners allow for mismatches, and for this reason we can see reads with mismatches. This information can be used to identify SNPs, and can be stored again in a tabular format with the information of position and mismatch type and number of reads supporting the mismatch. The original algorithms are a bit more complicated than just counting mismatches but the general idea is the same, what they are doing differently is trying to minimize false positive rates by using filters, so that not every mismatch is recorded as SNP. Figure 2.7: High-throughput sequencing summary 2.4.2.2 Future of high-throughput sequencing The sequencing tech is still evolving. Next frontier is the obtaining longer single-molecule reads and preferably being able to call base modifications on the fly. With longer reads, the genome asssembly will be easier for the regions that have high repeat content. With single-molecule sequencing, we will be able tell how many transcripts are present in a given cell population without relying on fragment amplification methods which can introduce biases. Another recent development is single-cell sequencing. Current technologies usually work on genetic material from thousands to millions of cells. This means that the results you receive represents the population of cells that were used in the experiment. However, there is a lot variation between the same type of cells, this variation is not observed at all. Newer sequecing techniques can work on single cells and give quantititave information on each cell. Want to know more on high-throughput sequencing? Current and the future high-throughput sequencing technologies http://www.sciencedirect.com/science/article/pii/S1097276515003408 Illumina repository for different library preperation protocols for sequencing http://www.illumina.com/techniques/sequencing/ngs-library-prep/library-prep-methods.html 2.5 Visualization and data repositories for genomics There are ~100 animal genomes sequenced as of 2016. On top these, there are many research projects from either individual labs or consortiums that produce petabytes of auxiliary genomics data, such as ChIP-seq, RNA-seq, etc. There are two requirements for to be able to visualize genomes and its associated data, 1) you need to be able to work with a species that has a sequenced genome and 2) you want to have annotation on that genome, meaning at the very least you want to know where the genes are. Most genomes after sequencing quickly annotated with gene-predictions or know gene sequences are mapped on to it, you can also have conservation to other species to filter functional elements. If you are working with a model organism or human you will also have a lot of auxiliary information to help demarcate the functional regions such as regulatory regions, ncRNA, SNPs that are common in the population. Or you might have disease or tissue specific data available. . The more the organism is worked on the more auxiliary data you will have. 2.5.0.1 Accessing genome sequences and annotations via genome browsers As someone intends to work with genomics, you will need to visualize a large amount of data to make biological inferences or simply check regions of interest in the genome visually. Looking at the genome case by case with all the additional datasets is a necessary step to develop hypothesis and understand the data. Many genomes and associated data is available through genome browsers. A genome browser is a website or an app that helps you visualize the genome and all the available data associated with it. Via genome browsers, you will be able to see where genes are in relation to each other and other functional elements. You will be able to see gene structure. You will be able to see auxiliary data such as conservation, repeat content and SNPs. Here we review some of the popular browsers. UCSC genome browser: This is an online browser hosted by University of California, Santa Cruz at http://genome.ucsc.edu/. This is an interactive website that contains genomes and annotations for many species. You can search for genes or genome coordinates for the species of your interest. It is usually very responsive and allows you to visualize large amounts of data. In addition, it has multiple other tools that can be used in connection with the browser. One of the most useful tool is UCSC Table Browser, which lets you download the all the data you see on the browser, including sequence data, in multiple formats . Users can upload data or provide links to the data to visualize user specific data. Ensembl: This is another online browser maintained by European Bioinformatics Institute and the Wellcome Trust Sanger Institute in the UK, http://www.ensembl.org. Similar to UCSC browser, users can visualize genes or genomic coordinates from multiple species and it also comes with auxiliary data. Ensemlb is associated with Biomart tool which is similar to UCSC Table browser, can download genome data including all the auxiliary data set in multiple formats. IGV: Integrated genomics viewer (IGV) is a desktop application developed by Broad institute (https://www.broadinstitute.org/igv/). It is developed to deal with large amounts of high-throughput sequencing data which is harder to view in online browsers. IGV can integrate your local sequencing results with online annotation on your desktop machine. This is useful when viewing sequencing data, especially alignments. Other browsers mentioned above have similar functionalities however you will need to make your large sequencing data available online somewhere before it can be viewed by browsers. 2.5.0.2 Data repositories for high-throughput assays Genome browser contain lots of auxiliary high-throughput data. However, there are many more public high-throughput data sets available and they are certainly not available through genome browsers. Normally, every high-throughput dataset associated with a publication should be deposited to public archieves. There are two major public archives we use to deposit data. One of them is Gene expression Omnibus(GEO) hosted at http://www.ncbi.nlm.nih.gov/geo/, and the other one is European nucleotide archive(ENA) hosted at http://www.ebi.ac.uk/ena. These repositories accept high-throughput datasets and users can freely download and use these public data sets for their own research. Many data sets in these repositories are in their raw format, for example the format the sequencer provides mostly. Some data sets will also have processed data but that is not a norm. Apart from these repositories, there are multiple multi-national consortia that is dedicated to certain genome biology or disease related problems and they maintain their own databases and provide access to processed and raw data. Some of these consortia is mentioned below. Consortium what is it for? ENCODE Transcription factor binding sites, gene expression and epigenomics data for cell lines Epigenomics Roadmap Epigenomics data for multiple cell types The cancer genome atlas (TCGA) Expression, mutation and epigenomics data for multiple cancer types 1000 genomes project Human genetic variation data obtained by sequencing 1000s of individuals "],
["introduction-to-r.html", "Chapter 3 Introduction to R 3.1 The Setup 3.2 Computations in R 3.3 Data structures 3.4 Data types 3.5 Reading and writing data 3.6 Plotting in R 3.7 Functions and control structures (for, if/else etc.) 3.8 Session info 3.9 Exercises", " Chapter 3 Introduction to R R is a free statistical programming language that is popular among researchers and data miners to build software and analyze data1. In next sections, we will first get you started with the setup of environment for using R and then introduce some basic R operations and data structures that will be good to know if you do not have prior experience with R. If you need more in depth R introduction you will want to check out some beginner level books and online tutorials. This website has a bunch of resources listed: http: //www.introductoryr.co.uk/R_Resources_for_Beginners.html 3.1 The Setup Download and install R http://cran.r-project.org/ and RStudio http://www.rstudio.com/ if you do not have them already. Rstudio is optional but it is a great tool if you are just starting to learn R. You will need specific data sets to run the codes in this document. Download the data.zip[URL to come] and extract it to your directory of choice. The folder name should be “data” and your R working directory should be level above the data folder. That means in your R console, when you type “dir(“data”)” you should be able to see the contents of the data folder. You can change your working directory by setwd() command and get your current working directory with getwd() command in R. In RStudio, you can click on the top menu and change the location of your working directory via user interface. 3.1.1 Installing packages R packages are add-ons to base R that help you achieve additional tasks that are not directly supported by base R. It is by the action of these extra functionality that R excels as a tool for computational genomics. Bioconductor project (http://bioconductor.org/) is a dedicated package repository for computational biology related packages. However main package repository of R, called CRAN, has also computational biology related packages. In addition, R-Forge(http://r-forge.r-project.org/), GitHub(https://github. com/), and googlecode(http://code.google.com) are other locations where R packages might be hosted. You can install CRAN packages using install.packages(). (# is the comment character in R) # install package named &quot;randomForests&quot; from CRAN install.packages(&quot;randomForests&quot;) You can install bioconductor packages with a specific installer script # get the installer package source(&quot;http://bioconductor.org/biocLite.R&quot;) # install bioconductor package &quot;rtracklayer&quot; biocLite(&quot;rtracklayer&quot;) You can install packages from github using install_github() function from devtools library(devtools) install_github(&quot;hadley/stringr&quot;) Another way to install packages are from the source. # download the source file download.file(&quot;http://goo.gl/3pvHYI&quot;, destfile=&quot;methylKit_0.5.7.tar.gz&quot;) # install the package from the source file install.packages(&quot;methylKit_0.5.7.tar.gz&quot;, repos=NULL,type=&quot;source&quot;) # delete the source file unlink(&quot;methylKit_0.5.7.tar.gz&quot;) You can also update CRAN and Bioconductor packages. # updating CRAN packages update.packages() # updating bioconductor packages source(&quot;http://bioconductor.org/biocLite.R&quot;) biocLite(&quot;BiocUpgrade&quot;) 3.1.2 Installing packages in custom locations If you will be using R on servers or computing clusters rather than your personal computer it is unlikely that you will have administrator access to install packages. In that case, you can install packages in custom locations by telling R where to look for additional packages. This is done by setting up an .Renviron file in your home directory and add the following line: R_LIBS=~/Rlibs This tells R that “Rlibs” directory at your home directory will be the first choice of locations to look for packages and install packages (The directory name and location is up to you above is just an example). You should go and create that directory now. After that, start a fresh R session and start installing packages. From now on, packages will be installed to your local directory where you have read-write access. 3.1.3 Getting help on functions and packages You can get help on functions by help() and help.search() functions. You can list the functions in a package with ls() function library(MASS) ls(&quot;package:MASS&quot;) # functions in the package ls() # objects in your R enviroment # get help on hist() function ?hist help(&quot;hist&quot;) # search the word &quot;hist&quot; in help pages help.search(&quot;hist&quot;) ??hist 3.1.3.1 More help needed? In addition, check package vignettes for help and practical understanding of the functions. All Bionconductor packages have vignettes that walk you through example analysis. Google search will always be helpful as well, there are many blogs and web pages that have posts about R. R-help, Stackoverflow and R-bloggers are usually source of good and reliable information. 3.2 Computations in R R can be used as an ordinary calculator, some say it is an over-grown calculator. Here are some examples. Remember # is the comment character. The comments give details about the operations in case they are not clear. 2 + 3 * 5 # Note the order of operations. log(10) # Natural logarithm with base e 5^2 # 5 raised to the second power 3/2 # Division sqrt(16) # Square root abs(3-7) # Absolute value of 3-7 pi # The number exp(2) # exponential function # This is a comment line 3.3 Data structures R has multiple data structures. If you are familiar with excel you can think of data structures as building blocks of a table and the table itself, and a table is similar to a sheet in excel. Most of the time you will deal with tabular data sets, you will manipulate them, take sub-sections of them. It is essential to know what are the common data structures in R and how they can be used. R deals with named data structures, this means you can give names to data structures and manipulate or operate on them using those names. 3.3.1 Vectors Vectors are one the core R data structures. It is basically a list of elements of the same type (numeric,character or logical). Later you will see that every column of a table will be represented as a vector. R handles vectors easily and intuitively. You can create vectors with c() function, however that is not the only way. The operations on vectors will propagate to all the elements of the vectors. x&lt;-c(1,3,2,10,5) #create a vector named x with 5 components x = c(1,3,2,10,5) x ## [1] 1 3 2 10 5 y&lt;-1:5 #create a vector of consecutive integers y y+2 #scalar addition ## [1] 3 4 5 6 7 2*y #scalar multiplication ## [1] 2 4 6 8 10 y^2 #raise each component to the second power ## [1] 1 4 9 16 25 2^y #raise 2 to the first through fifth power ## [1] 2 4 8 16 32 y #y itself has not been unchanged ## [1] 1 2 3 4 5 y&lt;-y*2 y #it is now changed ## [1] 2 4 6 8 10 r1&lt;-rep(1,3) # create a vector of 1s, length 3 length(r1) #length of the vector ## [1] 3 class(r1) # class of the vector ## [1] &quot;numeric&quot; a&lt;-1 # this is actually a vector length one 3.3.2 Matrices A matrix refers to a numeric array of rows and columns. You can think of it as a stacked version of vectors where each row or column is a vector. One of the easiest ways to create a matrix is to combine vectors of equal length using cbind(), meaning ‘column bind’. x&lt;-c(1,2,3,4) y&lt;-c(4,5,6,7) m1&lt;-cbind(x,y);m1 ## x y ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 ## [4,] 4 7 t(m1) # transpose of m1 ## [,1] [,2] [,3] [,4] ## x 1 2 3 4 ## y 4 5 6 7 dim(m1) # 2 by 5 matrix ## [1] 4 2 You can also directly list the elements and specify the matrix: m2&lt;-matrix(c(1,3,2,5,-1,2,2,3,9),nrow=3) m2 ## [,1] [,2] [,3] ## [1,] 1 5 2 ## [2,] 3 -1 3 ## [3,] 2 2 9 Matrices and the next data structure data frames are tabular data structures. You can subset them using and providing desired rows and columns to subset. Here is how that works conceptually: slicing/subsetting of a matrix and data frame.There are additional ways to subset data frames, see the next section. 3.3.3 Data Frames A data frame is more general than a matrix, in that different columns can have different modes (numeric, character, factor, etc.). A data frame can be constructed by data.frame() function. For example, we illustrate how to construct a data frame from genomic intervals or coordinates. chr &lt;- c(&quot;chr1&quot;, &quot;chr1&quot;, &quot;chr2&quot;, &quot;chr2&quot;) strand &lt;- c(&quot;-&quot;,&quot;-&quot;,&quot;+&quot;,&quot;+&quot;) start&lt;- c(200,4000,100,400) end&lt;-c(250,410,200,450) mydata &lt;- data.frame(chr,start,end,strand) #change column names names(mydata) &lt;- c(&quot;chr&quot;,&quot;start&quot;,&quot;end&quot;,&quot;strand&quot;) mydata # OR this will work too ## chr start end strand ## 1 chr1 200 250 - ## 2 chr1 4000 410 - ## 3 chr2 100 200 + ## 4 chr2 400 450 + mydata &lt;- data.frame(chr=chr,start=start,end=end,strand=strand) mydata ## chr start end strand ## 1 chr1 200 250 - ## 2 chr1 4000 410 - ## 3 chr2 100 200 + ## 4 chr2 400 450 + There are a variety of ways to extract the elements of a data frame. You can extract certain columns using column numbers or names, or you can extract certain rows by using row numbers. You can also extract data using logical arguments, such as extracting all rows that has a value in a column larger than your threshold. mydata[,2:4] # columns 2,3,4 of data frame ## start end strand ## 1 200 250 - ## 2 4000 410 - ## 3 100 200 + ## 4 400 450 + mydata[,c(&quot;chr&quot;,&quot;start&quot;)] # columns chr and start from data frame ## chr start ## 1 chr1 200 ## 2 chr1 4000 ## 3 chr2 100 ## 4 chr2 400 mydata$start # variable start in the data frame ## [1] 200 4000 100 400 mydata[c(1,3),] # get 1st and 3rd rows ## chr start end strand ## 1 chr1 200 250 - ## 3 chr2 100 200 + mydata[mydata$start&gt;400,] # get all rows where start&gt;400 ## chr start end strand ## 2 chr1 4000 410 - 3.3.4 Lists An ordered collection of objects (components). A list allows you to gather a variety of (possibly unrelated) objects under one name. # example of a list with 4 components # a string, a numeric vector, a matrix, and a scalar w &lt;- list(name=&quot;Fred&quot;, mynumbers=c(1,2,3), mymatrix=matrix(1:4,ncol=2), age=5.3) w ## $name ## [1] &quot;Fred&quot; ## ## $mynumbers ## [1] 1 2 3 ## ## $mymatrix ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## $age ## [1] 5.3 You can extract elements of a list using the [] convention using either its position in the list or its name. w[[3]] # 3rd component of the list ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 w[[&quot;mynumbers&quot;]] # component named mynumbers in list ## [1] 1 2 3 w$age ## [1] 5.3 3.3.5 Factors Factors are used to store categorical data. They are important for statistical modeling since categorical variables are treated differently in statistical models than continuous variables. This ensures categorical data treated accordingly in statistical models. features=c(&quot;promoter&quot;,&quot;exon&quot;,&quot;intron&quot;) f.feat=factor(features) Important thing to note is that when you are reading a data.frame with read.table() or creating a data frame with data.frame() character columns are stored as factors by default, to change this behavior you need to set stringsAsFactors=FALSE in read.table() and/or data.frame() function arguments. 3.4 Data types There are four common data types in R, they are numeric, logical, character and integer. All these data types can be used to create vectors natively. #create a numeric vector x with 5 components x&lt;-c(1,3,2,10,5) x ## [1] 1 3 2 10 5 #create a logical vector x x&lt;-c(TRUE,FALSE,TRUE) x ## [1] TRUE FALSE TRUE # create a character vector x&lt;-c(&quot;sds&quot;,&quot;sd&quot;,&quot;as&quot;) x ## [1] &quot;sds&quot; &quot;sd&quot; &quot;as&quot; class(x) ## [1] &quot;character&quot; # create an integer vector x&lt;-c(1L,2L,3L) x ## [1] 1 2 3 class(x) ## [1] &quot;integer&quot; 3.5 Reading and writing data Most of the genomics data sets are in the form of genomic intervals associated with a score. That means mostly the data will be in table format with columns denoting chromosome, start positions, end positions, strand and score. One of the popular formats is BED format used primarily by UCSC genome browser but most other genome browsers and tools will support BED format. We have all the annotation data in BED format. In R, you can easily read tabular format data with read.table() function. enh.df &lt;- read.table(&quot;intro2R_data/data/subset.enhancers.hg18.bed&quot;, header = FALSE) # read enhancer marker BED file cpgi.df &lt;- read.table(&quot;intro2R_data/data/subset.cpgi.hg18.bed&quot;, header = FALSE) # read CpG island BED file # check first lines to see how the data looks like head(enh.df) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 ## 1 chr20 266275 267925 . 1000 . 9.11 13.1693 -1 ## 2 chr20 287400 294500 . 1000 . 10.53 13.0231 -1 ## 3 chr20 300500 302500 . 1000 . 9.10 13.3935 -1 ## 4 chr20 330400 331800 . 1000 . 6.39 13.5105 -1 ## 5 chr20 341425 343400 . 1000 . 6.20 12.9852 -1 ## 6 chr20 437975 439900 . 1000 . 6.31 13.5184 -1 head(cpgi.df) ## V1 V2 V3 V4 ## 1 chr20 195575 195851 CpG:_28 ## 2 chr20 207789 208148 CpG:_32 ## 3 chr20 219055 219437 CpG:_33 ## 4 chr20 225831 227155 CpG:_135 ## 5 chr20 252826 256323 CpG:_286 ## 6 chr20 275376 276977 CpG:_116 You can save your data by writing it to disk as a text file. A data frame or matrix can be written out by using write.table() function. Now let us write out cpgi.df, we will write it out as a tab-separated file, pay attention to the arguments. write.table(cpgi.df,file=&quot;cpgi.txt&quot;,quote=FALSE, row.names=FALSE,col.names=FALSE,sep=&quot;\\t&quot;) You can save your R objects directly into a file using save() and saveRDS() and load them back in with load() and readRDS(). By using these functions you can save any R object whether or not they are in data frame or matrix classes. save(cpgi.df,enh.df,file=&quot;mydata.RData&quot;) load(&quot;mydata.RData&quot;) # saveRDS() can save one object at a type saveRDS(cpgi.df,file=&quot;cpgi.rds&quot;) x=readRDS(&quot;cpgi.rds&quot;) head(x) ## V1 V2 V3 V4 ## 1 chr20 195575 195851 CpG:_28 ## 2 chr20 207789 208148 CpG:_32 ## 3 chr20 219055 219437 CpG:_33 ## 4 chr20 225831 227155 CpG:_135 ## 5 chr20 252826 256323 CpG:_286 ## 6 chr20 275376 276977 CpG:_116 One important thing is that with save() you can save many objects at a time and when they are loaded into memory with load() they retain their variable names. For example, in the above code when you use load(“mydata.RData”) in a fresh R session, an object names “cpg.df” will be created. That means you have to figure out what name you gave it to the objects before saving them. On the contrary to that, when you save an object by saveRDS() and read by readRDS() the name of the object is not retained, you need to assign the output of readRDS() to a new variable (“x” in the above code chunk). 3.6 Plotting in R R has great support for plotting and customizing plots. We will show only a few below. Let us sample 50 values from normal distribution and plot them as a histogram. # sample 50 values from normal distribution # and store them in vector x x&lt;-rnorm(50) hist(x) # plot the histogram of those values We can modify all the plots by providing certain arguments to the plotting function. Now let’s give a title to the plot using ‘main’ argument. We can also change the color of the bars using ‘col’ argument. You can simply provide the name of the color. Below, we are using ‘red’ for the color. See Figure below for the result this chunk. hist(x,main=&quot;Hello histogram!!!&quot;,col=&quot;red&quot;) Next, we will make a scatter plot. Scatter plots are one the most common plots you will encounter in data analysis. We will sample another set of 50 values and plotted those against the ones we sampled earlier. Scatterplot shows values of two variables for a set of data points. It is useful to visualize relationships between two variables. It is frequently used in connection with correlation and linear regression. There are other variants of scatter plots which show density of the points with different colors. We will show examples of those that in following chapters. The scatter plot from our sampling experiment is shown in the figure. Notice that, in addition to main we used “xlab” and “ylab” arguments to give labels to the plot. You can customize the plots even more than this. See ?plot and ?par for more arguments that can help you customize the plots. # randomly sample 50 points from normal distribution y&lt;-rnorm(50) #plot a scatter plot # control x-axis and y-axis labels plot(x,y,main=&quot;scatterplot of random samples&quot;, ylab=&quot;y values&quot;,xlab=&quot;x values&quot;) we can also plot boxplots for vectors x and y. Boxplots depict groups of numerical data through their quartiles. The edges of the box denote 1st and 3rd quartile, and the line that crosses the box is the median. Whiskers usually are defined using interquantile range: lowerWhisker=Q1-1.5[IQR] and upperWhisker=Q1+1.5[IQR] In addition, outliers can be depicted as dots. In this case, outliers are the values that remain outside the whiskers. boxplot(x,y,main=&quot;boxplots of random samples&quot;) Next up is bar plot which you can plot by barplot() function. We are going to plot four imaginary percentage values and color them with two colors, and this time we will also show how to draw a legend on the plot using legend() function. perc=c(50,70,35,25) barplot(height=perc,names.arg=c(&quot;CpGi&quot;,&quot;exon&quot;,&quot;CpGi&quot;,&quot;exon&quot;), ylab=&quot;percentages&quot;,main=&quot;imagine %s&quot;, col=c(&quot;red&quot;,&quot;red&quot;,&quot;blue&quot;,&quot;blue&quot;)) legend(&quot;topright&quot;,legend=c(&quot;test&quot;,&quot;control&quot;),fill=c(&quot;red&quot;,&quot;blue&quot;)) ## Saving plots If you want to save your plots to an image file there are couple of ways of doing that. Normally, you will have to do the following: 1. Open a graphics device 2. Create the plot 3. Close the graphics device pdf(&quot;mygraphs/myplot.pdf&quot;,width=5,height=5) plot(x,y) dev.off() Alternatively, you can first create the plot then copy the plot to a graphic device. plot(x,y) dev.copy(pdf,&quot;mygraphs/myplot.pdf&quot;,width=7,height=5) dev.off() 3.7 Functions and control structures (for, if/else etc.) 3.7.1 User defined functions Functions are useful for transforming larger chunks of code to re-usable pieces of code. Generally, if you need to execute certain tasks with variable parameters then it is time you write a function. A function in R takes different arguments and returns a definite output, much like mathematical functions. Here is a simple function takes two arguments, x and y, and returns the sum of their squares. sqSum&lt;-function(x,y){ result=x^2+y^2 return(result) } # now try the function out sqSum(2,3) ## [1] 13 Functions can also output plots and/or messages to the terminal. Here is a function that prints a message to the terminal: sqSumPrint&lt;-function(x,y){ result=x^2+y^2 cat(&quot;here is the result:&quot;,result,&quot;\\n&quot;) } # now try the function out sqSumPrint(2,3) ## here is the result: 13 Sometimes we would want to execute a certain part of the code only if certain condition is satisfied. This condition can be anything from the type of an object (Ex: if object is a matrix execute certain code), or it can be more complicated such as if object value is between certain thresholds. Let us see how they can be used3. They can be used anywhere in your code, now we will use it in a function. cpgi.df &lt;- read.table(&quot;intro2R_data/data/subset.cpgi.hg18.bed&quot;, header = FALSE) # function takes input one row # of CpGi data frame largeCpGi&lt;-function(bedRow){ cpglen=bedRow[3]-bedRow[2]+1 if(cpglen&gt;1500){ cat(&quot;this is large\\n&quot;) } else if(cpglen&lt;=1500 &amp; cpglen&gt;700){ cat(&quot;this is normal\\n&quot;) } else{ cat(&quot;this is short\\n&quot;) } } largeCpGi(cpgi.df[10,]) largeCpGi(cpgi.df[100,]) largeCpGi(cpgi.df[1000,]) 3.7.2 Loops and looping structures in R When you need to repeat a certain task or a execute a function multiple times, you can do that with the help of loops. A loop will execute the task until a certain condition is reached. The loop below is called a “for-loop” and it executes the task sequentially 10 times. for(i in 1:10){ # number of repetitions cat(&quot;This is iteration&quot;) # the task to be repeated print(i) } ## This is iteration[1] 1 ## This is iteration[1] 2 ## This is iteration[1] 3 ## This is iteration[1] 4 ## This is iteration[1] 5 ## This is iteration[1] 6 ## This is iteration[1] 7 ## This is iteration[1] 8 ## This is iteration[1] 9 ## This is iteration[1] 10 The task above is a bit pointless, normally in a loop, you would want to do something meaningful. Let us calculate the length of the CpG islands we read in earlier. Although this is not the most efficient way of doing that particular task, it serves as a good example for looping. The code below will be execute hundred times, and it will calculate the length of the CpG islands for the first 100 islands in the data frame (by subtracting the end coordinate from the start coordinate). Note:If you are going to run a loop that has a lot of repetitions, it is smart to try the loop with few repetitions first and check the results. This will help you make sure the code in the loop works before executing it for thousands of times. # this is where we will keep the lenghts # for now it is an empty vector result=c() # start the loop for(i in 1:100){ #calculate the length len=cpgi.df[i,3]-cpgi.df[i,2]+1 #append the length to the result result=c(result,len) } # check the results head(result) ## [1] 277 360 383 1325 3498 1602 3.7.2.1 apply family functions instead of loops R has other ways of repeating tasks that tend to be more efficient than using loops. They are known as the “apply” family of functions, which include apply, lapply, mapply and tapply (and some other variants). All of these functions apply a given function to a set of instances and returns the result of those functions for each instance. The differences between them is that they take different type of inputs. For example apply works on data frames or matrices and applies the function on each row or column of the data structure. lapply works on lists or vectors and applies a function which takes the list element as an argument. Next we will demonstrate how to use apply() on a matrix. The example applies the sum function on the rows of a matrix, it basically sums up the values on each row of the matrix, which is conceptualized in Figure below. alt text mat=cbind(c(3,0,3,3),c(3,0,0,0),c(3,0,0,3),c(1,1,0,0),c(1,1,1,0),c(1,1,1,0)) result&lt;-apply(mat,1,sum) result ## [1] 12 3 5 6 # OR you can define the function as an argument to apply() result&lt;-apply(mat,1,function(x) sum(x)) result ## [1] 12 3 5 6 Notice that we used a second argument which equals to 1, that indicates that rows of the matrix/ data frame will be the input for the function. If we change the second argument to 2, this will indicate that columns should be the input for the function that will be applied. See Figure for the visualization of apply() on columns. alt text result&lt;-apply(mat,2,sum) result ## [1] 9 3 6 2 3 3 Next, we will use lapply(), which applies a function on a list or a vector. The function that will be applied is a simple function that takes the square of a given number. input=c(1,2,3) lapply(input,function(x) x^2) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 9 mapply() is another member of apply family, it can apply a function on an unlimited set of vectors/lists, it is like a version of lapply that can handle multiple vectors as arguments. In this case, the argument to the mapply() is the function to be applied and the sets of parameters to be supplied as arguments of the function. This conceptualized Figure below, the function to be applied is a function that takes to arguments and sums them up. The arguments to be summed up are in the format of vectors, Xs and Ys. mapply() applies the summation function to each pair in Xs and Ys vector. Notice that the order of the input function and extra arguments are different for mapply. Xs=0:5 Ys=c(2,2,2,3,3,3) result&lt;-mapply(function(x,y) sum(x,y),Xs,Ys) result ## [1] 2 3 4 6 7 8 3.7.2.2 apply family functions on multiple cores If you have large data sets apply family functions can be slow (although probably still better than for loops). If that is the case, you can easily use the parallel versions of those functions from parallel package. These functions essentially divide your tasks to smaller chunks run them on separate CPUs and merge the results from those parallel operations. This concept is visualized at Figure below , mcapply runs the summation function on three different processors. Each processor executes the summation function on a part of the data set, and the results are merged and returned as a single vector that has the same order as the input parameters Xs and Ys. 3.7.2.3 Vectorized Functions in R The above examples have been put forward to illustrate functions and loops in R because functions using sum() are not complicated and easy to understand. You will probably need to use loops and looping structures with more complicated functions. In reality, most of the operations we used do not need the use of loops or looping structures because there are already vectorized functions that can achieve the same outcomes, meaning if the input arguments are R vectors the output will be a vector as well, so no need for loops or vectorization. For example, instead of using mapply() and sum() functions we can just use + operator and sum up Xs and Ys. result=Xs+Ys result ## [1] 2 3 4 6 7 8 In order to get the column or row sums, we can use the vectorized functions colSums() and rowSums(). colSums(mat) ## [1] 9 3 6 2 3 3 rowSums(mat) ## [1] 12 3 5 6 However, remember that not every function is vectorized in R, use the ones that are. But sooner or later, apply family functions will come in handy. 3.8 Session info sessionInfo() ## R version 3.3.0 (2016-05-03) ## Platform: x86_64-apple-darwin13.4.0 (64-bit) ## Running under: OS X 10.9.5 (Mavericks) ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] knitr_1.14 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.6 bookdown_0.1.7 codetools_0.2-14 digest_0.6.10 ## [5] mime_0.5 R6_2.1.2 xtable_1.8-2 formatR_1.4 ## [9] magrittr_1.5 evaluate_0.9 stringi_1.1.1 miniUI_0.1.1 ## [13] rstudioapi_0.6 rmarkdown_1.0 tools_3.3.0 stringr_1.1.0 ## [17] shiny_0.13.2 yaml_2.1.13 httpuv_1.3.3 htmltools_0.3.5 3.9 Exercises Set your working directory to the source file location in RStudio top menu: ‘Session &gt; Set Working Directory &gt; To Source File Location’ if you have done it correctly you should see this script when you type dir() &gt; dir() [1] &quot;intro2R.exercises.html&quot; 3.9.1 Computations in R 3.9.1.1 Sum 2 and 3, use + 3.9.1.2 Take the square root of 36, use sqrt() 3.9.1.3 Take the log10 of 1000, use function log10() 3.9.1.4 Take the log2 of 32, use function log2() 3.9.1.5 Assign the sum of 2,3 and 4 to variable x 3.9.1.6 Find the absolute value of 5 - 145 using abs() function 3.9.1.7 Calculate the square root of 625, divide it by 5 and assign it to variable x. Ex: y= log10(1000)/5, the previous statement takes log10 of 1000, divides it by 5 and assigns the value to variable y 3.9.1.8 Multiply the value you get from previous exercise with 10000, assign it variable x Ex: y=y*5, multiplies y with 5 and assigns the value to y. KEY CONCEPT: results of computations or arbitrary values can be stored in variables we can re-use those variables later on and over-write them with new values 3.9.2 Data structures in R 3.9.2.1 Make a vector of 1,2,3,5 and 10 using c(), assign it to vec variable. Ex: vec1=c(1,3,4) makes a vector out of 1,3,4. 3.9.2.2 Check the length of your vector with length(). Ex: length(vec1) should return 3 3.9.2.3 Make a vector of all numbers between 2 and 15. Ex: vec=1:6 makes a vector of numbers between 1 and 6, assigns to vec variable 3.9.2.4 Make a vector of 4s repeated 10 times using rep() function. Ex: rep(x=2,times=5) makes a vector of 2s repeated 5 times 3.9.2.5 Make a logical vector with TRUE, FALSE values of length 4, use c(). Ex: c(TRUE,FALSE) 3.9.2.6 Make a character vector of gene names PAX6,ZIC2,OCT4 and SOX2. Ex: avec=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) a makes a character vector of a,b and c 3.9.2.7 Subset the vector using [] notation, get 5th and 6th elements. Ex: vec1[1] gets the first element. vec1[c(1,3)] gets 1st and 3rd elements 3.9.2.8 You can also subset any vector using a logical vector in []. Run the following: myvec=1:5 myvec[c(TRUE,TRUE,FALSE,FALSE,FALSE)] # the length of the logical vector should be equal to length(myvec) myvec[c(TRUE,FALSE,FALSE,FALSE,TRUE)] 3.9.2.9 ==,&gt;,&lt;, &gt;=, &lt;= operators create logical vectors. See the results of the following operations: myvec &gt; 3 myvec == 4 myvec &lt;= 2 myvec != 4 3.9.2.10 Use &gt; operator in myvec[ ] to get elements larger than 2 in myvec whic is described above 3.9.2.11 make a 5x3 matrix (5 rows, 3 columns) using matrix(). Ex: matrix(1:6,nrow=3,ncol=2) makes a 3x2 matrix using numbers between 1 and 6 3.9.2.12 What happens when you use byrow = TRUE in your matrix() as an additional argument? Ex: mat=matrix(1:6,nrow=3,ncol=2,byrow = TRUE) 3.9.2.13 Extract first 3 columns and first 3 rows of your matrix using [] notation. 3.9.2.14 Extract last two rows of the matrix you created earlier. Ex: mat[2:3,] or mat[c(2,3),] extracts 2nd and 3rd rows. 3.9.2.15 Extract the first two columns and run class() on the result. 3.9.2.16 Extract first column and run class() on the result, compare with the above exercise. 3.9.2.17 Make a data frame with 3 columns and 5 rows, make sure first column is sequence of numbers 1:5, and second column is a character vector. Ex: df=data.frame(col1=1:3,col2=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;),col3=3:1) # 3x3 data frame. Remember you need to make 3x5 data frame 3.9.2.18 Extract first two columns and first two rows. HINT: Same notation as matrices 3.9.2.19 Extract last two rows of the data frame you made. HINT: Same notation as matrices 3.9.2.20 Extract last two columns using column names of the data frame you made. 3.9.2.21 Extract second column using column names. You can use [] or $ as in lists, use both in two different answers. 3.9.2.22 Extract rows where 1st column is larger than 3. HINT: you can get a logical vector using &gt; operator ,logical vectors can be used in [] when subsetting. 3.9.2.23 Extract rows where 1st column is larger than or equal to 3. 3.9.2.24 Convert data frame to the matrix. HINT: use as.matrix(). Observe what happens to numeric values in the data frame. 3.9.2.25 Make a list using list() function, your list should have 4 elements the one below has 2. Ex: mylist= list(a=c(1,2,3),b=c(&quot;apple,&quot;orange&quot;)) 3.9.2.26 Select the 1st element of the list you made using $ notation. Ex: mylist$a selects first element named “a” 3.9.2.27 Select the 4th element of the list you made earlier using $ notation. 3.9.2.28 Select the 1st element of your list using [ ] notation. Ex: mylist[1] selects first element named “a”, you get a list with one element. Ex: mylist[&quot;a&quot;] selects first element named “a”, you get a list with one element. 3.9.2.29 select the 4th element of your list using ‘’ notation. 3.9.2.30 Make a factor using factor(), with 5 elements. Ex: fa=factor(c(&quot;a&quot;,&quot;a&quot;,&quot;b&quot;)) 3.9.2.31 Convert a character vector to factor using as.factor(). First, make a character vector using c() then use as.factor(). 3.9.2.32 Convert the factor you made above to character using as.character(). 3.9.3 Reading in and writing data out in R 3.9.3.1 Read CpG island (CpGi) data from intro2R_data/data/CpGi.table.hg18.txt, this is a tab-separated file, store it in a variable called “cpgi”. 3.9.3.2 Use head() on CpGi to see first few rows. 3.9.3.3 Why doesn’t the following work? see ‘sep’ argument at help(read.table). cpgiSepComma=read.table(&quot;intro2R_data/data/CpGi.table.hg18.txt&quot;,header=TRUE,sep=&quot;,&quot;) head(cpgiSepComma) ## chrom.chromStart.chromEnd.name.length.cpgNum.gcNum.perCpg.perGc.obsExp ## 1 chr1\\t18598\\t19673\\tCpG: 116\\t1075\\t116\\t787\\t21.6\\t73.2\\t0.83 ## 2 chr1\\t124987\\t125426\\tCpG: 30\\t439\\t30\\t295\\t13.7\\t67.2\\t0.64 ## 3 chr1\\t317653\\t318092\\tCpG: 29\\t439\\t29\\t295\\t13.2\\t67.2\\t0.62 ## 4 chr1\\t427014\\t428027\\tCpG: 84\\t1013\\t84\\t734\\t16.6\\t72.5\\t0.64 ## 5 chr1\\t439136\\t440407\\tCpG: 99\\t1271\\t99\\t777\\t15.6\\t61.1\\t0.84 ## 6 chr1\\t523082\\t523977\\tCpG: 94\\t895\\t94\\t570\\t21\\t63.7\\t1.04 3.9.3.4 What happens when stringsAsFactors=FALSE ? cpgiHF=read.table(&quot;intro2R_data/data/CpGi.table.hg18.txt&quot;,header=FALSE,sep=&quot;\\t&quot;, stringsAsFactors=FALSE) 3.9.3.5 Read only first 10 rows of the CpGi table. 3.9.3.6 Use read.table(&quot;intro2R_data/data/CpGi.table.hg18.txt&quot;,... with header=FALSE. Do head() to see the results. 3.9.3.7 Write CpG islands to a text file called “my.cpgi.file.txt”. Write the file to your home folder, you can use file=&quot;~/my.cpgi.file.txt&quot; in linux. ~/ denotes home folder. 3.9.3.8 Same as above but this time make sure use quote=FALSE,sep=&quot;\\t&quot; and row.names=FALSE arguments. Save the file to “my.cpgi.file2.txt” and compare it with “my.cpgi.file.txt” 3.9.3.9 Write out the first 10 rows of ‘cpgi’ data frame. HINT: use subsetting for data frames we learned before. 3.9.3.10 Write the first 3 columns of ‘cpgi’ data frame. 3.9.3.11 Write CpG islands only on chr1. HINT: use subsetting with [], feed a logical vector using == operator. 3.9.3.12 Read two other data sets “intro2R_data/data/rn4.refseq.bed” and “intro2R_data/data/rn4.refseq2name.txt” with header=FALSE, assign them to df1 and df2 respectively. 3.9.3.13 Use head() to see what is inside of the the data frames above. 3.9.3.14 Merge data sets using merge() and assign the results to variable named ‘new.df’, and use head() to see the results. 3.9.4 Plotting in R Please run the following for the rest of the exercises. set.seed(1001) x1=1:100+rnorm(100,mean=0,sd=15) y1=1:100 3.9.4.1 Make a scatter plot using x1 and y1 vectors generated above. 3.9.4.2 Use main argument to give a title to plot() as in plot(x,y,main=&quot;title&quot;) 3.9.4.3 Use xlab argument to set a label to x-axis.Use ylab argument to set a label to y-axis. 3.9.4.4 See what mtext(side=3,text=&quot;hi there&quot;) does. HINT: mtext stands for margin text. 3.9.4.5 See what mtext(side=2,text=&quot;hi there&quot;) does.check your plot after execution. 3.9.4.6 You can use paste() as ‘text’ argument in mtext() try that, you need to re-plot. your plot first. HINT: mtext(side=3,text=paste(...)) See what paste() is used for. paste(&quot;Text&quot;,&quot;here&quot;) ## [1] &quot;Text here&quot; myText=paste(&quot;Text&quot;,&quot;here&quot;) myText ## [1] &quot;Text here&quot; Use mtext() and paste() to put a margin text on the plot. 3.9.4.7 cor() calculates correlation between two vectors. Pearson correlation is a measure of the linear correlation (dependence) between two variables X and Y. corxy=cor(x1,y1) # calculates pearson correlation 3.9.4.8 Try use mtext(),cor() and paste() to display correlation coefficient on your scatterplot ? 3.9.4.9 Change the colors of your plot using col argument. Ex: plot(x,y,col=&quot;red&quot;) 3.9.4.10 Use pch=19 as an argument in your plot() command. 3.9.4.11 Use pch=18 as an argument to your plot() command. 3.9.4.12 Make histogram of x1 with hist() function.Histogram is a graphical representation of the data distribution. 3.9.4.13 You can change colors with ‘col’, add labels with ‘xlab’, ‘ylab’, and add a ‘title’ with ‘main’ arguments. Try all these in a histogram. 3.9.4.14 Make boxplot of y1 with boxplot(). 3.9.4.15 Make boxplots of x1 and y1 vectors in the same plot. 3.9.4.16 In boxplot use horizontal = TRUE argument 3.9.4.17 make multiple plots with par(mfrow=c(2,1)) 1. run par(mfrow=c(2,1)) 2. make a boxplot 3. make a histogram 3.9.4.18 Do the same as above but this time with par(mfrow=c(1,2)) 3.9.4.19 Save your plot using “Export” button 3.9.4.20 Save your plot by running : dev.copy(pdf,file=&quot;plot.file.pdf&quot;);dev.off() 3.9.4.21 Save your plot running : dev.copy(png,filename=&quot;plot.file.png&quot;);dev.off() 3.9.4.22 Another way to save the plot is the following Open a graphics device Create the plot Close the graphics device 3.9.4.23 EXTRA: Making color density scatterplot. You can make a scatter plot showing density of points rather than points themselves. If you use points it looks like this: x2=1:1000+rnorm(1000,mean=0,sd=200) y2=1:1000 plot(x2,y2,pch=19,col=&quot;blue&quot;) If you use smoothScatter() function, you get the densities. smoothScatter(x2,y2,colramp=colorRampPalette(c(&quot;white&quot;,&quot;blue&quot;, &quot;green&quot;,&quot;yellow&quot;,&quot;red&quot;))) Now, plot with colramp=heat.colors argument and then use a custom color scale using the following argument. colramp = colorRampPalette(c(&quot;white&quot;,&quot;blue&quot;, &quot;green&quot;,&quot;yellow&quot;,&quot;red&quot;))) 3.9.5 Functions and control structures (for, if/else etc.) 3.9.5.1 Read CpG island data cpgi=read.table(&quot;intro2R_data/data/CpGi.table.hg18.txt&quot;,header=TRUE,sep=&quot;\\t&quot;) head(cpgi) ## chrom chromStart chromEnd name length cpgNum gcNum perCpg perGc ## 1 chr1 18598 19673 CpG: 116 1075 116 787 21.6 73.2 ## 2 chr1 124987 125426 CpG: 30 439 30 295 13.7 67.2 ## 3 chr1 317653 318092 CpG: 29 439 29 295 13.2 67.2 ## 4 chr1 427014 428027 CpG: 84 1013 84 734 16.6 72.5 ## 5 chr1 439136 440407 CpG: 99 1271 99 777 15.6 61.1 ## 6 chr1 523082 523977 CpG: 94 895 94 570 21.0 63.7 ## obsExp ## 1 0.83 ## 2 0.64 ## 3 0.62 ## 4 0.64 ## 5 0.84 ## 6 1.04 3.9.5.2 Check values at perGc column using a histogram. ‘perGc’ stands for GC percent =&gt; percentage of C+G nucleotides 3.9.5.3 Make a boxplot for ‘perGc’ column 3.9.5.4 Use if/else structure to decide if given GC percent high, low or medium. If it is low, high, or medium. low &lt; 60, high&gt;75, medium is between 60 and 75 use greater or less than operators &lt; or &gt; . Fill in the values in the in code below, where it is written ‘YOU_FILL_IN’ GCper=65 #result=&quot;low&quot;# set initial value if(GCper &lt; 60){ # check if GC value is lower than 60, assign &quot;low&quot; to result result=&quot;low&quot; cat(&quot;low&quot;) } else if(GCper &gt; 75){ # check if GC value is higher than 75, assign &quot;high&quot; to result result=&quot;high&quot; cat(&quot;high&quot;) }else{ # if those two conditions fail then it must be &quot;medium&quot; result=&quot;medium&quot; } result 3.9.5.5 Write a function that takes a value of GC percent and decides if it is low, high, or medium. low &lt; 60, high&gt;75, medium is between 60 and 75. Fill in the values in the in code below, where it is written ‘YOU_FILL_IN’ GCclass&lt;-function(my.gc){ YOU_FILL_IN return(result) } GCclass(10) # should return &quot;low&quot; GCclass(90) # should return &quot;high&quot; GCclass(65) # should return &quot;medium&quot; 3.9.5.6 Use a for loop to get GC percentage classes for gcValues below. Use the function you wrote above. gcValues=c(10,50,70,65,90) for( i in YOU_FILL_IN){ YOU_FILL_IN } 3.9.5.7 Use lapply to get to get GC percentage classes for gcValues. Example: vec=c(1,2,4,5) power2=function(x){ return(x^2) } lapply(vec,power2) 3.9.5.8 Use sapply to get values to get GC percentage classes for gcValues 3.9.5.9 Is there a way to decide on the GC percentage class of given vector of GCpercentages without using if/else structure and loops ? if so, how can you do it? HINT: subsetting using &lt; and &gt; operators "],
["statistics-for-genomics.html", "Chapter 4 Statistics for Genomics 4.1 How to summarize collection of data points: The idea behind statistical distributions 4.2 How to test for differences in samples 4.3 Relationship between variables: linear models and correlation 4.4 Roadmap for future 4.5 Exercises", " Chapter 4 Statistics for Genomics This chapter will summarize statistics and machine-learning methods frequently used in computational genomics. As these fields are continuously evolving, the techniques introduced here do not form an exhaustive list but mostly corner stone methods that are often and still being used. In addition, we focused on giving intuitive and practical understanding of the methods with relevant examples from the field. If you want to dig deeper into statistics and math, beyond what is described here, we included appropriate references with annotation after each major section. 4.1 How to summarize collection of data points: The idea behind statistical distributions In biology and many other fields data is collected via experimentation. The nature of the experiments and natural variation in biology makes it impossible to get the same exact measurements every time you measure something. For example, if you are measuring gene expression values for a certain gene, say PAX6, and let’s assume you are measuring expression per sample and cell with any method( microarrays, rt-qPCR, etc.). You will not get the same expression value even if your samples are homogeneous. Due to technical bias in experiments or natural variation in the samples. Instead, we would like to describe this collection of data some other way that represents the general properties of the data. The figure shows a sample of 20 expression values from PAX6 gene. 4.1.1 Describing the central tendency: mean and median As seen in the figure above, the points from this sample are distributed around a central value and the histogram below the dot plot shows number of points in each bin. Another observation is that there are some bins that have more points than others. If we want to summarize what we observe, we can try to represent the collection of data points with an expression value that is typical to get, something that represents the general tendency we observe on the dot plot and the histogram. This value is sometimes called central value or central tendency, and there are different ways to calculate such a value. In the figure above, we see that all the values are spread around 6.13 (red line), and that is indeed what we call mean value of this sample of expression values. It can be calculated with the following formula \\(\\overline{X}=\\sum_{i=1}^n x_i/n\\), where \\(x_i\\) is the expression value of an experiment and \\(n\\) is the number of expression value obtained from the experiments. In R, mean() function will calculate the mean of a provided vector of numbers. This is called a “sample mean”. In reality the possible values of PAX6 expression for all cells (provided each cell is of the identical cell type and is in identical conditions) are much much more than 20. If we had the time and the funding to sample all cells and measure PAX6 expression we would get a collection values that would be called, in statistics, a “population”. In our case the population will look like the left hand side of the figure below. What we have done with our 20 data points is that we took a sample of PAX6 expression values from this population, and calculated the sample mean. The mean of the population is calculated the same way but traditionally Greek letter \\(\\mu\\) is used to denote the population mean. Normally, we would not have access to the population and we will use sample mean and other quantities derived from the sample to estimate the population properties. This is the basic idea behind statistical inference which we will see this in action in later sections as well. We estimate the population parameters from the sample parameters and there is some uncertainty associated with those estimates. We will be trying to assess those uncertainties and make decisions in the presence of those uncertainties. We are not yet done with measuring central tendency. There are other ways to describe it, such as the median value. Mean can be affected by outliers easily. If certain values are very high or low from the bulk of the sample this will shift mean towards those outliers. However, median is not affected by outliers. It is simply the value in a distribution where half of the values are above and the other half is below. In R, median() function will calculate the mean of a provided vector of numbers. Let’s create a set of random numbers and calculate their mean and median using R. #create 10 random numbers from uniform distribution x=runif(10) # calculate mean mean(x) ## [1] 0.3738963 # calculate median median(x) ## [1] 0.3277896 4.1.2 Describing the spread: measurements of variation Another useful way to summarize a collection of data points is to measure how variable the values are. You can simply describe the range of the values , such as minimum and maximum values. You can easily do that in R with range() function. A more common way to calculate variation is by calculating something called “standard deviation” or the related quantity called “variance”. This is a quantity that shows how variable the values are, a value around zero indicates there is not much variation in the values of the data points, and a high value indicates high variation in the values. The variance is the squared distance of data points from the mean. Population variance is again a quantity we usually do not have access to and is simply calculate as follows \\(\\sigma^2=\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{n}\\), where \\(\\mu\\) is the population mean, \\(x_i\\) is the ith data point in the population and \\(n\\) is the population size. However, when the we have only access to a sample this formulation is biased. It means that it underestimates the population variance, so we make a small adjustment when we calculate the sample variance, denoted as \\(s^2\\): \\[ \\begin{align} s^2=\\sum_{i=1}^n \\frac{(x_i-\\overline{X})^2}{n-1} &amp;&amp; \\text{ where $x_i$ is the ith data point and $\\overline{X}$ is the sample mean.} \\end{align} \\] The sample standard deviation is simply the square-root of the sample variance. The good thing about standard deviation is that it has the same unit as the mean so it is more intuitive. \\[s=\\sqrt{\\sum_{i=1}^n \\frac{(x_i-\\overline{X})^2}{n-1}}\\] We can calculate sample standard deviation and variation with sd() and var() functions in R. These functions take vector of numeric values as input and calculate the desired quantities. Below we use those functions on a randomly generated vector of numbers. x=rnorm(20,mean=6,sd=0.7) var(x) ## [1] 0.2531495 sd(x) ## [1] 0.5031397 One potential problem with the variance is that it could be affected by outliers. The points that are too far away from the mean will have a large affect on the variance even though there might be few of them. A way to measure variance that could be less affected by outliers is looking at where bulk of the distribution is. How do we define where the bulk is? One common way is to look at the the difference between 75th percentile and 25th percentile, this effectively removes a lot of potential outliers which will be towards the edges of the range of values. This is called interquartile range , and can be easily calculated using R via IQR() function and the quantiles of a vector is calculated with quantile() function. Let us plot the boxplot for a random vector and also calculate IQR using R. In the boxplot below, 25th and 75th percentiles are the edges of the box, and the median is marked with a thick line going through roughly middle the box. x=rnorm(20,mean=6,sd=0.7) IQR(x) ## [1] 0.5010954 quantile(x) ## 0% 25% 50% 75% 100% ## 5.437119 5.742895 5.860302 6.243991 6.558112 boxplot(x,horizontal = T) 4.1.2.1 Frequently used statistical distributions The distributions have parameters (such as mean and variance) that summarizes them but also they are functions that assigns each outcome of a statistical experiment to its probability of occurrence. One distribution that you will frequently encounter is the normal distribution or Gaussian distribution. The normal distribution has a typical “bell-curve” shape and, characterized by mean and standard deviation. A set of data points that follow normal distribution mostly will be close to the mean but spread around it controlled by the standard deviation parameter. That means if we sample data points from a normal distribution we are more likely to sample nearby the mean and sometimes away from the mean. Probability of an event occurring is higher if it is nearby the mean. The effect of the parameters for normal distribution can be observed in the following plot. The normal distribution is often denoted by \\(\\mathcal{N}(\\mu,\\,\\sigma^2)\\) When a random variable \\(X\\) is distributed normally with mean \\(\\mu\\) and variance \\(\\sigma^2\\), we write: \\[X\\ \\sim\\ \\mathcal{N}(\\mu,\\,\\sigma^2).\\] The probability density function of Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is as follows \\[P(x)=\\frac{1}{\\sigma\\sqrt{2\\pi} } \\; e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} } \\] The probability density function gives the probability of observing a value on a normal distribution defined by \\(\\mu\\) and \\(\\sigma\\) parameters. Often times, we do not need the exact probability of a value but we need the probability of observing a value larger or smaller than a critical value or reference point. For example, we might want to know the probability of \\(X\\) being smaller than or equal to -2 for a normal distribution with mean 0 and standard deviation 2. ,\\(P(X &lt;= -2 \\; | \\; \\mu=0,\\sigma=2)\\). In this case, what we want is the are under the curve shaded in blue. To be able to that we need to integrate the probability density function but we will usually let software do that. Traditional, you calculate a Z-score which is simply \\((X-\\mu)/\\sigma=(-2-0)/2=1\\), and corresponds to how many standard deviations you are away from the mean. This is also called “standardization”, the corresponding value is distributed in “standard normal distribution” where \\(\\mathcal{N}(0,\\,1)\\). After calculating the Z-score, we can go look up in a table, that contains the area under the curve for the left and right side of the Z-score, but again use software for that tables are old school. Below we are showing the Z-score and the associated probabilities derived from the calculation above for \\(P(X &lt;= -2 \\; | \\; \\mu=0,\\sigma=2)\\). In R, family of *norm functions (rnorm,dnorm,qnorm and pnorm) can be used to operate with normal distribution, such as calculating probabilities and generating random numbers drawn from normal distribution. # get the probability of P(X= -2) where mean=0 and sd=2 dnorm(-2, mean=0, sd=2) ## [1] 0.1209854 # get the probability of P(X =&lt; -2) where mean=0 and sd=2 pnorm(-2, mean=0, sd=2) ## [1] 0.1586553 # get the probability of P(X &gt; -2) where mean=0 and sd=2 pnorm(-2, mean=0, sd=2,lower.tail = FALSE) ## [1] 0.8413447 # get 5 random numbers from normal dist with mean=0 and sd=2 rnorm(5, mean=0 , sd=2) ## [1] -1.8109030 -1.9220710 -0.5146717 0.8216728 -0.7900804 # get y value corresponding to P(X &gt; y) = 0.15 with mean=0 and sd=2 qnorm( 0.15, mean=0 , sd=2) ## [1] -2.072867 There are many other distribution functions in R that can be used the same way. You have to enter the distribution specific parameters along with your critical value, quantiles or number of random numbers depending on which function you are using in the family.We will list some of those functions below. dbinom is for binomial distribution. This distribution is usually used to model fractional data and binary data. Examples from genomics includes methylation data. dpois is used for Poisson distribution and dnbinom is used for negative binomial distribution. These distributions are used to model count data such as sequencing read counts. df (F distribution) and dchisq (Chi-Squared distribution) are used in relation to distribution of variation. F distribution is used to model ratios of variation and Chi-Squared distribution is used to model distribution of variations. You will frequently encounter these in linear models and generalized linear models. 4.1.3 Precision of estimates: Confidence intervals When we take a random sample from a population and compute a statistic, such as the mean, we are trying to approximate the mean of the population. How well this sample statistic estimates the population value will always be a concern. A confidence interval addresses this concern because it provides a range of values which is plausible to contain the population parameter of interest. Normally, we would not have access to a population. If we did, we would not have to estimate the population parameters and its precision. When we do not have access to the population, one way to estimate intervals is to repeatedly take samples from the original sample with replacement, that is we take a data point from the sample we replace, and we take another data point until we have sample size of the original sample. Then, we calculate the parameter of interest, in this case mean, and repeat this step a large number of times, such as 1000. At this point, we would have a distribution of re-sampled means, we can then calculate the 2.5th and 97.5th percentiles and these will be our so-called 95% confidence interval. This procedure, resampling with replacement to estimate the precision of population parameter estimates, is known as the bootstrap. Let’s see how we can do this in practice. We simulate a sample coming from a normal distribution (but we pretend we don’t know the population parameters). We will try to estimate the precision of the mean of the sample using bootstrap to build confidence intervals. set.seed(21) require(mosaic) sample1= rnorm(50,20,5) # simulate a sample # do bootstrap resampling, sampling with replacement boot.means=do(1000) * mean(resample(sample1)) # get percentiles from the bootstrap means q=quantile(boot.means[,1],p=c(0.025,0.975)) # plot the histogram hist(boot.means[,1],col=&quot;cornflowerblue&quot;,border=&quot;white&quot;, xlab=&quot;sample means&quot;,main=&quot;1000 bootstrap means&quot;) abline(v=c(q[1], q[2] ),col=&quot;red&quot;) text(x=q[1],y=200,round(q[1],3),adj=c(1,0)) text(x=q[2],y=200,round(q[2],3),adj=c(0,0)) If we had a convenient mathematical method to calculate confidence interval we could also do without resampling methods. It turns out that if we take repeated samples from a population of with sample size \\(n\\), the distribution of means ( \\(\\overline{X}\\)) of those samples will be approximately normal with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{n}\\). This is also known as Central Limit Theorem(CLT) and is one of the most important theorems in statistics. This also means that \\(\\frac{\\overline{X}-\\mu}{\\sigma\\sqrt{n}}\\) has a standard normal distribution and we can calculate the Z-score and then we can get the percentiles associated with the Z-score. Below, we are showing the Z-score calculation for the distribution of \\(\\overline{X}\\), and then we are deriving the confidence intervals starting with the fact that probability of Z being between -1.96 and 1.96 is 0.95. We then use algebra to show that the probability that unknown \\(\\mu\\) is captured between \\(\\overline{X}-1.96\\sigma\\sqrt{n}\\) and \\(\\overline{X}+1.96\\sigma\\sqrt{n}\\) is 0.95, which is commonly known as 95% confidence interval. \\[ \\begin{equation} Z=\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}\\\\ P(-1.96 &lt; Z &lt; 1.96)=0.95 \\\\ P(-1.96 &lt; \\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}} &lt; 1.96)=0.95\\\\ P(\\mu-1.96\\sigma\\sqrt{n} &lt; \\overline{X} &lt; \\mu+1.96\\sigma\\sqrt{n})=0.95\\\\ P(\\overline{X}-1.96\\sigma\\sqrt{n} &lt; \\mu &lt; \\overline{X}+1.96\\sigma\\sqrt{n})=0.95\\\\ confint=[\\overline{X}-1.96\\sigma\\sqrt{n},\\overline{X}+1.96\\sigma\\sqrt{n}] \\end{equation} \\] A 95% confidence interval for population mean is the most common common interval to use, and would mean that we would expect 95% of the interval estimates to include the population parameter, in this case mean. However, we can pick any value such as 99% or 90%. We can generalize the confidence interval for \\((1-\\alpha)100%\\) as follows: \\[\\overline{X} \\pm Z_{\\alpha/2}\\sigma\\sqrt{n}\\] In R, we can do this using qnorm() function to get Z-scores associated with \\({\\alpha/2}\\) and \\({1-\\alpha/2}\\). As you can see, the confidence intervals we calculated using CLT are very similar to the ones we got from bootstrap for the same sample. For bootstrap we got \\([19.21, 21.989]\\) and for the CLT based estimate we got \\([19.23638, 22.00819]\\). alpha=0.05 sd=5 n=50 mean(sample1)+qnorm(c(alpha/2,1-alpha/2))*sd/sqrt(n) ## [1] 19.23638 22.00819 The good thing about CLT as long as the sample size is large regardless of the population distribution, the distribution of sample means drawn from that population will always be normal. Here we are repeatedly drawing samples 1000 times with sample size \\(n\\)=10,30, and 100 from a bimodal, exponential and a uniform distribution and we are getting sample mean distributions following normal distribution. However, we should note that how we constructed the confidence interval using standard normal distribution, \\(N(0,1)\\), only works when the when we know the population standard deviation. In reality, we usually have only access to a sample and have no idea about the population standard deviation. If this is the case we should use estimate the standard deviation using sample standard deviation and use something called t distribution instead of standard normal distribution in our interval calculation. Our confidence interval becomes \\(\\overline{X} \\pm t_{\\alpha/2}s\\sqrt{n}\\), with t distribution parameter \\(d.f=n-1\\), since now the following quantity is t distributed \\(\\frac{\\overline{X}-\\mu}{s/\\sqrt{n}}\\) instead of standard normal distribution. The t distribution is similar to standard normal distribution has mean 0 but its spread is larger than the normal distribution especially when sample size is small, and has one parameter \\(v\\) for the degrees of freedom, which is \\(n-1\\) in this case. Degrees of freedom is simply number of data points minus number of parameters estimated. Here we are estimating the mean from the data and the distribution is for the means, therefore degrees of freedom is \\(n-1\\). 4.2 How to test for differences in samples Often times we would want to compare sets of samples. Such comparisons include if wild-type samples have different expression compared to mutants or if healthy samples are different from disease samples in some measurable feature (blood count, gene expression, methylation of certain loci). Since there is variability in our measurements, we need to take that into account when comparing the sets of samples. We can simply subtract the means of two samples, but given the variability of sampling, at the very least we need to decide a cutoff value for differences of means, small differences of means can be explained by random chance due to sampling. That means we need to compare the difference we get to a value that is typical to get if the difference between two group means were only due to sampling. If you followed the logic above, here we actually introduced two core ideas of something called “hypothesis testing”, this is simply using statistics to determine the probability that a given hypothesis (if two sample sets are from the same population or not) is true. Formally, those two core ideas are as follows: Decide on a hypothesis to test, often called “null hypothesis” (\\(H_0\\)). In our case, the hypothesis is there is no difference between sets of samples. An the “Alternative hypothesis” (\\(H_1\\)) is there is a difference between the samples. Decide on a statistic to test the truth of the null hypothesis. Calculate the statistic Compare it to a reference value to establish significance, the P-value. Based on that either accept or reject the null hypothesis, \\(H_0\\) 4.2.1 randomization based testing for difference of the means There is one intuitive way to go about this. If we believe there are no differences between samples that means the sample labels (test-control or healthy-disease) has no meaning. So, if we randomly assign labels to the samples that and calculate the difference of the mean, this creates a null distribution for the \\(H_0\\) where we can compare the real difference and measure how unlikely it is to get such a value under the expectation of the null hypothesis. We can calculate all possible permutations to calculate the null distribution. However, sometimes that is not very feasible and equivalent approach would be generating the null distribution by taking a smaller number of random samples with shuffled group membership. Below, we are doing this process in R. We are first simulating two samples from two different distributions. These would be equivalent to gene expression measurements obtained under different conditions. Then, we calculate the differences in the means and do the randomization procedure to get a null distribution when we assume there is no difference between samples, \\(H_0\\). We than calculate how often we would get the original difference we calculated under the assumption that \\(H_0\\) is true. set.seed(100) gene1=rnorm(30,mean=4,sd=2) gene2=rnorm(30,mean=2,sd=2) org.diff=mean(gene1)-mean(gene2) gene.df=data.frame(exp=c(gene1,gene2), group=c( rep(&quot;test&quot;,30),rep(&quot;control&quot;,30) ) ) exp.null &lt;- do(1000) * diff(mean(exp ~ shuffle(group), data=gene.df)) hist(exp.null[,1],xlab=&quot;null distribution | no difference in samples&quot;, main=expression(paste(H[0],&quot; :no difference in means&quot;) ), xlim=c(-2,2),col=&quot;cornflowerblue&quot;,border=&quot;white&quot;) abline(v=quantile(exp.null[,1],0.95),col=&quot;red&quot; ) abline(v=org.diff,col=&quot;blue&quot; ) text(x=quantile(exp.null[,1],0.95),y=200,&quot;0.05&quot;,adj=c(1,0),col=&quot;red&quot;) text(x=org.diff,y=200,&quot;org. diff.&quot;,adj=c(1,0),col=&quot;blue&quot;) p.val=sum(exp.null[,1]&gt;org.diff)/length(exp.null[,1]) p.val ## [1] 0 After doing random permutations and getting a null distribution, it is possible to get a confidence interval for the distribution of difference in means. This is simply the 2.5th and 97.5th percentiles of the null distribution, and directly related to the P-value calculation above. 4.2.2 Using t-test for difference of the means between two samples We can also calculate the difference between means using a t-test. Sometimes we will have too few data points in a sample to do meaningful randomization test, also randomization takes more time than doing a t-test. This is a test that depends on t distribution. The line of thought follows from the CLT and we can show differences in means are t distributed. There are couple of variants of the t-test for this purpose. If we assume the variances are equal we can use the following version \\[t = \\frac{\\bar {X}_1 - \\bar{X}_2}{s_{X_1X_2} \\cdot \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\\] where \\[s_{X_1X_2} = \\sqrt{\\frac{(n_1-1)s_{X_1}^2+(n_2-1)s_{X_2}^2}{n_1+n_2-2}}\\] In the first equation above the quantity is t distributed with \\(n_1+n_2-2\\) degrees of freedom. We can calculate the quantity then use software to look for the percentile of that value in that t distribution, which is our P-value. When we can not assume equal variances we use “Welch’s t-test” which is the default t-test in R and also works well when variances and the sample sizes are the same. For this test we calculate the following quantity: \\[t = {\\overline{X}_1 - \\overline{X}_2 \\over s_{\\overline{X}_1 - \\overline{X}_2}}\\] where \\[s_{\\overline{X}_1 - \\overline{X}_2} = \\sqrt{{s_1^2 \\over n_1} + {s_2^2 \\over n_2}} \\] and the degrees of freedom equals to: \\[\\mathrm{d.f.} = \\frac{(s_1^2/n_1 + s_2^2/n_2)^2}{(s_1^2/n_1)^2/(n_1-1) + (s_2^2/n_2)^2/(n_2-1)} \\] Luckily, R does all those calculations for us. Below we will show the use of t.test() function in R. We will use it on the samples we simulated above. # Welch&#39;s t-test stats::t.test(gene1,gene2) ## ## Welch Two Sample t-test ## ## data: gene1 and gene2 ## t = 3.7653, df = 47.552, p-value = 0.0004575 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.872397 2.872761 ## sample estimates: ## mean of x mean of y ## 4.057728 2.185149 # t-test with equal varience assumption stats::t.test(gene1,gene2,var.equal=TRUE) ## ## Two Sample t-test ## ## data: gene1 and gene2 ## t = 3.7653, df = 58, p-value = 0.0003905 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.8770753 2.8680832 ## sample estimates: ## mean of x mean of y ## 4.057728 2.185149 A final word on t-tests: they generally assume population where samples coming from have normal distribution, however it is been shown t-test can tolerate deviations from normality. Especially, when two distributions are moderately skewed in the same direction. This is due to central limit theorem which says means of samples will be distributed normally no matter the population distribution if sample sizes are large. 4.2.3 multiple testing correction We should think of hypothesis testing as a non-error-free method of making decisions. There will be times when we declare something significant and accept \\(H_1\\) but we will be wrong. These decisions are also called “false positives” or “false discoveries”, this is also known as “type I error”. Similarly, we can fail to reject a hypothesis when we actually should. These cases are known as “false negatives”, also known as “type II error”. The ratio of true negatives to the sum of true negatives and false positives (\\(\\frac{TN}{FP+TN}\\)) is known as specificity. And we usually want to decrease the FP and get higher specificity. The ratio of true positives to the sum of true positives and false negatives (\\(\\frac{TP}{TP+FN}\\)) is known as sensitivity. And, again we usually want to decrease the FN and get higher sensitivity. Sensitivity is also known as “power of a test” in the context of hypothesis testing. More powerful tests will be highly sensitive and will do less type II errors. For the t-test the power is positively associated with sample size and the effect size. Higher the sample size, smaller the standard error and looking for the larger effect sizes will similarly increase the power. The general summary of these the different combination of the decisions are included in the table below. \\(H_0\\) is TRUE, [Gene is NOT differentially expressed] \\(H_1\\) is TRUE, [Gene is differentially expressed] Accept \\(H_0\\) (claim that the gene is not differentially expressed) True Negatives (TN) False Negatives (FN) ,type II error \\(m_0\\): number of truly null hypotheses reject \\(H_0\\) (claim that the gene is differentially expressed) False Positives (FP) ,type I error True Positives (TP) \\(m-m_0\\): number of truly alternative hypotheses We expect to make more type I errors as the number of tests increase, that means we will reject the null hypothesis by mistake. For example, if we perform a test the 5% significance level, there is a 5% chance of incorrectly rejecting the null hypothesis if the null hypothesis is true. However, if we make 1000 tests where all null hypotheses are true for each of them, the average number of incorrect rejections is 50. And if we apply the rules of probability, there are is almost a 100% chance that we will have at least one incorrect rejection. There are multiple statistical techniques to prevent this from happening. These techniques generally shrink the P-values obtained from multiple tests to higher values, if the individual P-value is low enough it survives this process. The most simple method is just to multiply the individual, P-value (\\(p_i\\)) with the number of tests (\\(m\\)): \\(m \\cdot p_i\\), this is called “Bonferroni correction”. However, this is too harsh if you have thousands of tests. Other methods are developed to remedy this. Those methods rely on ranking the P-values and dividing \\(m \\cdot p_i\\) by the rank,\\(i\\), :\\(\\frac{m \\cdot p_i }{i}\\), this is derived from Benjamini–Hochberg procedure. This procedure is developed to control for “False Discovery Rate (FDR)” , which is proportion of false positives among all significant tests. And in practical terms, we get the “FDR adjusted P-value” from the procedure described above. This gives us an estimate of proportion of false discoveries for a given test. To elaborate, p-value of 0.05 implies that 5% of all tests will be false positives. An FDR adjusted p-value of 0.05 implies that 5% of significant tests will be false positives. The FDR adjusted P-values will result in a lower number of false positives. One final method that is also popular is called the “q-value” method and related to the method above. This procedure relies on estimating the proportion of true null hypotheses from the distribution of raw p-values and using that quantity to come up with what is called a “q-value”, which is also an FDR adjusted P-value . That can be practically defined as “the proportion of significant features that turn out to be false leads.” A q-value 0.01 would mean 1% of the tests called significant at this level will be truly null on average. Within the genomics community q-value and FDR adjusted P-value are synonymous although they can be calculated differently. In R, the base function p.adjust() implements most of the p-value correction methods described above. For the q-value, we can use the qvalue package from Bioconductor. Below we are demonstrating how to use them on a set of simulated p-values.The plot shows that Bonferroni correction does a terrible job. FDR(BH) and q-value approach are better but q-value approach is more permissive than FDR(BH). library(qvalue) data(hedenfalk) qvalues &lt;- qvalue(hedenfalk$p)$q bonf.pval=p.adjust(hedenfalk$p,method =&quot;bonferroni&quot;) fdr.adj.pval=p.adjust(hedenfalk$p,method =&quot;fdr&quot;) plot(hedenfalk$p,qvalues,pch=19,ylim=c(0,1), xlab=&quot;raw P-values&quot;,ylab=&quot;adjusted P-values&quot;) points(hedenfalk$p,bonf.pval,pch=19,col=&quot;red&quot;) points(hedenfalk$p,fdr.adj.pval,pch=19,col=&quot;blue&quot;) legend(&quot;bottomright&quot;,legend=c(&quot;q-value&quot;,&quot;FDR (BH)&quot;,&quot;Bonferroni&quot;), fill=c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;)) 4.2.4 moderated t-tests: using information from multiple comparisons In genomics, we usually do not do one test but many, as described above. That means we may be able to use the information from the parameters obtained from all comparisons to influence the individual parameters. For example, if you have many variances calculated for thousands of genes across samples, you can force individual variance estimates to shrunk towards the mean or the median of the distribution of variances. This usually creates better performance in individual variance estimates and therefore better performance in significance testing which depends on variance estimates. How much the values be shrunk towards a common value comes in many flavors. These tests in general are called moderated t-tests or shrinkage t-tests. One approach popularized by Limma software is to use so-called “Empirical Bayesian methods”. The main formulation in these methods is \\(\\hat{V_g} = aV_0 + bV_g\\), where \\(V_0\\) is the background variability and \\(V_g\\) is the individual variability. Then, these methods estimate \\(a\\) and \\(b\\) in various ways to come up with shrunk version of variability, \\(\\hat{V_g}\\). In a Bayesian viewpoint, the prior knowledge is used to calculate the variability of an individual gene. In this case, \\(V_0\\) would be the prior knowledge we have on variability of the genes and we use that knowledge to influence our estimate for the individual genes. Below we are simulating a gene expression matrix with 1000 genes, and 3 test and 3 control groups. Each row is a gene and in normal circumstances we would like to find out differentially expressed genes. In this case, we are simulating them from the same distribution so in reality we do not expect any differences. We then use the adjusted standard error estimates in empirical Bayesian spirit but in a very crude way. We just shrink the gene-wise standard error estimates towards the median with equal \\(a\\) and \\(b\\) weights. That is to say, we add individual estimate to the median of standard error distribution from all genes and divide that quantity by 2. So if we plug that in the to the above formula what we do is: \\[ \\hat{V_g} = (V_0 + V_g)/2 \\] In the code below, we are avoiding for loops or apply family functions by using vectorized operations. set.seed(100) #sample data matrix from normal distribution gset=rnorm(3000,mean=200,sd=70) data=matrix(gset,ncol=6) # set groups group1=1:3 group2=4:6 n1=3 n2=3 dx=rowMeans(data[,group1])-rowMeans(data[,group2]) require(matrixStats) # get the esimate of pooled variance stderr &lt;- sqrt( (rowVars(data[,group1])*(n1-1) + rowVars(data[,group2])*(n2-1)) / (n1+n2-2) * ( 1/n1 + 1/n2 )) # do the shrinking towards median mod.stderr &lt;- (stderr + median(stderr)) / 2 # moderation in variation # esimate t statistic with moderated variance t.mod = dx / mod.stderr # calculate P-value of rejecting null p.mod = 2*pt( -abs(t.mod), n1+n2-2 ) # esimate t statistic without moderated variance t = dx / stderr # calculate P-value of rejecting null p = 2*pt( -abs(t), n1+n2-2 ) par(mfrow=c(1,2)) hist(p,col=&quot;cornflowerblue&quot;,border=&quot;white&quot;,main=&quot;&quot;,xlab=&quot;P-values t-test&quot;) mtext(paste(&quot;signifcant tests:&quot;,sum(p&lt;0.05)) ) hist(p.mod,col=&quot;cornflowerblue&quot;,border=&quot;white&quot;,main=&quot;&quot;,xlab=&quot;P-values mod. t-test&quot;) mtext(paste(&quot;signifcant tests:&quot;,sum(p.mod&lt;0.05)) ) 4.2.5 Want to know more… basic statistical concepts “Cartoon guide to statistics” by Gonick &amp; Smith “Introduction to statistics” by Mine Rundel, et al. (Free e-book) Hands-on statistics recipes with R “The R book” by Crawley moderated tests comparison of moderated tests for differential expression http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-17 limma method: Smyth, G. K. (2004). Linear models and empirical Bayes methods for assessing differential expression in microarray experiments. Statistical Applications in Genetics and Molecular Biology, 3, No. 1, Article 3. http://www.statsci.org/smyth/pubs/ebayes.pdf 4.3 Relationship between variables: linear models and correlation In genomics, we would often need to measure or model the relationship between variables. We might want to know about expression of a particular gene in liver in relation to the dosage of a drug that patient receives. Or, we may want to know DNA methylation of certain locus in the genome in relation to age of the sample donor’s. Or, we might be interested in the relationship between histone modifications and gene expression. Is there a linear relationship, the more histone modification the more the gene is expressed ? In these situations and many more, linear regression or linear models can be used to model the relationship with a “dependent” or “response” variable (expression or methylation in the above examples) and one or more “independent”&quot; or “explanatory” variables (age, drug dosage or histone modification in the above examples). Our simple linear model has the following components. \\[ Y= \\beta_0+\\beta_1X + \\epsilon \\] In the equation above, \\(Y\\) is the response variable and \\(X\\) is the explanatory variable. \\(\\epsilon\\) is the mean-zero error term. Since, the line fit will not be able to precisely predict the \\(Y\\) values, there will be some error associated with each prediction when we compare it to the original \\(Y\\) values. This error is captured in \\(\\epsilon\\) term. We can alternatively write the model as follows to emphasize that the model approximates \\(Y\\), in this case notice that we removed the \\(\\epsilon\\) term: \\(Y \\sim \\beta_0+\\beta_1X\\) The graph below shows the relationship between histone modification (trimethylated forms of histone H3 at lysine 4, aka H3K4me3) and gene expression for 100 genes. The blue line is our model with estimated coefficients (\\(\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1X\\), where \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) the estimated values of \\(\\beta_0\\) and \\(\\beta_1\\), and \\(\\hat{y}\\) indicates the prediction). The red lines indicate the individual errors per data point, indicated as \\(\\epsilon\\) in the formula above. There could be more than one explanatory variable, we then simply add more \\(X\\) and \\(\\beta\\) to our model. If there are two explanatory variables our model will look like this: \\[ Y= \\beta_0+\\beta_1X_1 +\\beta_2X_2 + \\epsilon \\] In this case, we will be fitting a plane rather than a line. However, the fitting process which we will describe in the later sections will not change. For our gene expression problem. We can introduce one more histone modification, H3K27me3. We will then have a linear model with 2 explanatory variables and the fitted plane will look like the one below. The gene expression values are shown as dots below and above the fitted plane. 4.3.0.1 Matrix notation for linear models We can naturally have more explanatory variables than just two.The formula below has \\(n\\) explanatory variables. \\[Y= \\beta_0+\\beta_1X_1+\\beta_2X_2 + \\beta_3X_3 + .. + \\beta_nX_n +\\epsilon\\] If there are many variables, it would be easier to write the model in matrix notation. The matrix form of linear model with two explanatory variables will look like the one below. First matrix would be our data matrix. This contains our explanatory variables and a column of 1s. The second term is a column vector of \\(\\beta\\) values. We add a vector of error terms,\\(\\epsilon\\)s to the matrix multiplication. \\[\\mathbf{Y} = \\left[\\begin{array} {r,r,r} 1 &amp; X_{1,1} &amp; X_{1,2} \\\\ 1 &amp; X_{2,1} &amp; X_{2,2} \\\\ 1 &amp; X_{3,1} &amp; X_{3,2} \\\\ 1 &amp; X_{4,1} &amp; X_{4,2} \\end{array}\\right] % \\left[\\begin{array} {r,r,r} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{array}\\right] % + \\left[\\begin{array} {r,r,r} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_0 \\end{array}\\right] \\] The multiplication of data matrix and \\(\\beta\\) vector and addition of the error terms simply results in the the following set of equations per data point: \\[ \\begin{align} Y_1= \\beta_0+\\beta_1X_{1,1}+\\beta_2X_{1,2} +\\epsilon_1 \\\\ Y_2= \\beta_0+\\beta_1X_{2,1}+\\beta_2X_{2,2} +\\epsilon_2 \\\\ Y_3= \\beta_0+\\beta_1X_{3,1}+\\beta_2X_{3,2} +\\epsilon_3 \\\\ Y_4= \\beta_0+\\beta_1X_{4,1}+\\beta_2X_{4,2} +\\epsilon_4 \\end{align} \\] This expression involving the multiplication of the data matrix, the \\(\\beta\\) vector and vector of error terms (\\(\\epsilon\\)) could be simply written as follows. \\[Y=X\\beta + \\epsilon\\] In the equation above \\(Y\\) is the vector of response variables and \\(X\\) is the data matrix and \\(\\beta\\) is the vector of coefficients. This notation is more concise and often used in scientific papers. However, this also means you need some understanding of linear algebra to follow the math laid out in such resources. 4.3.1 How to fit a line At this point a major questions is left unanswered: How did we fit this line? We basically need to define \\(\\beta\\) values in a structured way. There are multiple ways or understanding how to do this, all of which converges to the same end point. We will describe them one by one. 4.3.1.1 The cost or loss function approach This is the first approach and in my opinion is easiest to understand. We try to optimize a function, often called “cost function” or “loss function”. The cost function is the sum of squared differences between the predicted \\(\\hat{Y}\\) values from our model and the original \\(Y\\) values. The optimization procedure tries to find \\(\\beta\\) values that minimizes this difference between reality and the predicted values. \\[min \\sum{(y_i-(\\beta_0+\\beta_1x_i))^2}\\] Note that this is related to the the error term, \\(\\epsilon\\), we already mentioned above, we are trying to minimize the squared sum of \\(\\epsilon_i\\) for each data point. We can do this minimization by a bit of calculus. The rough algorithm is as follows: Pick a random starting point, random \\(\\beta\\) values Take the partial derivatives of the cost function to see which direction is the way to go in the cost function. Take a step toward the direction that minimizes the cost function. step size is parameter to choose, there are many variants. repeat step 2,3 until convergence. This is the basis of “gradient descent” algorithm. With the help of partial derivatives we define a “gradient” on the cost function and follow that through multiple iterations and until convergence, meaning until the results do not improve defined by a margin. The algorithm usually converges to optimum \\(\\beta\\) values. Below, we show the cost function over various \\(\\beta_0\\) and \\(\\beta_1\\) values for the histone modification and gene expression data set. The algorithm will pick a point on this graph and traverse it incrementally based on the derivatives and converge on the bottom of the cost function “well”. -://..com/watch?v=5Q 4.3.1.2 Not cost function but maximum likelihood function We can also think of this problem from more a statistical point of view. In essence, we are looking for best statistical parameters, in this case \\(\\beta\\) values, for our model that are most likely to produce such a scatter of data points given the explanatory variables.This is called “Maximum likelihood” approach. Probability of observing a \\(Y\\) value, given that the distribution of it on a given \\(X\\) value follows a normal distribution with mean \\(\\beta_0+\\beta_1x_i\\) and variance \\(s^2\\) , and is shown below. Note that this assumes variance is constant and \\(s^2=\\frac{\\sum{\\epsilon_i}}{n-2}\\) is an unbiased estimation for population variance, \\(\\sigma^2\\). \\[P(y_{i})=\\frac{1}{s\\sqrt{2\\pi} }e^{-\\frac{1}{2}\\left(\\frac{y_i-(\\beta_0 + \\beta_1x_i)}{s}\\right)^2}\\] Following from this, then the likelihood function ,shown as \\(L\\) below, for linear regression is multiplication of \\(P(y_{i})\\) for all data points. \\[L=P(y_1)P(y_2)P(y_3)..P(y_n)=\\prod\\limits_{i=1}^n{P_i}\\] This can be simplified to this by some algebra and taking logs (since it is easier to add than multiply) \\[ln(L) = -nln(s\\sqrt{2\\pi}) - \\frac{1}{2s^2} \\sum\\limits_{i=1}^n{(y_i-(\\beta_0 - \\beta_1x_i))^2} \\] As you can see, the right part of the function is the negative of the cost function defined above. If we wanted to optimize this function we would need to take derivative of the function with respect to \\(\\beta\\) parameters. That means we can ignore the first part since there is no \\(\\beta\\) terms there. This simply reduces to the negative of the cost function. Hence, this approach produces exactly the same result as the cost function approach. The difference is that we defined our problem within the domain of statistics. This particular function has still to be optimized. This can be done with some calculus without the need for an iterative approach. 4.3.1.3 Linear algebra and closed-form solution to linear regression The last approach we will describe is the minimization process using linear algebra. But in this case, we do not use an iterative approach. Instead, we will minimize cost function by explicitly taking its derivatives with respect to \\(\\beta\\)’s and setting them to zero. This is doable by employing linear algebra and matrix calculus. This approach is also called “ordinary least squares”. We will not show the whole derivation here but the following expression is what we are trying to minimize in matrix notation, this is basically a different notation of the same minimization problem defined above. Remember \\(\\epsilon_i=Y_i-(\\beta_0+\\beta_1x_i)\\) \\[ \\begin{align} \\sum\\epsilon_{i}^2=\\epsilon^T\\epsilon=(Y-{\\beta}{X})^T(Y-{\\beta}{X}) \\\\ =Y^T{Y}-2{\\beta}^T{Y}+{\\beta}^TX^TX{\\beta} \\end{align} \\] After rearranging the terms, we take the derivative of \\(\\epsilon^T\\epsilon\\) with respect to \\(\\beta\\), and equalize that to zero. We then arrive at the following for estimated \\(\\beta\\) values, \\(\\hat{\\beta}\\): \\[\\hat{\\beta}=(X^TX)^{-1}X^TY\\] This requires for you to calculate the inverse of the \\(X^TX\\) term, which could be slow for large matrices. Iterative approach over the cost function derivatives will be faster for larger problems. The linear algebra notation is something you will see in the papers or other resources often. If you input the data matrix X and solve the \\((X^TX)^{-1}\\) , you get the following values for \\(\\beta_0\\) and \\(\\beta_1\\) for simple regression . However, we should note that this simple linear regression case can easily be solved algebraically without the need for matrix operations. This can be done by taking the derivative of \\(\\sum{(y_i-(\\beta_0+\\beta_1x_i))^2}\\) with respect to \\(\\beta_1\\), rearranging the terms and equalizing the derivative to zero. \\[\\hat{\\beta_1}=\\frac{\\sum{(x_i-\\overline{X})(y_i-\\overline{Y})}}{ \\sum{(x_i-\\overline{X})^2} }\\] \\[\\hat{\\beta_0}=\\overline{Y}-\\hat{\\beta_1}\\overline{X}\\] 4.3.1.4 Fitting lines in R After all this theory, you will be surprised how easy it is to fit lines in R. This is achieved just by lm() command, stands for linear models. Let’s do this for a simulated data set and plot the fit. First step is to simulate the data, we will decide on \\(\\beta_0\\) and \\(\\beta_1\\) values. The we will decide on the variance parameter,\\(\\sigma\\) to be used in simulation of error terms, \\(\\epsilon\\). We will first find \\(Y\\) values, just using the linear equation \\(Y=\\beta0+\\beta_1X\\), for a set of \\(X\\) values. Then, we will add the error terms get our simulated values. # set random number seed, so that the random numbers from the text # is the same when you run the code. set.seed(32) # get 50 X values between 1 and 100 x = runif(50,1,100) # set b0,b1 and varience (sigma) b0 = 10 b1 = 2 sigma = 20 # simulate error terms from normal distribution eps = rnorm(50,0,sigma) # get y values from the linear equation and addition of error terms y = b0 + b1*x+ eps Now let us fit a line using lm() function. The function requires a formula, and optionally a data frame. We need the pass the following expression within the lm function, y~x, where y is the simulated \\(Y\\) values and x is the explanatory variables \\(X\\).We will then use abline() function to draw the fit. mod1=lm(y~x) # plot the data points plot(x,y,pch=20, ylab=&quot;Gene Expression&quot;,xlab=&quot;Histone modification score&quot;) # plot the linear fit abline(mod1,col=&quot;blue&quot;) 4.3.2 How to estimate the error of the coefficients Since we are using a sample to estimate the coefficients they are not exact, with every random sample they will vary. Below, we are taking multiple samples from the population and fitting lines to each sample, with each sample the lines slightly change.We are overlaying the points and the lines for each sample on top of the other samples .When we take 200 samples and fit lines for each of them,the lines fit are variable. And, we get a normal-like distribution of \\(\\beta\\) values with a defined mean and standard deviation a, which is called standard error of the coefficients. As usually we will not have access to the population to do repeated sampling, model fitting and estimation of the standard error for the coefficients. But there is statistical theory that helps us infer the population properties from the sample. When we assume that error terms have constant variance and mean zero , we can model the uncertainty in the regression coefficients, \\(\\beta\\)s. The estimates for standard errors of \\(\\beta\\)s for simple regression are as follows and shown without derivation. \\[ \\begin{align} s=RSE=\\sqrt{\\frac{\\sum{(y_i-(\\beta_0+\\beta_1x_i))^2}}{n-2} } =\\sqrt{\\frac{\\sum{\\epsilon^2}}{n-2} } \\\\ SE(\\hat{\\beta_1})=\\frac{s}{\\sqrt{\\sum{(x_i-\\overline{X})^2}}} \\\\ SE(\\hat{\\beta_0})=s\\sqrt{ \\frac{1}{n} + \\frac{\\overline{X}^2}{\\sum{(x_i-\\overline{X})^2} } } \\end{align} \\] Notice that that \\(SE(\\beta_1)\\) depends on the estimate of variance of residuals shown as \\(s\\) or Residual Standard Error (RSE). Notice alsos standard error depends on the spread of \\(X\\). If \\(X\\) values have more variation, the standard error will be lower. This intuitively makes sense since if the spread of the \\(X\\) is low, the regression line will be able to wiggle more compared to a regression line that is fit to the same number of points but covers a greater range on the X-axis. The standard error estimates can also be used to calculate confidence intervals and test hypotheses, since the following quantity called t-score approximately follows a t-distribution with \\(n-p\\) degrees of freedom, where \\(n\\) is the number of data points and \\(p\\) is the number of coefficients estimated. \\[ \\frac{\\hat{\\beta_i}-\\beta_test}{SE(\\hat{\\beta_i})}\\] Often, we would like to test the null hypothesis if a coefficient is equal to zero or not. For simple regression this could mean if there is a relationship between explanatory variable and response variable. We would calculate the t-score as follows \\(\\frac{\\hat{\\beta_i}-0}{SE(\\hat{\\beta_i})}\\), and compare it t-distribution with \\(d.f.=n-p\\) to get the p-value. We can also calculate the uncertainty of the regression coefficients using confidence intervals, the range of values that are likely to contain \\(\\beta_i\\). The 95% confidence interval for \\(\\hat{\\beta_i}\\) is \\(\\hat{\\beta_i}\\) ± \\(t_{0.975}SE(\\hat{\\beta_i})\\). \\(t_{0.975}\\) is the 97.5% percentile of the t-distribution with \\(d.f. = n – p\\). In R, summary() function will test all the coefficients for the null hypothesis \\(\\beta_i=0\\). The function takes the model output obtained from the lm() function. To demonstrate this, let us first get some data. The procedure below simulates data to be used in a regression setting and it is useful to examine what the linear model expect to model the data. Since we have the data, we can build our model and call the summary function. We will then use confint() function to get the confidence intervals on the coefficients and coef() function to pull out the estimated coefficients from the model. mod1=lm(y~x) summary(mod1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -77.11 -18.44 0.33 16.06 57.23 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.24538 6.28869 2.106 0.0377 * ## x 0.49954 0.05131 9.736 4.54e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.77 on 98 degrees of freedom ## Multiple R-squared: 0.4917, Adjusted R-squared: 0.4865 ## F-statistic: 94.78 on 1 and 98 DF, p-value: 4.537e-16 # get confidence intervals confint(mod1) ## 2.5 % 97.5 % ## (Intercept) 0.7656777 25.7250883 ## x 0.3977129 0.6013594 # pull out coefficients from the model coef(mod1) ## (Intercept) x ## 13.2453830 0.4995361 The summary() function prints out an extensive list of values. The “Coefficients” section has the estimates, their standard error, t score and the p-value from the hypothesis test \\(H_0:\\beta_i=0\\). As you can see, the estimate we get for the coefficients and their standard errors are close to the ones we get from the repeatedly sampling and getting a distribution of coefficients. This is statistical inference at work, we can estimate the population properties within a certain error using just a sample. 4.3.3 Accuracy of the model If you have observed the table output by summary() function, you must have noticed there are some other outputs, such as “Residual standard error”, “Multiple R-squared” and “F-statistic”. These are metrics that are useful for assessing the accuracy of the model. We will explain them one by one. _ (RSE)_ simply is the square-root of the the sum of squared error terms, divided by degrees of freedom, \\(n-p\\), for simple linear regression case, \\(n-2\\). Sum of of the squares of the error terms is also called “Residual sum of squares”, RSS. So RSE is calculated as follows: \\[ s=RSE=\\sqrt{\\frac{\\sum{(y_i-\\hat{Y_i})^2 }}{n-p}}=\\sqrt{\\frac{RSS}{n-p}}\\] RSE is a way of assessing the model fit. The larger the RSE the worse the model is. However, this is an absolute measure in the units of \\(Y\\) and we have nothing to compare against. One idea is that we divide it by RSS of a simpler model for comparative purposes. That simpler model is in this case is the model with the intercept,\\(\\beta_0\\). A very bad model will have close zero coefficients for explanatory variables, and the RSS of that model will be close to the RSS of the model with only the intercept. In such a model intercept will be equal to \\(\\overline{Y}\\). As it turns out, RSS of the the model with just the intercept is called “Total Sum of Squares” or TSS. A good model will have a low \\(RSS/TSS\\). The metric \\(R^2\\) uses these quantities to calculate a score between 0 and 1, and closer to 1 the better the model. Here is how it is calculated: \\[R^2=1-\\frac{RSS}{TSS}=\\frac{TSS-RSS}{TSS}=1-\\frac{RSS}{TSS}\\] \\(TSS-RSS\\) part of the formula often referred to as “explained variability” in the model. The bottom part is for “total variability”. With this interpretation, higher the “explained variability” better the model. For simple linear regression with one explanatory variable, the square root of \\(R^2\\) is a quantity known as absolute value of the correlation coefficient, which can be calculated for any pair of variables, not only the response and the explanatory variables. Correlation is a general measure of linear relationship between two variables. One of the most popular flavors of correlation is the Pearson correlation coefficient. Formally, It is the covariance of X and Y divided by multiplication of standard deviations of X and Y. In R, it can be calculated with cor() function. \\[ r_{xy}=\\frac{cov(X,Y)}{\\sigma_x\\sigma_y} =\\frac{\\sum\\limits_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})} {\\sqrt{\\sum\\limits_{i=1}^n (x_i-\\bar{x})^2 \\sum\\limits_{i=1}^n (y_i-\\bar{y})^2}} \\] In the equation above, cov is the covariance, this is again a measure of how much two variables change together, like correlation. If two variables show similar behavior they will usually have positive covariance value, if they have opposite behavior, the covariance will have negative value. However, these values are boundless. A normalized way of looking at covariance is to divide covariance by the multiplication of standard errors of X and Y. This bounds the values to -1 and 1, and as mentioned above called Pearson correlation coefficient. The values that change in a similar manner will have a positive coefficient, the values that change in opposite manner will have negative coefficient, and pairs do not have a linear relationship will have 0 or near 0 correlation. In the figure below, we are showing \\(R^2\\), correlation coefficient and covariance for different scatter plots. For simple linear regression, correlation can be used to asses the model. However, this becomes useless as a measure of general accuracy if the there are more than one explanatory variable as in multiple linear regression. In that case, \\(R^2\\) is a measure of accuracy for the model. Interestingly, square of the correlation of predicted values and original response variables (\\((cor(Y,\\hat{Y}))^2\\) ) equals to \\(R^2\\) for multiple linear regression. The last accuracy measure or the model fit in general we are going to explain is F-statistic. This is a quantity that depends on RSS and TSS again. It can also answer one important question that other metrics can not easily answer. That question is whether or not any of the explanatory variables have predictive value or in other words if all the explanatory variables are zero. We can write the null hypothesis as follows: \\[H_0: \\beta_1=\\beta_2=\\beta_3=...=\\beta_p=0 \\] where the alternative is: \\[H_1: \\text{at least one } \\beta_i \\neq 0 \\] Remember \\(TSS-RSS\\) is analogous to “explained variability” and the RSS is analogous to “unexplained variability”. For the F-statistic, we divide explained variance to unexplained variance. Explained variance is just the \\(TSS-RSS\\) divided by degrees of freedom, and unexplained variance is the RSE. The ratio will follow the F-distribution with two parameters, the degrees of freedom for the explained variance and the degrees of freedom for the the unexplained variance.F-statistic for a linear model is calculated as follows. \\[F=\\frac{(TSS-RSS)/(p-1)}{RSS/(n-p)}=\\frac{(TSS-RSS)/(p-1)}{RSE} \\sim F(p-1,n-p)\\] If the variances are the same, the ratio will be 1, and when \\(H_0\\) is true, then it can be shown that expected value of \\((TSS-RSS)/(p-1)\\) will be \\(\\sigma^2\\) which is estimated by RSE. So, if the variances are significantly different, the ratio will need to be significantly bigger than 1. If the ratio is large enough we can reject the null hypothesis. To asses that we need to use software or look up the tables for F statistics with calculated parameters. In R, function qf() can be used to calculate critical value of the ratio. Benefit of the F-test over looking at significance of coefficients one by one is that we circumvent multiple testing problem. If there are lots of explanatory variables at least 5% of the time (assuming we use 0.05 as P-value significance cutoff), p-values from coefficient t-tests will be wrong. In summary, F-test is a better choice for testing if there is any association between the explanatory variables and the response variable. 4.3.4 Regression with categorical variables An important feature of linear regression is that categorical variables can be used as explanatory variables, this feature is very useful in genomics where explanatory variables often could be categorical. To put it in context, in our histone modification example we can also include if promoters have CpG islands or not as a variable. In addition, in differential gene expression, we usually test the difference between different condition which can be encoded as categorical variables in a linear regression. We can sure use t-test for that as well if there are only 2 conditions, but if there are more conditions and other variables to control for such as Age or sex of the samples, we need to take those into account for our statistics, and t-test alone can not handle such complexity. In addition, when we have categorical variables we can also have numeric variables in the model and we certainly do not have to include only one type of variable in a model. The simplest model with categorical variables include two levels that can be encoded in 0 and 1. set.seed(100) gene1=rnorm(30,mean=4,sd=2) gene2=rnorm(30,mean=2,sd=2) gene.df=data.frame(exp=c(gene1,gene2), group=c( rep(1,30),rep(0,30) ) ) mod2=lm(exp~group,data=gene.df) summary(mod2) ## ## Call: ## lm(formula = exp ~ group, data = gene.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7290 -1.0664 0.0122 1.3840 4.5629 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1851 0.3517 6.214 6.04e-08 *** ## group 1.8726 0.4973 3.765 0.000391 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.926 on 58 degrees of freedom ## Multiple R-squared: 0.1964, Adjusted R-squared: 0.1826 ## F-statistic: 14.18 on 1 and 58 DF, p-value: 0.0003905 require(mosaic) plotModel(mod2) we can even compare more levels, we do not even have to encode them ourselves. We can pass categorical variables to lm() function. gene.df=data.frame(exp=c(gene1,gene2,gene2), group=c( rep(&quot;A&quot;,30),rep(&quot;B&quot;,30),rep(&quot;C&quot;,30) ) ) mod3=lm(exp~group,data=gene.df) summary(mod3) ## ## Call: ## lm(formula = exp ~ group, data = gene.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7290 -1.0793 -0.0976 1.4844 4.5629 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.0577 0.3781 10.731 &lt; 2e-16 *** ## groupB -1.8726 0.5348 -3.502 0.000732 *** ## groupC -1.8726 0.5348 -3.502 0.000732 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.071 on 87 degrees of freedom ## Multiple R-squared: 0.1582, Adjusted R-squared: 0.1388 ## F-statistic: 8.174 on 2 and 87 DF, p-value: 0.0005582 4.3.5 Regression pitfalls In most cases one should look at the error terms (residuals) vs fitted values plot. Any structure in this plot indicates problems such as non-linearity, correlation of error terms, non-constant variance or unusual values driving the fit. Below we briefly explain the potential issues with the linear regression. 4.3.5.0.1 non-linearity If the true relationship is far from linearity, prediction accuracy is reduced and all the other conclusions are questionable. In some cases, transforming the data with \\(logX\\), \\(\\sqrt{X}\\) and \\(X^2\\) could resolve the issue. 4.3.5.0.2 correlation of explanatory variables If the explanatory variables are correlated that could lead to something known as multicolinearity. When this happens SE estimates of the coefficients will be too large. This is usually observed in time-course data. 4.3.5.0.3 correlation of error terms This assumes that the errors of the response variables are uncorrelated with each other. If they are confidence intervals in the coefficients might too narrow. 4.3.5.0.4 Non-constant variance of error terms This means that different response variables have the same variance in their errors, regardless of the values of the predictor variables. If the errors are not constant, if for the errors grow as X grows this will result in unreliable estimates in standard errors as the model assumes constant variance. Transformation of data, such as \\(logX\\) and \\(\\sqrt{X}\\) could help in some cases. 4.3.5.0.5 outliers and high leverage points Outliers are extreme values for Y and high leverage points are unusual X values. Both of these extremes have power to affect the fitted line and the standard errors. In some cases (measurement error), they can be removed from the data for a better fit. 4.3.6 Want to know more… linear models and derivations of equations including matrix notation Applied Linear Statistical Models by Kutner, Nachtsheim, et al. Elements of statistical learning by Hastie &amp; Tibshirani An Introduction to statistical learning by James, Witten, et al. 4.4 Roadmap for future 4.4.1 Clustering Clustering is the task of grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups. Thise groupings are called clusters. Resource: Free book chapter on practical clustering with R https://manning-content.s3.amazonaws.com/download/e/dc31390-3cb7-49dd-ab02-937c1af1c2e1/PDSwR_CH08.pdf 4.4.1.0.1 Learning objectives distance metrics household clustering algorithms How to decide best number of clusters 4.4.2 Dimension reduction with PCA 4.4.2.0.1 Learning objectives why do we need it? What is Eigen vectors and values ? Matrix operations in 2D geometry Resource: - Easy intro: http://www.nature.com/nbt/journal/v26/n3/abs/nbt0308-303.html More involved with R code: https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/ 4.4.3 Classification 4.4.3.0.1 learning objectives penalized logistic and linear regression (remedies multicolinearity problem) random forests Support Vector Machines resource: Introduction to statistical learning by James, Witten, et al. 4.5 Exercises 4.5.1 How to summarize collection of data points: The idea behind statistical distributions 4.5.1.1 Calculate the means and variances of the rows of the following simulated data set, plot the distributions of means and variances using hist() and boxplot() functions. set.seed(100) #sample data matrix from normal distribution gset=rnorm(600,mean=200,sd=70) data=matrix(gset,ncol=6) 4.5.1.2 Using the data generated above, calculate the standard deviation of the distribution of the means using sd() function. Compare that to the expected standard error obtained from central limit theorem keeping in mind the population parameters were \\(\\sigma=70\\) and \\(n=6\\). How does the estimate from the random samples change if we simulate more data with data=matrix(rnorm(6000,mean=200,sd=70),ncol=6) 4.5.1.3 simulate 30 random variables using rpois() function, do this 1000 times and calculate means of sample. Plot the sampling distributions of the means using a histogram. Get the 2.5th and 97.5th percentiles of the distribution. Use t.test function to calculate confidence intervals of the first random sample pois1 simulated fromrpois() function below. Use bootstrap confidence interval for the mean on pois1 compare all the estimates set.seed(100) #sample 30 values from poisson dist with lamda paramater =30 pois1=rpois(30,lambda=5) 4.5.1.4 Optional exercise: Try to recreate the following figure, which demonstrates the CLT concept. 4.5.2 How to test for differences in samples 4.5.2.1 Test the difference of means of the following simulated genes using the randomization, t-test and wilcox.test() functions. Plot the distributions using histograms and boxplots. set.seed(101) gene1=rnorm(30,mean=4,sd=3) gene2=rnorm(30,mean=3,sd=3) 4.5.2.2 Test the difference of means of the following simulated genes using the randomization, t-test and wilcox.test() functions. Plot the distributions using histograms and boxplots. set.seed(100) gene1=rnorm(30,mean=4,sd=2) gene2=rnorm(30,mean=2,sd=2) 4.5.2.3 read the gene expression data set with data=readRDS(&quot;StatisticsForGenomics/geneExpMat.rds&quot;). The data has 100 differentially expressed genes.First 3 columns are the test samples, and the last 3 are the control samples. Do a t-test for each gene (each row is a gene), record the p-values. Then, do a moderated t-test, as shown in the lecture notes and record the p-values. Do a p-value histogram and compare two approaches in terms of the number of significant tests with 0.05 threshold. On the p-values use FDR (BH), bonferroni and q-value adjustment methods. Calculate how many adjusted p-values are below 0.05 for each approach. 4.5.3 Relationship between variables: linear models and correlation 4.5.3.1 Below we are going to simulate X and Y values. Run the code then fit a line to predict Y based on X. Plot the scatter plot and the fitted line. Calculate correlation and R^2. Run the summary() function and try to extract P-values for the model from the object returned by summary. see ?summary.lm Plot the residuals vs fitted values plot, by calling plot function with which=1 as the second argument. First argument is the model returned by lm. # set random number seed, so that the random numbers from the text # is the same when you run the code. set.seed(32) # get 50 X values between 1 and 100 x = runif(50,1,100) # set b0,b1 and varience (sigma) b0 = 10 b1 = 2 sigma = 20 # simulate error terms from normal distribution eps = rnorm(50,0,sigma) # get y values from the linear equation and addition of error terms y = b0 + b1*x+ eps 4.5.3.2 Read the data set histone modification data set with using a variation of: df=readRDS(&quot;StatisticsForGenomics_data/HistoneModeVSgeneExp.rds&quot;). There are 3 columns in the data set these are measured levels of H3K4me3, H3K27me3 and gene expression per gene. plot the scatter plot for H3K4me3 vs expression plot the scatter plot for H3K27me3 vs expression fit the model model for prediction of expression data using: only H3K4me3 as explanatory variable only H3K27me3 as explanatory variable using both H3K4me3 and H3K27me3 as explanatory variables inspect summary() function output in each case, which terms are significant Is using H3K4me3 and H3K27me3 better than the model with only H3K4me3. Plot H3k4me3 vs H3k27me3. Inspect the points that does not follow a linear trend. Are they clustered at certain segments of the plot. Bonus: Is there any biological or technical interpretation for those points ? "],
["genomic-intervals-and-r.html", "Chapter 5 Genomic Intervals and R 5.1 Operations on Genomic Intervals with GenomicRanges package 5.2 Dealing with high-throughput sequencing reads 5.3 Dealing with continuous scores over the genome 5.4 Visualizing and summarizing genomic intervals 5.5 Session info 5.6 Exercises", " Chapter 5 Genomic Intervals and R A considerable time in computational genomics is spent on overlapping different features of the genome. Each feature can be represented with a genomic interval within the chromosomal coordinate system. In addition, each interval can carry different sorts of information. An interval may for instance represent exon coordinates or a transcription factor binding site. On the other hand, you can have base-pair resolution, continuous scores over the genome such as read coverage or scores that could be associated with only certain bases such as in the case of CpG methylation (See Figure 5.1 ). Typically, you will need to overlap intervals on interest with other features of the genome, again represented as intervals. For example, you may want to overlap transcription factor binding sites with CpG islands or promoters to quantify what percentage of binding sites overlap with your regions of interest. Overlapping mapped reads from high-throughput sequencing experiments with genomic features such as exons, promoters, enhancers can also be classified as operations on genomic intervals. You can think of a million other ways that involves overlapping two sets of different features on the genome. This chapter aims to show how to do analysis involving operations on genomic intervals. Figure 5.1: Summary of genomic intervals with different kinds of information 5.1 Operations on Genomic Intervals with GenomicRanges package The Bioconductor project has a dedicated package called GenomicRanges to deal with genomic intervals. In this section, we will provide use cases involving operations on genomic intervals. The main reason we will stick to this package is that it provides tools to do overlap operations. However package requires that users operate on specific data types that are conceptually similar to a tabular data structure implemented in a way that makes overlapping and related operations easier. The main object we will be using is called GRanges object and we will also see some other related objects from the GenomicRanges package. 5.1.1 How to create and manipulate a GRanges object GRanges (from GenomicRanges package) is the main object that holds the genomic intervals and extra information about those intervals. Here we will show how to create one. Conceptually, it is similar to a data frame and some operations such as using notation to subset the table will work also on GRanges, but keep in mind that not everything that works for data frames will work on GRanges objects. library(GenomicRanges) gr=GRanges(seqnames=c(&quot;chr1&quot;,&quot;chr2&quot;,&quot;chr2&quot;), ranges=IRanges(start=c(50,150,200),end=c(100,200,300)), strand=c(&quot;+&quot;,&quot;-&quot;,&quot;-&quot;) ) gr ## GRanges object with 3 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## [1] chr1 [ 50, 100] + ## [2] chr2 [150, 200] - ## [3] chr2 [200, 300] - ## ------- ## seqinfo: 2 sequences from an unspecified genome; no seqlengths # subset like a data frame gr[1:2,] ## GRanges object with 2 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## [1] chr1 [ 50, 100] + ## [2] chr2 [150, 200] - ## ------- ## seqinfo: 2 sequences from an unspecified genome; no seqlengths As you can see it looks a bit like a data frame. Also, note that the peculiar second argument “ranges” which basically contains start and end positions of the genomic intervals. However, you can not just give start and end positions you actually have to provide another object of IRanges. Do not let this confuse you, GRanges actually depends on another object that is very similar to itself called IRanges and you have to provide the “ranges” argument as an IRanges object. In its simplest for, an IRanges object can be constructed by providing start and end positions to IRanges() function. Think of it as something you just have to provide in order to construct the GRanges object. GRanges can also contain other information about the genomic interval such as scores, names, etc. You can provide extra information at the time of the construction or you can add it later. Here is how you can do those: gr=GRanges(seqnames=c(&quot;chr1&quot;,&quot;chr2&quot;,&quot;chr2&quot;), ranges=IRanges(start=c(50,150,200),end=c(100,200,300)), names=c(&quot;id1&quot;,&quot;id3&quot;,&quot;id2&quot;), scores=c(100,90,50) ) # or add it later (replaces the existing meta data) mcols(gr)=DataFrame(name2=c(&quot;pax6&quot;,&quot;meis1&quot;,&quot;zic4&quot;), score2=c(1,2,3)) gr=GRanges(seqnames=c(&quot;chr1&quot;,&quot;chr2&quot;,&quot;chr2&quot;), ranges=IRanges(start=c(50,150,200),end=c(100,200,300)), names=c(&quot;id1&quot;,&quot;id3&quot;,&quot;id2&quot;), scores=c(100,90,50) ) # or appends to existing meta data mcols(gr)=cbind(mcols(gr), DataFrame(name2=c(&quot;pax6&quot;,&quot;meis1&quot;,&quot;zic4&quot;)) ) gr ## GRanges object with 3 ranges and 3 metadata columns: ## seqnames ranges strand | names scores name2 ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; &lt;numeric&gt; &lt;character&gt; ## [1] chr1 [ 50, 100] * | id1 100 pax6 ## [2] chr2 [150, 200] * | id3 90 meis1 ## [3] chr2 [200, 300] * | id2 50 zic4 ## ------- ## seqinfo: 2 sequences from an unspecified genome; no seqlengths # elementMetadata() and values() do the same things elementMetadata(gr) ## DataFrame with 3 rows and 3 columns ## names scores name2 ## &lt;character&gt; &lt;numeric&gt; &lt;character&gt; ## 1 id1 100 pax6 ## 2 id3 90 meis1 ## 3 id2 50 zic4 values(gr) ## DataFrame with 3 rows and 3 columns ## names scores name2 ## &lt;character&gt; &lt;numeric&gt; &lt;character&gt; ## 1 id1 100 pax6 ## 2 id3 90 meis1 ## 3 id2 50 zic4 # you may also add metadata using the $ operator, as for data frames gr$name3 = c(&quot;A&quot;,&quot;C&quot;, &quot;B&quot;) gr ## GRanges object with 3 ranges and 4 metadata columns: ## seqnames ranges strand | names scores name2 ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; &lt;numeric&gt; &lt;character&gt; ## [1] chr1 [ 50, 100] * | id1 100 pax6 ## [2] chr2 [150, 200] * | id3 90 meis1 ## [3] chr2 [200, 300] * | id2 50 zic4 ## name3 ## &lt;character&gt; ## [1] A ## [2] C ## [3] B ## ------- ## seqinfo: 2 sequences from an unspecified genome; no seqlengths 5.1.2 Getting genomic regions into R as GRanges objects There are multiple ways you can read in your genomic features into R and create a GRanges object. Most genomic interval data comes as a tabular format that has the basic information about the location of the interval and some other information. We already showed how to read BED files as data frame. Now we will show how to convert it to GRanges object. This is one way of doing it, but there are more convenient ways described further in the text. # read CpGi data set cpgi.df = read.table(&quot;GenomicIntervals_data/data/cpgi.hg19.chr21.bed&quot;, header = FALSE, stringsAsFactors=FALSE) # remove chr names with &quot;_&quot; cpgi.df =cpgi.df [grep(&quot;_&quot;,cpgi.df[,1],invert=TRUE),] cpgi.gr=GRanges(seqnames=cpgi.df[,1], ranges=IRanges(start=cpgi.df[,2], end=cpgi.df[,3])) You may need to do some pre-processing before/after reading in the BED file. Below is an example of getting transcription start sites from BED files containing refseq transcript locations. # read refseq file ref.df = read.table(&quot;GenomicIntervals_data/data/refseq.hg19.chr21.bed&quot;, header = FALSE, stringsAsFactors=FALSE) ref.gr=GRanges(seqnames=ref.df[,1], ranges=IRanges(start=ref.df[,2], end=ref.df[,3]), strand=ref.df[,6],name=ref.df[,4]) # get TSS tss.gr=ref.gr # end of the + strand genes must be equalized to start pos end(tss.gr[strand(tss.gr)==&quot;+&quot;,]) =start(tss.gr[strand(tss.gr)==&quot;+&quot;,]) # startof the - strand genes must be equalized to end pos start(tss.gr[strand(tss.gr)==&quot;-&quot;,])=end(tss.gr[strand(tss.gr)==&quot;-&quot;,]) # remove duplicated TSSes ie alternative transcripts # this keeps the first instance and removes duplicates tss.gr=tss.gr[!duplicated(tss.gr),] Another way of doing this is from a BED file is to use readTranscriptfeatures() function from the genomation package. Reading the genomic features as text files and converting to GRanges is not the only way to create GRanges object. With the help of the rtracklayer package we can directly import BED files. require(rtracklayer) import.bed(&quot;GenomicIntervals_data/data/refseq.hg19.chr21.bed&quot;) Next, we will show how to use other methods to automatically obtain the data in GRanges format from online databases. But you will not be able to use these methods for every data set so it is good to now how to read data from flat files as well. We will use rtracklayer package to download data from UCSC browser. We will download CpG islands as GRanges objects. require(rtracklayer) session &lt;- browserSession(&quot;UCSC&quot;,url = &#39;http://genome-euro.ucsc.edu/cgi-bin/&#39;) genome(session) &lt;- &quot;mm9&quot; ## choose CpG island track on chr12 query &lt;- ucscTableQuery(session, track=&quot;CpG Islands&quot;,table=&quot;cpgIslandExt&quot;, range=GRangesForUCSCGenome(&quot;mm9&quot;, &quot;chr12&quot;)) ## get the GRanges object for the track track(query) 5.1.2.1 Frequently used file formats and how to read them into R as a table There are multiple file formats in genomics but some of them you will see more frequently than others. We already mentioned some of them. Here is a list of files and functions to read them into R as GRanges objects or something coercible to GRanges objects. BED: These are used and popularized by UCSC browser, and can hold a variety of information including exon/intron structure of transcripts in a single line. genomation::readBed() genomation::readTranscriptFeatures() good for getting intron/exon/promoters from BED12 files rtracklayer::import.bed() GFF: GFF format is a tabular text format for genomic features similar to BED. However, it is a more flexible format than BED, which makes it harder to parse at times. Many gene annotation files are in this format. genomation::gffToGranges() rtracklayer::impot.gff() BAM: BAM format is compressed and indexed tabular file format designed for sequencing reads. GenomicAlignments::readGAlignments Rsamtools::scanBam returns a data frame with columns from SAM/BAM file. BigWig: This is used to for storing scores associated with genomic intervals. It is an indexed format. Similar to BAM, this makes it easier to query and only necessary portions of the file could be loaded into memory. rtracklayer::import.bw() Generic Text files: This represents any text file with the minimal information of chromosome, start and end coordinates. genomation::readGeneric() Tabix/Bcf: These are tabular file formats indexed and compressed similar to BAM. The following functions return lists rather than tabular data structures. These formats are mostly used to store genomic variation data such as SNPs and indels. Rsamtools::scanTabix Rsamtools::scanBcf 5.1.3 Finding regions that do/do not overlap with another set of regions This is one of the most common tasks in genomics. Usually, you have a set of regions that you are interested in and you want to see if they overlap with another set of regions or see how many of them overlap. A good example is transcription factor binding sites determined by ChIP-seq experiments. In these types of experiments and followed analysis, one usually ends up with genomic regions that are bound by transcription factors. One of the standard next questions would be to annotate binding sites with genomic annotations such as promoter,exon,intron and/or CpG islands. Below is a demonstration of how transcription factor binding sites can be annotated using CpG islands. First, we will get the subset of binding sites that overlap with the CpG islands. In this case, binding sites are ChIP-seq peaks. We can find the subset of peaks that overlap with the CpG islands using the subsetByoverlaps() function. You will also see another way of converting data frames to GRanges. library(genomation) pk1.gr=readBroadPeak(&quot;GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep1.broadPeak.gz&quot;) pk1.gr=pk1.gr[seqnames(pk1.gr)==&quot;chr21&quot;,] # get the peaks that overlap with CpG # islands subsetByOverlaps(pk1.gr,cpgi.gr) ## GRanges object with 44 ranges and 5 metadata columns: ## seqnames ranges strand | name score ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; &lt;integer&gt; ## [1] chr21 [ 9825359, 9826582] * | peak14562 56 ## [2] chr21 [ 9968468, 9968984] * | peak14593 947 ## [3] chr21 [15755367, 15755956] * | peak14828 90 ## [4] chr21 [19191578, 19192525] * | peak14840 290 ## [5] chr21 [26979618, 26980048] * | peak14854 32 ## ... ... ... ... . ... ... ## [40] chr21 [46237463, 46237809] * | peak15034 32 ## [41] chr21 [46707701, 46708084] * | peak15037 67 ## [42] chr21 [46961551, 46961875] * | peak15039 38 ## [43] chr21 [47743586, 47744125] * | peak15050 353 ## [44] chr21 [47878411, 47878891] * | peak15052 104 ## signalValue pvalue qvalue ## &lt;numeric&gt; &lt;integer&gt; &lt;integer&gt; ## [1] 183.11 -1 -1 ## [2] 3064.92 -1 -1 ## [3] 291.90 -1 -1 ## [4] 940.03 -1 -1 ## [5] 104.67 -1 -1 ## ... ... ... ... ## [40] 106.36 -1 -1 ## [41] 217.02 -1 -1 ## [42] 124.31 -1 -1 ## [43] 1141.58 -1 -1 ## [44] 338.78 -1 -1 ## ------- ## seqinfo: 23 sequences from an unspecified genome; no seqlengths For each CpG island, we can count the number of peaks that overlap with a given CpG island with countOverlaps(). #count the peaks that # overlap with CpG islands counts=countOverlaps(pk1.gr,cpgi.gr) head(counts) ## [1] 0 0 0 0 0 0 The findOverlaps() function can be used to see one-to-one overlaps between peaks and CpG islands. It returns a matrix showing which peak overlaps with which CpGi island. findOverlaps(pk1.gr,cpgi.gr) ## Hits object with 45 hits and 0 metadata columns: ## queryHits subjectHits ## &lt;integer&gt; &lt;integer&gt; ## [1] 123 1 ## [2] 154 3 ## [3] 389 8 ## [4] 401 13 ## [5] 415 16 ## ... ... ... ## [41] 595 155 ## [42] 598 166 ## [43] 600 176 ## [44] 611 192 ## [45] 613 200 ## ------- ## queryLength: 620 / subjectLength: 205 Another interesting thing would be to look at the distances to nearest CpG islands for each peak. In addition, just finding the nearest CpG island could also be interesting. Often times, you will need to find nearest TSS or gene to your regions of interest, and the code below is handy for doing that. # find nearest CpGi to each TSS n.ind=nearest(pk1.gr,cpgi.gr) # get distance to nearest dists=distanceToNearest(pk1.gr,cpgi.gr,select=&quot;arbitrary&quot;) dists ## Hits object with 620 hits and 1 metadata column: ## queryHits subjectHits | distance ## &lt;integer&gt; &lt;integer&gt; | &lt;integer&gt; ## [1] 1 1 | 384188 ## [2] 2 1 | 382968 ## [3] 3 1 | 381052 ## [4] 4 1 | 379311 ## [5] 5 1 | 376978 ## ... ... ... . ... ## [616] 616 205 | 26211 ## [617] 617 205 | 27401 ## [618] 618 205 | 30467 ## [619] 619 205 | 31610 ## [620] 620 205 | 34089 ## ------- ## queryLength: 620 / subjectLength: 205 # histogram of the distances to nearest TSS dist2plot=mcols(dists)[,1] hist(log10(dist2plot),xlab=&quot;log10(dist to nearest TSS)&quot;, main=&quot;Distances&quot;) Figure 5.2: histogram of distances 5.2 Dealing with high-throughput sequencing reads In recent years, advances in sequencing technology helped researchers sequence the genome deeper than ever. The reads from sequencing machines are usually aligned to the genome and the next task is to quantify the enrichment of those aligned reads in the regions of interest. You may want to count how many reads overlapping with your promoter set of interest or you may want to quantify RNA-seq reads overlapping with exons. This is similar to operations on genomic intervals which are described previously. If you can read all your alignments into the memory and create a GRanges object, you can apply the previously described operations. However, most of the time we can not read all mapped reads into the memory, so we have to use specialized tools to query and quantify alignments on a given set of regions. One of the most common alignment formats is SAM/BAM format, most aligners will produce SAM/BAM output or you will be able to convert your specific alignment format to SAM/BAM format. The BAM format is a binary version of the human readable SAM format. The SAM format has specific columns that contain different kind of information about the alignment such as mismatches, qualities etc. (see http://samtools.sourceforge.net/SAM1.pdf for SAM format specification). 5.2.1 Quality check on sequencing reads and mapping reads to the genome The sequencing technologies usually produce basecalls with varying quality. In addition, there could be sample-prep specific issues in your sequencing run, such as adapter contamination. It is standard procedure to check the quality of the reads before further analysis. Checking the quality and making some decisions for the downstream analysis can influence the outcome of your analysis. For a long time, quality check and mapping tasks were outside the R domain. However, certain packages in Bioconductor currently can deal with those tasks. Although, we will not go into detail we will mention some packages that can help with quality check and mapping. Read quality checking is possible with Bioconductor packages: qcrc, Rsubread and QuasR. All the packages seem to have interface to C for fast quality score calculations or I/O operations. For the read mapping, QuasR uses Rbowtie package and produces BAM files (see below for short intro on BAM) and Rsubread employs its own mapping algorithm and can also produce BAM files. 5.2.2 Counting mapped reads for a set of regions Rsamtools package has functions to query BAM files. The function we will use in the first example is countBam which takes input of the BAM file and param argument. “param” argument takes a ScanBamParam object. The object is instantiated using ScanBamParam() and contains parameters for scanning the BAM file. The example below is a simple example where ScanBamParam() only includes regions of interest, promoters on chr21. # regions of interest # promoters on chr21 promoter.gr=tss.gr start(promoter.gr)=start(promoter.gr)-1000 end(promoter.gr) =end(promoter.gr)+1000 promoter.gr=promoter.gr[seqnames(promoter.gr)==&quot;chr21&quot;] library(Rsamtools) bamfile=&quot;GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xAlnRep1.chr21.bam&quot; # get reads for regions of interest from the bam file param &lt;- ScanBamParam(which=promoter.gr) counts=countBam(bamfile, param=param) Alternatively, aligned reads can be read in using GenomicAlignments package (which on this occasion relies on RSamtools package). library(GenomicAlignments) alns &lt;- readGAlignments(bamfile, param=param) 5.3 Dealing with continuous scores over the genome Most high-throughput data can be viewed as a continuous score over the bases of the genome. In case of RNA-seq or ChIP-seq experiments the data can be represented as read coverage values per genomic base position. In addition, other information (not necessarily from high-throughput experiments) can be represented this way. The GC content and conservation scores per base are prime examples of other data sets that can be represented as scores. This sort of data can be stored as a generic text file or can have special formats such as Wig (stands for wiggle) from UCSC, or the bigWig format is which is indexed binary format of the wig files. The bigWig format is great for data that covers large fraction of the genome with varying scores, because the file is much smaller than regular text files that have the same information and it can be queried easier since it is indexed. In R/Bioconductor, the continuous data can also be represented in a compressed format, in a format called Rle vector, which stands for run-length encoded vector. This gives superior memory performance over regular vectors because repeating consecutive values are represented as one value in the Rle vector (See Figure 5.3 ). Figure 5.3: Rle encoding explained Typically, for genome-wide data you will have a RleList object which is a list of Rle vectors per chromosome. You can obtain such vectors by reading the reads in and calling coverage() function from GenomicRanges package. Let’s try that on the above data set. covs=coverage(alns) # get coverage vectors covs ## RleList of length 24 ## $chr1 ## integer-Rle of length 249250621 with 1 run ## Lengths: 249250621 ## Values : 0 ## ## $chr2 ## integer-Rle of length 243199373 with 1 run ## Lengths: 243199373 ## Values : 0 ## ## $chr3 ## integer-Rle of length 198022430 with 1 run ## Lengths: 198022430 ## Values : 0 ## ## $chr4 ## integer-Rle of length 191154276 with 1 run ## Lengths: 191154276 ## Values : 0 ## ## $chr5 ## integer-Rle of length 180915260 with 1 run ## Lengths: 180915260 ## Values : 0 ## ## ... ## &lt;19 more elements&gt; Alternatively, you can get the coverage from the Bam file directly. Below, we are getting the coverage directly from the Bam file for our previously defined promoters. covs=coverage(bamfile, param=param) # get coverage vectors One of the most common ways of storing score data is, as mentioned, wig or bigWig format. Most of the ENCODE project data can be downloaded in bigWig format. In addition, conservation scores can also be downloaded as wig/bigWig format. You can import bigWig files into R using import() function from rtracklayer package. However, it is generally not advisable to read the whole bigWig file in memory as it was the case with BAM files. Usually, you will be interested in only a fraction of the genome, such as promoters, exons etc. So it is best you extract the data for those regions and read those into memory rather than the whole file. Below we read the a bigWig file only for promoters. The operation returns an GRanges object with score column which indicates the scores in the BigWig file per genomic region. library(rtracklayer) # File from ENCODE ChIP-seq tracks bwFile=&quot;GenomicIntervals_data/data/wgEncodeHaibTfbsA549.chr21.bw&quot; bw.gr=import(bwFile, which=promoter.gr) # get coverage vectors bw.gr ## GRanges object with 9205 ranges and 1 metadata column: ## seqnames ranges strand | score ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;numeric&gt; ## [1] chr21 [9825456, 9825457] * | 1 ## [2] chr21 [9825458, 9825464] * | 2 ## [3] chr21 [9825465, 9825466] * | 4 ## [4] chr21 [9825467, 9825470] * | 5 ## [5] chr21 [9825471, 9825471] * | 6 ## ... ... ... ... . ... ## [9201] chr21 [48055809, 48055856] * | 2 ## [9202] chr21 [48055857, 48055858] * | 1 ## [9203] chr21 [48055872, 48055921] * | 1 ## [9204] chr21 [48055944, 48055993] * | 1 ## [9205] chr21 [48056069, 48056118] * | 1 ## ------- ## seqinfo: 1 sequence from an unspecified genome Following this we can create an RleList object from the GRanges with coverage function. cov.bw=coverage(bw.gr,weight = &quot;score&quot;) # or get this directly from cov.bw=import(bwFile, which=promoter.gr,as = &quot;RleList&quot;) 5.3.1 Extracting subsections of Rle and RleList objects Frequently, we will need to extract subsections of the Rle vectors or RleList objects. We will need to do this to visualize that subsection or get some statistics out of those sections. For example, we could be interested in average coverage per base for the regions we are interested in. We have to extract those regions from RleList object and apply summary statistics. Below, we show how to extract subsections of RleList object. We are extracting promoter regions from ChIP-seq read coverage RleList. Following that, we will plot the one of the promoters associated coverage values. myViews=Views(cov.bw,as(promoter.gr,&quot;RangesList&quot;)) # get subsets of coverage # there is a views object for each chromosome myViews ## RleViewsList of length 1 ## names(1): chr21 myViews[[1]] ## Views on a 48129895-length Rle subject ## ## views: ## start end width ## [1] 42218039 42220039 2001 [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## [2] 17441841 17443841 2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## [3] 17565698 17567698 2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## [4] 30395937 30397937 2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## [5] 27542138 27544138 2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1 ...] ## [6] 27511708 27513708 2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## [7] 32930290 32932290 2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## [8] 27542446 27544446 2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## [9] 28338439 28340439 2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## ... ... ... ... ... ## [370] 47517032 47519032 2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...] ## [371] 47648157 47650157 2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...] ## [372] 47603373 47605373 2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## [373] 47647738 47649738 2001 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ...] ## [374] 47704236 47706236 2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## [375] 47742785 47744785 2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## [376] 47881383 47883383 2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...] ## [377] 48054506 48056506 2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...] ## [378] 48024035 48026035 2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...] # get the coverage vector from the 5th view and plot plot(myViews[[1]][[5]],type=&quot;l&quot;) Figure 5.4: Coverage vector extracted from RleList via Views() function is plotted as a line plot. Next, we are interested in average coverage per base for the promoters using summary functions that works on Views object. # get the mean of the views head( viewMeans(myViews[[1]]) ) ## [1] 0.2258871 0.3498251 1.2243878 0.4997501 2.0904548 0.6996502 # get the max of the views head( viewMaxs(myViews[[1]]) ) ## [1] 2 4 12 4 21 6 5.4 Visualizing and summarizing genomic intervals Data integration and visualization is corner stone of genomic data analysis. Below, we will show different ways of integrating and visualizing genomic intervals. 5.4.1 Visualizing intervals in locus of interest Often times, we will be interested in particular genomic locus and try to visualize different genomic datasets over that locus. This is similar to looking at the data over one of the genome browsers. Below we will display genes, GpG islands and read coverage from a ChIP-seq experiment using Gviz package.For Gviz, we first need to set the tracks to display. The tracks can be in various formats. They can be R objects such as IRanges,GRanges and data.frame, or they can be in flat file formats such as BigWig,BED and BAM. After the tracks are set, we can display them with plotTracks function. library(Gviz) # set tracks to display # set CpG island track cpgi.track=AnnotationTrack(cpgi.gr, name = &quot;CpG&quot;) # set gene track # we will get this from EBI Biomart webservice gene.track &lt;- BiomartGeneRegionTrack(genome = &quot;hg19&quot;, chromosome = &quot;chr21&quot;, start = 27698681, end = 28083310, name = &quot;ENSEMBL&quot;) # set track for ChIP-seq coverage cov.track=DataTrack(&quot;GenomicIntervals_data/data/wgEncodeHaibTfbsA549.chr21.bw&quot;,type = &quot;l&quot;, name=&quot;coverage&quot;) # call the display function plotTracks track.list=list(cpgi.track,gene.track,cov.track) plotTracks(track.list,from=27698681,to=28083310,chromsome=&quot;chr21&quot;) Figure 5.5: tracks visualized using Gviz 5.4.2 Summaries of genomic intervals Looking at data one region at a time could be inefficient. One can summarize different data sets over thousands of regions of interest and identify patterns. This summaries can include different data types such as motifs, read coverage and other scores associated with genomic intervals. The genomation package can summarize and help identify patterns in the datasets. The datasets can have different kinds of information and multiple file types can be used such as BED, GFF, BAM and bigWig. We will look at H3K4me3 ChIP-seq and DNAse-seq signals from H1 embryonic stem cell line. H3K4me3 is usually associated with promoters and regions with high DNAse-seq signal are associated with accessible regions, that means mostly regulatory regions. We will summarize those datasets around the transcription start sites (TSS) of genes on chromosome 20 of human hg19 assembly. We will first read the genes and extract the region around TSS, 500bp upstream and downstream. We will then create a matrix of ChIP-seq scores for those regions, each row will represent a region around a specific TSS and columns will be the scores per base. We will then plot, average enrichment values around the TSSes of genes on chromosome 20. # get transcription start sites on chr20 library(genomation) feat=readTranscriptFeatures(&quot;GenomicIntervals_data/data/refseq.hg19.chr20.bed&quot;,remove.unusual = TRUE, up.flank = 500, down.flank = 500) prom=feat$promoters # get promoters from the features # get for H3K4me3 values around TSSes # we use strand.aware=TRUE so - strands will # be reversed sm=ScoreMatrix(&quot;GenomicIntervals_data/data/H1.ESC.H3K4me3.chr20.bw&quot;,prom, type=&quot;bigWig&quot;,strand.aware = TRUE) # look for the average enrichment plotMeta(sm, profile.names = &quot;H3K4me3&quot;, xcoords = c(-500,500), ylab=&quot;H3K4me3 enrichment&quot;,dispersion = &quot;se&quot;, xlab=&quot;bases around TSS&quot;) Figure 5.6: meta region plot using genomation The pattern we see is expected, there is a dip just around TSS and signal is more intense on the downstream of the TSS. We can also plot a heatmap where each row is a region around TSS and color coded by enrichment. This can show us not only the general pattern as in the meta-region plot but also how many of the regions produce such a pattern. heatMatrix(sm,order=TRUE,xcoords = c(-500,500),xlab=&quot;bases around TSS&quot;) Figure 5.7: Heatmap of enrichment of H3K4me2 around TSS Here we saw that about half of the regions do not have any signal. In addition it seems the multi-modal profile we have observed earlier is more complicated. Certain regions seems to have signal on both sides of the TSS, whereas others have signal mostly on the downstream side. Normally, there would be more than one experiment or we can integrate datasets from public repositories. In this case, we can see how different signals look like on the regions we are interested in. Now, we will also use DNAse-seq data and create a list of matrices with our datasets and plot the average profile of the signals from both datasets. sml=ScoreMatrixList(c(H3K4me3=&quot;GenomicIntervals_data/data/H1.ESC.H3K4me3.chr20.bw&quot;, DNAse=&quot;GenomicIntervals_data/data/H1.ESC.dnase.chr20.bw&quot;),prom, type=&quot;bigWig&quot;,strand.aware = TRUE) plotMeta(sml) Figure 5.8: Average profiles of DNAse and H3K4me3 ChIP-seq We should now look at the heatmaps side by side and we should also cluster the rows based on their similarity. We will be using multiHeatMatrix since we have multiple ScoreMatrix objects in the list. In this case, we will also use winsorize argument to limit extreme values, every score above 95th percentile will be equalized the the value of the 95th percentile. In addition, heatMatrix and multiHeatMatrix can cluster the rows. Below, we will be using k-means clustering with 3 clusters. set.seed(1029) multiHeatMatrix(sml,order=TRUE,xcoords = c(-500,500), xlab=&quot;bases around TSS&quot;,winsorize = c(0,95), matrix.main = c(&quot;H3K4me3&quot;,&quot;DNAse&quot;), column.scale=TRUE, clustfun=function(x) kmeans(x, centers=3)$cluster) (#fig:Heatmaps of H3K4me3 &amp; DNAse data)multiHeatMatrix This revealed a different picture than we have observed before. Almost half of the promoters have no signal for DNAse or H3K4me3; these regions are probably not active and associated genes are not expressed. For regions with H3K4me3 signal, there are two major patterns. One pattern where both downstream and upstream of the TSS are enriched. On the other pattern, mostly downstream of the TSS is enriched. 5.4.3 Making karyograms and circos plots Chromosomal karyograms and circos plots are beneficial for displaying data over the whole genome of chromosomes of interest. Although,the information that can be displayed over these large regions are usually not very clear and only large trends can be discerned by eye, such as loss of methylation in large regions or genome-wide. Below, we are showing how to use ggbio package for plotting. This package has a slightly different syntax than base graphics. The syntax follows grammar of graphics logic. It is a deconstructed way of thinking about the plot. You add your data and apply mappings and transformations in order to achieve the final output. In ggbio, things are relatively easy since a high-level function autoplot function will recognize most of the datatypes and guess the most appropriate plot type. You can change it is behavior by applying low-level functions. We first get the sizes of chromosomes and make a karyogram template. library(ggbio) data(ideoCyto, package = &quot;biovizBase&quot;) p &lt;- autoplot(seqinfo(ideoCyto$hg19), layout = &quot;karyogram&quot;) Next, we would like to plot CpG islands on this Karyogram. We simply do this by adding a layer with layout_karyogram function. # read CpG islands from a generic text file cpgi.gr=genomation::readGeneric(&quot;GenomicIntervals_data/data/CpGi.hg19.table.txt&quot;, chr = 1, start = 2, end = 3,header=TRUE, keep.all.metadata =TRUE,remove.unusual=TRUE ) p + layout_karyogram(cpgi.gr) Figure 5.9: Karyogram of CpG islands Next, we would like to plot some data over the chromosomes. This could be ChIP-seq signal or any other signal over the genome, we will use CpG island scores from the data set we read earlier. We will plot a point proportional to “obsExp” column in the data set. We use ylim argument to squish the chromosomal rectangles and plot on top of those. aes argument defines how the data is mapped to geometry. In this case, it says the points will have x coordinate from CpG island start positions and y coordinate from obsExp score of CpG islands. p + layout_karyogram(cpgi.gr, geom = &quot;point&quot;, size = 0.5, aes(x = start, y = obsExp), ylim = c(11,40), color = &quot;red&quot;) Figure 5.10: Karyogram of CpG islands observed/expected scores Another way to depict regions or quantitative signals on the chromosomes is circos plots. These are circular plots usually used for showing chromosomal rearrangements, but can also be used for depicting signals.ggbio package can produce all kinds of circos plots. Below, we will show how to use that for our CpG island score example. # set the chromsome in a circle # color set to white to look transparent p &lt;- ggplot() + layout_circle(ideoCyto$hg19, geom = &quot;ideo&quot;, fill = &quot;white&quot;, colour=&quot;white&quot;,cytoband = TRUE, radius = 39, trackWidth = 2) # plot the scores as points p &lt;- p + layout_circle(cpgi.gr, geom = &quot;point&quot;, grid=TRUE, size = 1, aes(y = obsExp),color=&quot;red&quot;, radius = 42, trackWidth = 10) # set the chromosome names p &lt;- p + layout_circle(as(seqinfo(ideoCyto$hg19),&quot;GRanges&quot;), geom = &quot;text&quot;, aes(label = seqnames), vjust = 0, radius = 55, trackWidth = 7, size=3) # display the plot p Figure 5.11: circos plot for CpG islands scores 5.5 Session info sessionInfo() ## R version 3.3.0 (2016-05-03) ## Platform: x86_64-apple-darwin13.4.0 (64-bit) ## Running under: OS X 10.9.5 (Mavericks) ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid stats4 parallel stats graphics grDevices utils ## [8] datasets methods base ## ## other attached packages: ## [1] ggbio_1.20.2 Gviz_1.16.4 ## [3] rtracklayer_1.32.2 GenomicAlignments_1.8.4 ## [5] SummarizedExperiment_1.2.3 Biobase_2.32.0 ## [7] Rsamtools_1.24.0 Biostrings_2.40.2 ## [9] XVector_0.12.1 genomation_1.5.6 ## [11] GenomicRanges_1.24.2 GenomeInfoDb_1.8.3 ## [13] IRanges_2.6.1 S4Vectors_0.10.2 ## [15] BiocGenerics_0.18.0 scales_0.4.0 ## [17] plot3D_1.1 matrixStats_0.50.2 ## [19] qvalue_2.4.2 mosaic_0.14.4 ## [21] Matrix_1.2-6 mosaicData_0.14.0 ## [23] ggplot2_2.1.0 lattice_0.20-33 ## [25] dplyr_0.5.0 captioner_2.2.3 ## [27] knitr_1.14 ## ## loaded via a namespace (and not attached): ## [1] bitops_1.0-6 httr_1.2.1 ## [3] RColorBrewer_1.1-2 tools_3.3.0 ## [5] R6_2.1.2 rpart_4.1-10 ## [7] KernSmooth_2.23-15 Hmisc_3.17-4 ## [9] DBI_0.4-1 lazyeval_0.2.0 ## [11] colorspace_1.2-6 nnet_7.3-12 ## [13] seqPattern_1.4.0 GGally_1.2.0 ## [15] gridExtra_2.2.1 chron_2.3-47 ## [17] graph_1.50.0 formatR_1.4 ## [19] ggdendro_0.1-20 labeling_0.3 ## [21] bookdown_0.1.7 readr_1.0.0 ## [23] RBGL_1.48.1 stringr_1.1.0 ## [25] digest_0.6.10 foreign_0.8-66 ## [27] rmarkdown_1.0 dichromat_2.0-0 ## [29] htmltools_0.3.5 plotrix_3.6-3 ## [31] ensembldb_1.4.7 BSgenome_1.40.1 ## [33] rstudioapi_0.6 RSQLite_1.0.0 ## [35] impute_1.46.0 BiocInstaller_1.22.3 ## [37] shiny_0.13.2 BiocParallel_1.6.5 ## [39] acepack_1.3-3.3 VariantAnnotation_1.18.6 ## [41] RCurl_1.95-4.8 magrittr_1.5 ## [43] Formula_1.2-1 Rcpp_0.12.6 ## [45] munsell_0.4.3 stringi_1.1.1 ## [47] yaml_2.1.13 MASS_7.3-45 ## [49] zlibbioc_1.18.0 AnnotationHub_2.4.2 ## [51] plyr_1.8.4 misc3d_0.8-4 ## [53] miniUI_0.1.1 splines_3.3.0 ## [55] GenomicFeatures_1.24.5 reshape2_1.4.1 ## [57] codetools_0.2-14 biomaRt_2.28.0 ## [59] XML_3.98-1.4 evaluate_0.9 ## [61] biovizBase_1.20.0 latticeExtra_0.6-28 ## [63] data.table_1.9.6 httpuv_1.3.3 ## [65] gtable_0.2.0 tidyr_0.5.1 ## [67] reshape_0.8.5 assertthat_0.1 ## [69] gridBase_0.4-7 mime_0.5 ## [71] xtable_1.8-2 survival_2.39-5 ## [73] OrganismDbi_1.14.1 tibble_1.1 ## [75] AnnotationDbi_1.34.4 cluster_2.0.4 ## [77] interactiveDisplayBase_1.10.3 5.6 Exercises 5.6.1 The setup Set your working directory to the source file location in RStudio top menu: ‘Session &gt; Set Working Directory &gt; To Source File Location’. if you have done it correctly you should see this script when you type dir() &gt; dir() [1] &quot;GenomicInterval.exercises.html&quot; The data for the exercises is located at GenomicIntervals_data/data folder. Run the following to see the data files. dir(&quot;GenomicIntervals_data/data&quot;) 5.6.2 Operations on Genomic Intervals with GenomicRanges package 5.6.2.1 Create a GRanges object using the information in the table below: chr start end strand score chr1 10000 10300 + 10 chr1 11100 11500 - 20 chr2 20000 20030 + 15 5.6.2.2 use start(), end(), strand(),seqnames() and width() functions on the GRanges object you created. Figure out what they are doing. Can you get a subset of GRanges object for intervals that are only on + strand? If you can do that, try getting intervals that are on chr1. HINT: GRanges objects can be subset using operator similar to data.frames but you may need to use start(), end() and strand(),seqnames() within the . 5.6.2.3 Import mouse (mm9 assembly) CpG islands and refseq transcripts for chr12 from UCSC browser as GRanges objects using rtracklayer functions. HINT: Check the lecture material and modify the code there as necessary. If that somehow does not work, go to UCSC browser and download it as a BED file. The trackname for Refseq genes is “RefSeq Genes” and table name is “refGene”. 5.6.2.4 Following from the exercise above, get the promoters of Refseq transcripts (-1000bp and +1000 bp of the TSS) and calculate what percentage of them overlap with CpG islands. HINT: You have to get the promoter coordinates and use findOverlaps() or subsetByOverlaps() from GenomicRanges package. To get promoters, type ?promoters on the R console and see how to use that function to get promoters or calculate their coordinates as shown in the lecture material. 5.6.2.5 Plot the distribution of CpG island lengths for CpG islands that overlap with the promoters. 5.6.2.6 Get canonical peaks for SP1 (peaks that are in both replicates) on chr21. Peaks for each replicate are located in GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep1.broadPeak.gz and GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep2.broadPeak.gz files. HINT: You need to use findOverlaps() or subsetByOverlaps() to get the subset of peaks that occur in both replicates. You can try to read *broadPeak.gz files using genomation function readBroadPeak, broadPeak is just an extended BED format. EXTRA credit: Try use coverage() and slice() functions to get canonical peaks. library(genomation) rep1=readBroadPeak(&quot;GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep1.broadPeak.gz&quot;) rep2=readBroadPeak(&quot;GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep2.broadPeak.gz&quot;) 5.6.3 Dealing with mapped high-throughput sequencing reads 5.6.3.1 Count the reads overlapping with canonical Sp1 peaks using the BAM file for one of the replicates: GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xAlnRep1.chr21.bam. HINT: Use functions from GenomicAlignments, see lecture notes. 5.6.4 Dealing with contiguous scores over the genome 5.6.4.1 Extract Views object for the promoters on chr20 from GenomicIntervals_data/data/H1.ESC.H3K4me1.chr20.bw file. Plot the first “View” as a line plot. HINT: see lecture notes, adapt the code from there. 5.6.4.2 Make a histogram of the maximum signal for the Views in the object you extracted above. You can use any of the view summary functions or use lapply() and write your own summary function. 5.6.4.3 Get the genomic positions of maximum signal in each view and make a GRanges object. HINT: See ?viewRangeMaxs help page. Try to make a GRanges object out of the returned object. 5.6.5 Visualizing and summarizing genomic intervals 5.6.5.1 Extract -500,+500 bp regions around TSSes on chr21, there are refseq files in the GenomicIntervals_data/data folder or you can pull the data out of UCSC browser. Use SP1 ChIP-seq data (GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xAlnRep1.chr21.bam ) to create an average profile of read coverage around TSSes. Following that, visualize the read coverage with a heatmap. HINT: All of these possible using genomation package functions. 5.6.5.2 Extract -500,+500 bp regions around TSSes on chr20. Use H3K4me3 (GenomicIntervals_data/data/H1.ESC.H3K4me3.chr20.bw) and H3K27ac (GenomicIntervals_data/data/H1.ESC.H3K27ac.chr20.bw) ChIP-seq enrichment data in the data folder and create heatmaps and average signal profiles for regions around the TSSes. 5.6.5.3 Visualize one of the -500,+500 bp regions around TSS using Gviz functions. You should visualize both H3K4me3 and H3K27ac and the gene models. "]
]
