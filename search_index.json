[
["index.html", "Computational Genomics With R Preface", " Computational Genomics With R Altuna Akalin 2019-03-10 Preface The aim of this book is to provide the fundamentals for data analysis for genomics. We developed this book based on the computational genomics courses we are giving every year. We have had invariably an interdisicplineary audience with backgrounds from physics, biology medicine, math, computer science or other quantitative fields. We want this book to be a starting point for computational genomics students and a guide for further data analysis in more specific topics in genomics. This is why we tried to cover a large variety of topics from programming to basic genome biology. As the field is interdisciplineary, it requires different starting points for people with different backgrounds. A biologist might skip sections on basic genome biology and start with R programming whereas a computer scientist might want to start with genome biology. In the same manner, a more experienced person might want to refer to this book when s/he needs to do a certain type of analysis where s/he does not have prior experience. The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["who-is-this-book-for.html", "Who is this book for?", " Who is this book for? The book contains practical and theoretical aspects for computational genomics. Biology and medicine generate more data than ever before. Therefore, we need to educate more people with data analysis skills and understanding of computational genomics. Since computational genomics is interdisciplinary; this book aims to be accessible for biologists, medical scientists, computer scientists and people from other quantitative backgrounds. We wrote this book for the following audiences: Biologists and medical scientists who generate the data and are keen on analyzing it themselves. Students and researchers who are formally starting to do research on or using computational genomics but do not have extensive domain specific knowledge but has at least a beginner level in a quantitative field: math, stats Experienced researchers looking for recipes or quick how-tos to get started in specific data analysis tasks related to computational genomics. What will you get out of this? This resource describes the skills and provides how-tos that will help readers analyze their own genomics data. After reading: If you are not familiar with R, you will get the basics of R and dive right in to specialized uses of R for computational genomics. you will understand genomic intervals and operations on them, such as overlap You will be able to use R and its vast package library to do sequence analysis: Such as calculating GC content for given segments of a genome or find transcription factor binding sites You will be familiar with visualization techniques used in genomics, such as heatmaps,meta-gene plots and genomic track visualization You will be familiar with supervised and unsupervised learning techniques which are important in data modelling and exploratory analysis of high-dimensional data You will be familiar with analysis of different high-throughput sequencing data sets mostly using R based tools. "],
["structure-of-the-book.html", "Structure of the book", " Structure of the book The book is designed with the idea that practical and conceptual understanding of data analysis methods is as important, if not more important, than the theoretical understanding, such as detailed derivation of equations in statistics or machine-learning. That is why we first try to give a conceptual explanation of the concepts then we try to give essential parts of the mathematical formulas for more detailed understanding. In this sprit, we always show the code and explain the code for a particular data analysis task. In addition, we give additional references such as books, websites and scientific papers for readers who desire to gain deeper theoretical understanding of data analysis related methods or concepts. Introduction chapter introduce data analysis paradigms and what computational genomics looks like in practice. “Essentials for genomics” chapter introduces the basic concepts in genome biology and genomics. Understanding these concepts are important for computational genomics. Chapters “Introduction to R” and “Statistics and Exploratory Data Analysis for Genomics” introduce the necessary programming and practical data analysis skillls for computational genomics. Related to these, “Operations on genomic intervals and genome arithmetic” introduces fundemental tools for dealing with genomic intervals and their relationship to eachother over the genome. These three chapters composes the necessary technical toolkit for analysis of specific high-throughput sequencing techniques. Chapters “RNA-seq analysis”, “ChIP-seq analysis” and “BS-seq analysis” deals with the analysis techniques for popular high-throughput sequencing techniques. The last chapter “multi-omics data integration” deals with methods for integrating multiple omics data sets. To sum it up, this book is a comprehensive guide for computational genomics. Some sections are there for the sake of the wide interdisciplineary audience and completeness, and not all sections will be equally useful to all readers of this broad audience. "],
["software-information-and-conventions.html", "Software information and conventions", " Software information and conventions This book is primarily about using R packages to analyze genomics data, therefore if you want to reproduce the analysis in this book you need to install the relevant packages in each chapter using install.packages or BiocManager::install functions. We rely on data from different R and Bioconductor packages through out the book. For the datasets that do not ship with those packages, we created our own package compGenomRData. You can install this package via devtools::install_github(&quot;compgenomr/compGenomRData&quot;). Package names are in bold text (e.g., methylKit), and inline code and filenames are formatted in a typewriter font. Function names are followed by parentheses (e.g., genomation::ScoreMatrix()). The double-colon operator :: means accessing an object from a package. "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements We wish to thank R and Bioconductor community for developing and maintaining libraries for genomic data analysis. Without their constant work and dedication, writing such a book will not be possible. "],
["how-to-contribute.html", "How to contribute", " How to contribute We have been developing this book on github and your contributions are very welcome. We believe more eyes looking at this will increase the quality. There are a number of ways you can help make the book better: If you don’t understand something, please let Altuna know. Your feedback on what is confusing or hard to understand is valuable. If you spot a typo, feel free to edit the underlying page and send a pull request. If you’ve never done this before, the process is very easy: Click the edit this page on the top bar of the book website. The icon is near the search and download icons on the top of the page. Make the changes using GitHub’s in-page editor (fixing typos is perfectly adequate). Include a brief description of your changes in the area below the editor, then propose the file change. If you make significant changes, include the phrase “I assign the copyright of this contribution to Altuna Akalin” - We need this so I can publish the printed book. Create a pull request. "],
["about-the-authors.html", "About the Authors", " About the Authors The authors have decades of combined experience in data analysis for genomics. They are developers of Bioconductor packages such as methylKit, genomation, RCAS and netSmooth. In addition, they have played key roles in developing end-to-end genomics data analysis pipelines for RNA-seq, ChIP-seq, Bisulfite-seq, and single cell RNA-seq called PiGx. "],
["intro.html", "Chapter 1 Introduction to Genomics", " Chapter 1 Introduction to Genomics The aim of this chapter is to provide the reader with some of the fundamentals required for understanding genome biology. By no means, this is a complete overview of the subject but just a summary that will help the non-biologist reader understand the recurring biological concepts in computational genomics. Readers that are well-versed in genome biology and modern genome-wide quantitative assays should feel free to skip this chapter or skim it through. "],
["genes-dna-and-central-dogma.html", "1.1 Genes, DNA and central dogma", " 1.1 Genes, DNA and central dogma A central concept that will come up again and again is “the gene”. Before we can explain that we need to introduce a few other concepts that are important to understand the gene concept. Human body is made up of billions of cells. These cells specialize in different tasks. For example, in the liver there are cells that help produce enzymes to break toxins. In the heart, there are specialized muscle cells that make the heart beat. Yet, all these different kinds of cells come from a single celled embryo. All the instructions to make different kinds of cells are contained within that single cell and with every division of that cell, those instructions are transmitted to new cells. These instructions can be coded into a string - a molecule of DNA, a polymer made of recurring units called nucleotides. The four nucleotides in DNA molecules, Adenine, Guanine, Cytosine and Thymine (coded as four letters: A, C, G, and T) in a specific sequence, store the information for life. DNA is organized in a double-helix form where two complementary polymers interlace with each other and twist into the familiar helical shape. 1.1.1 What is a genome? The full DNA sequence of an organism, which contains all the hereditary information, is called a genome. The genome contains all the information to build and maintain an organism. Genomes come in different sizes and structures. Our genome is not only a naked strech of DNA. In eukaryotic cells, DNA is wrapped around proteins (histones) forming higher-order structures like nucleosomes which make up chromatins and chromosomes (see Figure 1.1). FIGURE 1.1: Chromosome structure in animals There might be several chromosomes depending on the organism. However, in some species (such as most prokaryotes) DNA is stored in a circular form. The size of genome between species differs too. Human genome has 46 chromosomes and over 3 billion base-pairs, whereas wheat genome has 42 chromosomes and 17 billion base-pairs, both genome size and chromosome numbers are variable between different organisms. Genome sequences of organisms are obtained using sequencing technology. With this technology, fragments of the DNA sequence from the genome, called reads, are obtained. Larger chunks of the genome sequence is later obtained by stitching the initial fragments to larger ones by using the overlapping reads. Latest, sequencing technologies made genome sequencing cheaper and faster. These technologies output more reads, longer reads and more accurate reads. Estimated cost of the first human genome is $300 million in 1999-2000, today a high-quality human genome can be obtained for $1500. Since the costs are going down, researchers and clinicians can generate more data. This drives up to costs for data storage and also drives up the demand for qualified people to analyze genomic data. This was one of the motivations behind writing this book. 1.1.2 What is a gene? In the genome, there are specific regions containing the precise information that encodes for physical products of genetic information. A region in the genome with this information is traditionally called a “gene”. However, the precise definition of the gene is still developing. According to the classical textbooks in molecular biology, a gene is a segment of a DNA sequence corresponding to a single protein or to a single catalytic and structural RNA molecule [1]. A modern definition is: “A region (or regions) that includes all of the sequence elements necessary to encode a functional transcript” [2]. No matter how variable the definitions are, all agree on the fact that genes are basic units of heredity in all living organisms. All cells use their hereditary information in the same way most of the time; the DNA is replicated to transfer the information to new cells. If activated, the genes are transcribed into messenger RNAs (mRNAs) in nucleus (in eukaryotes), followed by mRNAs (if the gene is protein coding) getting translated into proteins in the cytoplasm. This is essentially a process of information transfer between information carrying polymers; DNA, RNA and proteins, known as the “central dogma” of molecular biology (see Figure 1.2 for a summary). Proteins are essential elements for life. The growth and repair, functioning and structure of all living cells depends on them. This is why the gene is a central concept in genome biology, because a gene can encode information for proteins and other functional molecules. How genes are controled and activated dictates everything about an organism. From the identity of a cell to response to an infection, how cells develop and behave against certain stimuli is governed by activity of the genes and functional molecules they encode. The liver cell becomes a liver cell because certain genes are activated and their functional products are produced to help liver cell achieve its tasks. FIGURE 1.2: Central Dogma: replication, transcription, translation 1.1.3 How genes are controlled ? The transcriptional and the post-transcriptional regulation In order to answer this question, we have to dig a little deeper on the transcription concept we introduced via the central dogma. The first step in a process of information transfer - a production of an RNA copy of a part of the DNA sequence - is called transcription. This task is carried out by the RNA polymerase enzyme. RNA polymerase-dependent initiation of transcription is enabled by the existence of a specific region in the sequence of DNA - a core promoter. Core promoters are regions of DNA that promote transcription and are found upstream from the start site of transcription. In eukaryotes, several proteins, called general transcription factors recognize and bind to core promoters and form a pre-initiation complex. RNA polymerases recognize these complexes and initiate synthesis of RNAs, the polymerase travels along the template DNA and making an RNA copy[3]. After mRNA is produced it is often spliced by splicesosome. The sections called ‘introns’ are removed and sections called ‘exons’ left in. Then, the remaining mRNA translated into proteins. Which exons will be part of the final mature transcript can also be regulated and creates diversity in protein structure and function (See Figure 1.3`). FIGURE 1.3: Transcription could be followed by splicing, which creates different transcript isoforms. This will in return create different protein isoforms since the information required to produce the protein is encoded in the transcripts. Differences in transcript of the same gene can give rise to different protein isoforms On the contrary to protein coding genes, non-coding RNA (ncRNAs) genes are processed and assume their functional structures after transcription and without going into translation, hence the name: non-coding RNAs. certain ncRNAs can also be spliced but still not translated. ncRNAs and other RNAs in general can form complementary base-pairs within the RNA molecule which gives them additional complexity. This self-complementarity based structure, termed RNA secondary structure, is often necessary for functions of many ncRNA species. In summary, the set of processes, from transcription initiation to production of the functional product, is referred to as gene expression. Gene expression quantification and regulation is a fundamental topic in genome biology. 1.1.4 What does a gene look like? Before we move forward, it will be good to discuss how we can visualize genes. As someone interested in computational genomics, you will frequently encounter a gene on a computer screen, and how it is represented on the computer will be equivalent to what you imagine when you hear the word “gene”. In the online databases, the genes will appear as a sequence of letters or as a series of connected boxes showing exon-intron structure which may include the direction of transcription as well (see Figure 1.4). direction. You will encounter more with the latter so this is likely what will pop into your mind when you think of genes. As we have mentioned DNA has two strands, and a gene can be located on either of them, and direction of transcription will depend on that. In the Figure you can see arrows on introns (lines connecting boxes) indicating the direction of the gene. FIGURE 1.4: A) Representation of a gene at UCSC browser. Boxes indicate exons, and lines indicate introns. B) Partial sequence of FATE1 gene as shown in NCBI GenBank database. "],
["elements-of-gene-regulation.html", "1.2 Elements of gene regulation", " 1.2 Elements of gene regulation The mechanisms regulating gene expression are essential for all living organisms as they dictate where and how much of a gene product (may it be protein or ncRNA) should be manufactured. This regulation could occur at the pre- and co-transcriptional level by controlling how many transcripts should be produced and/or which version of the transcript should be produced by regulating splicing. Different versions of the same gene could encode for proteins by regulating splicing the process can decide which parts will go into the final mRNA that will code for the protein. In addition, gene products can be regulated post-transcriptionally where certain molecules bind to RNA and mark them for degradation even before they can be used in protein production. Gene regulation drives cellular differentiation; a process during which different tissues and cell types are produced. It also helps cells maintain differentiated states of cells/tissues. As a product of this process, at the final stage of differentiation, different kinds of cells maintain different expression profiles although they contain the same genetic material. As mentioned above there are two main types of regulation and next we will provide information on those. 1.2.1 Transcriptional regulation The rate of transcription initiation is the primary regulatory element in gene expression regulation. The rate is controlled by core promoter elements as well as distant-acting regulatory elements such as enhancers. On top of that, processes like histone modifications and/or DNA methylation have a crucial regulatory impact on transcription. If a region is not accessible for the transcriptional machinery, e.g. in the case when chromatin structure is compacted due to the presence of specific histone modifications, or if the promoter DNA is methylated, transcription may not start at all. Last but the not least, gene activity is also controlled post-transcriptionally by ncRNAs such as microRNAs (miRNAs), as well as by cell signaling resulting in protein modification or altered protein-protein interactions. 1.2.1.1 Regulation by transcription factors through regulatory regions Transcripton factors are proteins that recognize a specific DNA motif to bind on a regulatory region and regulate the transcription rate of the gene associated with that regulatory region (See Figure 1.5)` for an illustration). These factors bind to a variety of regulatory regions summarized in Figure 1.5, and their concerted action controls the transcription rate. Apart from their binding preference, their concentration, the availability of synergistic or competing transcription factors will also affect the transcription rate. FIGURE 1.5: Representation of regulatory regions in animal genomes 1.2.1.1.1 Core and proximal promoters Core promoters are the immediate neighboring regions around the transcription start site (TSS) that serves as a docking site for the transcriptional machinery and pre-initiation complex (PIC) assembly. The textbook model for transcription initiation is as follows: The core promoter has a TATA motif (referred as TATA-box) 30 bp upstream of an initiator sequence (Inr), which also contains TSS. Firstly, transcription factor TFIID binds to the TATA-box. Next, general transcription factors are recruited and transcription is initiated on the initiator sequence. Apart from the TATA-box and Inr, there are a number of sequence elements on the animal core promoters that are associated with transcription initiation and PIC assembly, such as downstream promoter elements (DPEs), the BRE elements and CpG islands. DPEs are found 28-32 bp downstream of the TSS in TATA-less promoters of Drosophila melanogaster, it generally co-occurs with the Inr element, and is thought to have a similar function to the TATA-box. The BRE element is recognized by TFIIB protein and lies upstream of the TATA-box. CpG islands are CG dinucleotide-enriched segments of vertebrate genomes, despite the general depletion of CG dinucleotides in those genomes. 50-70% of promoters in human genome are associated with CpG islands. Proximal promoter elements are typically right upstream of the core promoters and usually contain binding sites for activator transcription factors and they provide additional control over gene expression. 1.2.1.1.2 Enhancers: Proximal regulation is not the only, nor the most important mode of gene regulation. Most of the transcription factor binding sites in the human genome are found in intergenic regions or in introns . This indicates the widespread usage of distal regulatory elements in animal genomes. On a molecular function level, enhancers are similar to proximal promoters; they contain binding sites for the same transcriptional activators and they basically enhance the gene expression. However, they are often highly modular and several of them can affect the same promoter at the same time or in different time-points or tissues. In addition, their activity is independent of their orientation and their distance to the promoter they interact with. A number of studies showed that enhancers can act upon their target genes over several kilobases away. According to a popular model, enhancers achieve this by looping the DNA and coming to contact with their target genes. 1.2.1.1.3 Silencers: Silencers are similar to enhancers; however their effect is opposite of enhancers on the transcription of the target gene, and results in decreasing their level of transcription. They contain binding sites for repressive transcription factors. Repressor transcription factors can either block the binding of an activator , directly compete for the same binding site, or induce a repressive chromatin state in which no activator binding is possible. Silencer effects, similar to those of enhancers, are independent of orientation and distance to target genes. In contradiction to this general view, in Drosophila there are two types of silencers, long-range and short-range. Short-range silencers are close to promoters and long-range silencers can silence multiple promoters or enhancers over kilobases away. Like enhancers, silencers bound by repressors may also induce changes in DNA structure by looping and creating higher order structures. One class of such repressor proteins, which is thought to initiate higher-order structures by looping, is Polycomb group proteins (PcGs). 1.2.1.1.4 Insulators: Insulator regions limit the effect of other regulatory elements to certain chromosomal boundaries; in other words, they create regulatory domains untainted by the regulatory elements in regions outside that domain. Insulators can block enhancer-promoter communication and/or prevent spreading of repressive chromatin domains. In vertebrates and insects, some of the well-studied insulators are bound by CTCF (CCCTC-binding factor). Genome-wide studies from different mammalian tissues confirm that CTCF binding is largely invariant of cell type, and CTCF motif locations are conserved in vertebrates. At present, there are two models of explaining the insulator function; the most prevalent model claims insulators create physically separate domains by modifying chromosome structure. This is thought to be achieved by CTCF-driven chromatin looping and recent evidence shows that CTCF can induce a higher-order chromosome structure through creating loops of chromatins. According to the second model, an insulator-bound activator cannot bind an enhancer; thus enhancer-blocking activity is achieved and insulators can also recruit active histone domain, creating an active domain for enhancers to function. 1.2.1.1.5 Locus control regions: Locus control regions (LCRs) are clusters of different regulatory elements that control entire set of genes on a locus. LCRs help genes achieve their temporal and/or tissue-specific expression programs. LCRs may be composed of multiple cis-regulatory elements, such as insulators, enhancers and they act upon their targets even from long distances. However LCRs function with an orientation dependent manner, for example the activity of beta-globin LCR is lost if inverted. The mechanism of LCR function otherwise seems similar to other long-range regulators described above. The evidence is mounting in the direction of a model where DNA-looping creates a chromosomal structure in which target genes are clustered together, which seems to be essential for maintaining open chromatin domain. 1.2.1.2 Epigenetic regulation Epigenetics in biology usually refers to constructions (chromatin structure, DNA methylation etc.) other than DNA sequence that influence gene regulation. In essence, epigenetic regulation is the regulation of DNA packing and structure, the consequence of which is gene expression regulation. A typical example is that DNA packing inside the nucleus can directly influence gene expression by creating accessible regions for transcription factors to bind. There are two main mechanisms in epigenetic regulation: i) DNA modifications ii) histone modifications. Below, we will introduce these two mechanisms. 1.2.1.2.1 DNA modifications such as methylation: DNA methylation is usually associated with gene silencing. DNA methyltransferase enzyme catalyzes the addition of a methyl group to cytosine of CpG dinucleotides (while in mammals the addition of methyl group is largely restricted to CpG dinucleotides, methylation can occur in other bases as well) . This covalent modification either interferes with transcription factor binding on the region, or methyl-CpG binding proteins induce the spread of repressive chromatin domains, thus the gene is silenced if its promoter has methylated CG dinucleotides. DNA methylation usually occurs in repeat sequences to repress transposable elements, these elements when active can jump around and insert them to random parts of the genome, potentially distrupting the genomic functions. .DNA methylation is also related to a key core and proximal promoter element: CpG islands. CpG islands are usually unmethylated, however for some genes CpG island methylation accompanies their silenced expression. For example, during X-chromosome inactivation many CpG islands are heavily methylated and the associated genes are silenced. In addition, in embryonic stem cell differentiation pluripotency-associated genes are silenced due to DNA methylation. Apart from methylation, there are other kinds of DNA modifications present in mamalian genomes, such as hydroxy-methylation and formylcytosine. These are other modifications under current research that are either intermediate or stable modifications with distinct functional associations. There are as many as 12 distinct DNA modifications observed when we look accross all studied species. 1.2.1.2.2 Histone Modifications: Histones are proteins that constitute nucleosome. In eukaryotes, eight histones nucleosomes are wrapped around by DNA and build nucleosome. They help super-coiling of DNA and inducing high-order structure called chromatin. In chromatin, DNA is either densely packed (called heterochromatin or closed chromatin), or it is loosely packed (called euchromatin or open chromatin) [60, 61]. Heterochromatin is thought to harbor inactive genes since DNA is densely packed and transcriptional machinery cannot access it. On the other hand, euchromatin is more accessible for transcriptional machinery and might therefore harbor active genes. Histones have long and unstructured N-terminal tails which can be covalently modified. The most studied modifications include acetylation, methylation and phosphorylation [60]. Using their tails, histones interact with neighboring nucleosomes and the modifications on the tail affect the nucleosomes affinity to bind DNA and therefore influence DNA packaging around nucleosomes. Different modifications on histones are used in different combinations to program the activity of the genes during differentiation. Histone modifications have a distinct nomenclature, for example: H3K4me3 means the lysine (K) on the 4th position of histone H3 is tri-methylated. Table 1 Histone modifications and their effects. If more than one histone modification has the same effect, they are separated by commas. Modifications Effect H3K9ac Active promoters and enhancers H3K14ac Active transcription H3K4me3/me2/me1 Active p romoters and enhancers, H3K4me1 and H3K27ac is enhancer-specific H3K27ac H3K27ac is enhancer-specific H3K36me3 Active transcribed regions H3K27me3/me2/me1 Silent promoters H3K9me3/me2/me1 Silent promoters Histone modifications are associated with a number of different transcription-related conditions; some of them are summarized in Table 1. Histone modifications can indicate where the regulatory regions are and they can also indicate activity of the genes. From a gene regulatory perspective, maybe the most important modifications are the ones associated with enhancers and promoters. Certain genes in mouse embryonic stem cells have both active H3K4me3 and inactive H3K27me3 modifications in their promoters. Surprisingly, most of these genes have high CpG content, are important for development and are shown to have paused RNA polymerase II [66, 67]. In addition, Heintzman et al. showed that H3K4me1 could predict tissue-specific active enhancers in human cells [68, 69]. The examples above demonstrate the capability of histone modifications in predicting regulatory potential. Furthermore, certain proteins can influence chromatin structure by interacting with histones. Some of these proteins, like those of the Polycomb Group (PcG) and CTCF, are discussed above in the insulators and silencer sections. In vertebrates and insects, PcGs are responsible for maintaining the silent state of developmental genes, and trithorax group proteins (trxG) for maintaining their active state [70, 71]. PcGs and trxGs induce repressed or active states by catalyzing histone modifications or DNA methylation. Both the proteins bind PREs that can be on promoters or several kilobases away [30, 31, 71]. Another protein that induces histone modifications is CTCF. In b-globin locus, CTCF binding is shown to be associated with repressive H3K9/K27me2 modifications [72]. Want to know more ? Transcriptional regulatory elements in the human genome: http://www.ncbi.nlm.nih.gov/pubmed/16719718 On metazoan promoters: types and transcriptional properties: http://www.ncbi.nlm.nih.gov/pubmed/22392219 General principles of regulatory sequence function http://www.nature.com/nrg/journal/v15/n7/abs/nrg3684.html DNA methylation: roles in mammalian development http://www.nature.com/doifinder/10.1038/nrg3354 Histone modifications and organization of the genome http://www.nature.com/nrg/journal/v12/n1/full/nrg2905.html DNA methylation and histone modifications are linked http://www.nature.com/nrg/journal/v10/n5/abs/nrg2540.html 1.2.2 Post-transcriptional regulation 1.2.2.1 Regulation by non-coding RNAs Recent years have witnessed an explosion in noncoding RNA (ncRNA)-related research. Many publications implicated ncRNAs as important regulatory elements. Plants and animals produce many different types of ncRNAs such as long non-coding RNAs (lncRNAs), small-interferring RNAs (siRNAs), microRNAs (miRNAs), promoter-associated RNAs (PARs) and small nucleolar RNAs (snoRNAs). lncRNAs are typically &gt;200 bp long, they are involved in epigenetic regulation by interacting with chromatin remodeling factors and they function in gene regulation. siRNAs are short double-stranded RNAs which are involved in gene-regulation and transposon control, they silence their target genes by cooperating with Argonaute proteins . miRNAs are short single-stranded RNA molecules that interact with their target genes by using their complementary sequence and mark them for quicker degradation. PARs may regulate gene expression as well: they are ~18-200bp long ncRNAs originating from promoters of coding genes. snoRNAs also shown to play roles in gene regulation, although they are mostly believed to guide ribosomal RNA modifications. 1.2.2.2 Splicing regulation Splicing is regulated by regulatory elements on the pre-mRNA and proteins binding to those elements . Regulatory elements are categorized as splicing enhancers and repressors. They can be located either in exons or introns. Depending of their activity and their locations there are four types of regulatory elements: - exonic splicing enhancers (ESEs) - exonic splicing silencers (ESSs) - intronic splicing enhancers (ISEs) - intronic splicing silencers (ISSs). The majority of splicing repressors are heterogeneous nuclear ribonucleoproteins (hnRNPs). If splicing repressor protein bind silencer elements they reduce the change of nearby site to be used as splice junction. On the contrary, splicing enhancers are sites to which splicing activator proteins bind and binding on that region increases the probability that a nearby site will be used as a splice junction. Most of the activator proteins that bind to splicing enhancers are members of the SR protein family. Such proteins can recognize specific RNA recognition motifs. By regulating splicing exons can be skipped or included which creates protein diversity. Want to know more ? On miRNAs: Their genesis and modes of regulation http://www.sciencedirect.com/science/article/pii/S0092867404000455 Functions of small RNAs http://www.nature.com/nrg/journal/v15/n9/abs/nrg3765.html Functions of non coding RNAs http://www.nature.com/nrg/journal/v15/n6/abs/nrg3722.html on splicing: Wang, Zefeng; Christopher B. Burge (May 2008). “Splicing regulation: From a parts list of regulatory elements to an integrated splicing code”. RNA 14 (5): 802–813. doi:10.1261/rna.876308. ISSN 1355-8382. PMC 2327353. PMID 18369186. Retrieved 2013-08-15. "],
["shaping-the-genome-dna-mutation.html", "1.3 Shaping the genome: DNA mutation", " 1.3 Shaping the genome: DNA mutation Human and chimpanzee genomes are 98.8% similar. The 1.2% difference is what separetes us from chimpanzees. The further you move away from human in terms of evolutionary distance the higher the difference gets. However, even between the members of the same species differences in genome sequences exists. These differences are due to a process called mutation which drives differences between individuals but also the provides the fuel for evolution as the source of the genetic variation. Individuals with beneficial mutations can adapt to their surroundings better than others and in time these mutations which are beneficial for survival spreads in the population due to a process called “natural selection”. Selection acts upon individuals with beneficial features which gives them an edge for survival in a given environment. Genetic variation created by the mutations in individuals provide the material on which selection can act upon. If the selection process goes for a long time in a relatively isolated environment that requires adaptation, this population can evolve into a different species given enough time.This is the basic idea behind evolution in a nutshell, and without mutations providing the genetic variation there will be no evolution. Mutations in the genome occur due to multiple reasons. First, DNA replication is not an error-free process. Before a cell division, the DNA is replicated with 1 mistake per 10^8 to 10^10 base-pairs. Second, mutagens such as UV light can induce mutations on the genome. Third factor that contributes to mutation is imperfect DNA repair. Every day any human cell suffers multiple instances DNA damage. DNA repair enzymes are there to cope with this damage but they are also not error-free, depending on which DNA repair mechanism is used (there are multiple) mistakes will be made at varying rates. Mutations are classified by how many bases they effect, their effect on DNA structure and gene function. By their effect on DNA structure the mutations are classified as follows: Base substitution: A base is changed with another. Deletion: One or more bases is deleted. Insertion: New base or bases inserted into the genome. Microsatellite mutation: Small insertions or deletions of small tandemly repeating DNA segments. Inversion: A DNA fragment changes its orientation 180 degrees. Translocation: A DNA fragment moves to another location in the genome. Mutations can also be classified by their size as follows: Point mutations: mutations that involve one base. Substitutions, deletions and insertions are point mutations. They are also termed as single nucleotide polymorphisms (SNPs). small-scale mutations: mutations that involve several bases. Large-scale mutations: mutations which involve larger chromosomal regions. Transposable element insertions (where a segment of the genome jumps to another region in the genome) and segmental duplications ( a large region is copied multiple times in tandem) are typical large scale mutations. Aneuploidies: Insertions or deletions of whole chromosomes. Whole-genome polyploidies: duplications involving whole genome. Mutations by their effect on gene function can be classified as follows: gain-of-function mutations: A type of mutation in which the altered gene product possesses a new molecular function or a new pattern of gene expression. loss-of-function mutations: A mutation that results in reduced or abolished protein function. This is the more common type of mutation. "],
["high-throughput-experimental-methods-in-genomics.html", "1.4 High-throughput experimental methods in genomics", " 1.4 High-throughput experimental methods in genomics Most of the biological phenomena described above relating to transcription , gene regulation or DNA mutation can be measured over the entire genome using high-throughput experimental techniques, which are quickly becoming the standard for studying genome biology. In addition, their applications in the clinic are also gaining momemntum: there are already diagnostic tests that are based on these techniques. Some of the things that can be measured by high-throughput assays are as follows: Which genes are expressed and how much ? Where does a transcription factor bind ? Which bases are methylated in the genome ? Which transcripts are translated ? Where does RNA-binding proteins bind ? Which microRNAs are expressed ? Which parts of the genome are in contact with each other ? Where are the mutations in the genome located ? Which parts of the genome are nucleosome-free ? There are many more questions one can answer using modern genome-wide techniques and every other day a new variant of the existing techniques comes along to answer a new question. However, One has to keep in mind that these methods are at varying degrees of maturity and they all come with technical limitations and are not noise-free. Despite this, they are extremely useful for research and clinical purposes. And, thanks to these methods we are able to sequence and annotate genomes at a massive scale. 1.4.1 The general idea behind high-throughput techniques High-throughput methods aim to quantify or locate all or most of the genome that harbours the biological feature (expressed genes, binding sites, etc.) of interest. Most of the methods rely on some sort of enrichment of the the targeted biological feature. For example, if you want to measure expression of protein coding genes you need to be able to extract mRNA molecules with special post-transcriptional alterations that protein-coding genes acquire. If you are looking for transcription factor binding, you need to enrich for the DNA fragments that are bound by the protein if interest.This parts depends on available molecular biology and chemistry technques, and the final product of this part is RNA or DNA fragments. Next, you need to be able to tell where these fragments are coming from in the genome and how many of them are there. Microarrays were the standard tool for the quantification step until spread of sequencing techniques. In microarrays, one had to design complementary bases ,called “oligos” or “probes”, to the genetic material enriched via the experimental protocol. If the enriched material is complementary to the genetic material, a light signal will be produced and the intensity of the signal will be proportional to the amount of the genetic material pairing with that oligo. There will be more probes available for hybrdization (process of complementary bases forming bonds ), so the more fragments available stronger the signal. For this to be able to work, you need to know at least part of your genome sequence, and design probes. If you want to measure gene expression, your probes should overlap with genes and should be unique enough to not to bind sequences from other genes. This technology is now being replaced with sequencing technology, where you directly sequence your genetic material. If you have the sequence of your fragments, you can align them back to genome, see where they are coming from and count them. This is a better technology where the quantification is based on the real identiy of fragments rather than based on hybridization to designed probes. In summary HT techniques has the following steps, and this also summarized in Figure 1.6: Extraction: This is the step where you extract the genetic material of interest, RNA or DNA. and nrichment Enrichment: In this step, you enrich for the event you are interested in. For example, protein binding sites. In some cases such as whole-genome DNA sequencing there is no need for enrichment step. You just get fragments of genomic DNA and sequence them. Quantification: This is where you quantify your enriched material. Depending on the protocol you may need to quantify a control set as well, where you should see no enrichment or only background enrichment. FIGURE 1.6: Common steps of High-throughput assays in genome biology 1.4.2 High-throughput sequencing High-throughput sequencing, or massively parallel sequencing, is a collection of methods and technologies that can sequence DNA thousands/millons of fragments at a time. This is in contrast to older technologies that can produce a limited number of fragments at a time. Here, throughput refers to number of sequenced bases per hour. The older low-throughput sequencing methods have ~ 100 times less throughput compared to modern high-throughput methods. The increased throughput gives the ability to measure biological features on a genome-wide scale in a shorter time frame. Similar to other high-throughput methods, sequencing based methods also require an enrichment step. This step enriches for the features we are interested in. The main difference of the sequencing based methods is the quantification step. In high-throughput sequencing, enriched fragments are put through the sequencer which outputs the sequences for the fragments. Due to limitations in current leading technologies, only limited number of bases can be sequenced using from the input fragments. However, the length is usually enough to uniquely map the reads to the genome and quantify the input fragments. 1.4.2.1 High-throughput sequencing data If there is a genome available, the reads are aligned to the genome and based on the library preperation protocol different strategies are applied for analysis. A sequencing library is composed of fragments of RNA or DNA ready to be sequenced. The library preparation primarily depends on the experiment of interest. There are a number of library preperation protocols aimed at quantifying different signals from the genome. Some of the potential analysis strategies for different library-prep protocols and processed output of read alignments are depicted in Figure 1.7. For example, we maybe interested to quantify the gene expression. The experimental protocol, called RNA sequencing- RNA-seq, enriches for fragments of RNA that are coming from protein coding genes. Upon alignment, we can calculate the coverage profile which gives us a read count per base along the genome. This information can be stored in a text file or specialized file formats to be used in subsequent analysis or visualization. We can also just count how many reads overlap with exons of each gene and record read counts per gene for further analysis. This essentially produces a table with gene names and read counts for different samples. As we will see in later chapters, this is an essential information for statistical models that model RNA-seq data. Furthermore, we can stack up the reads and count how many times we see a base position in a read mismatches the base in the genome. Read aligners allow for mismatches, and for this reason we can see reads with mismatches. This information can be used to identify SNPs, and can be stored again in a tabular format with the information of position and mismatch type and number of reads supporting the mismatch. The original algorithms are a bit more complicated than just counting mismatches but the general idea is the same, what they are doing differently is trying to minimize false positive rates by using filters, so that not every mismatch is recorded as SNP. FIGURE 1.7: High-throughput sequencing summary 1.4.2.2 Future of high-throughput sequencing The sequencing tech is still evolving. Next frontier is the obtaining longer single-molecule reads and preferably being able to call base modifications on the fly. With longer reads, the genome asssembly will be easier for the regions that have high repeat content. With single-molecule sequencing, we will be able tell how many transcripts are present in a given cell population without relying on fragment amplification methods which can introduce biases. Another recent development is single-cell sequencing. Current technologies usually work on genetic material from thousands to millions of cells. This means that the results you receive represents the population of cells that were used in the experiment. However, there is a lot variation between the same type of cells, this variation is not observed at all. Newer sequecing techniques can work on single cells and give quantititave information on each cell. Want to know more ? Current and the future high-throughput sequencing technologies http://www.sciencedirect.com/science/article/pii/S1097276515003408 Illumina repository for different library preperation protocols for sequencing http://www.illumina.com/techniques/sequencing/ngs-library-prep/library-prep-methods.html "],
["visualization-and-data-repositories-for-genomics.html", "1.5 Visualization and data repositories for genomics", " 1.5 Visualization and data repositories for genomics There are ~100 animal genomes sequenced as of 2016. On top these, there are many research projects from either individual labs or consortiums that produce petabytes of auxiliary genomics data, such as ChIP-seq, RNA-seq, etc. There are two requirements for to be able to visualize genomes and its associated data, 1) you need to be able to work with a species that has a sequenced genome and 2) you want to have annotation on that genome, meaning at the very least you want to know where the genes are. Most genomes after sequencing quickly annotated with gene-predictions or know gene sequences are mapped on to it, you can also have conservation to other species to filter functional elements. If you are working with a model organism or human you will also have a lot of auxiliary information to help demarcate the functional regions such as regulatory regions, ncRNA, SNPs that are common in the population. Or you might have disease or tissue specific data available. . The more the organism is worked on the more auxiliary data you will have. 1.5.0.1 Accessing genome sequences and annotations via genome browsers As someone intends to work with genomics, you will need to visualize a large amount of data to make biological inferences or simply check regions of interest in the genome visually. Looking at the genome case by case with all the additional datasets is a necessary step to develop hypothesis and understand the data. Many genomes and associated data is available through genome browsers. A genome browser is a website or an app that helps you visualize the genome and all the available data associated with it. Via genome browsers, you will be able to see where genes are in relation to each other and other functional elements. You will be able to see gene structure. You will be able to see auxiliary data such as conservation, repeat content and SNPs. Here we review some of the popular browsers. UCSC genome browser: This is an online browser hosted by University of California, Santa Cruz at http://genome.ucsc.edu/. This is an interactive website that contains genomes and annotations for many species. You can search for genes or genome coordinates for the species of your interest. It is usually very responsive and allows you to visualize large amounts of data. In addition, it has multiple other tools that can be used in connection with the browser. One of the most useful tool is UCSC Table Browser, which lets you download the all the data you see on the browser, including sequence data, in multiple formats . Users can upload data or provide links to the data to visualize user specific data. Ensembl: This is another online browser maintained by European Bioinformatics Institute and the Wellcome Trust Sanger Institute in the UK, http://www.ensembl.org. Similar to UCSC browser, users can visualize genes or genomic coordinates from multiple species and it also comes with auxiliary data. Ensemlb is associated with Biomart tool which is similar to UCSC Table browser, can download genome data including all the auxiliary data set in multiple formats. IGV: Integrated genomics viewer (IGV) is a desktop application developed by Broad institute (https://www.broadinstitute.org/igv/). It is developed to deal with large amounts of high-throughput sequencing data which is harder to view in online browsers. IGV can integrate your local sequencing results with online annotation on your desktop machine. This is useful when viewing sequencing data, especially alignments. Other browsers mentioned above have similar functionalities however you will need to make your large sequencing data available online somewhere before it can be viewed by browsers. 1.5.0.2 Data repositories for high-throughput assays Genome browser contain lots of auxiliary high-throughput data. However, there are many more public high-throughput data sets available and they are certainly not available through genome browsers. Normally, every high-throughput dataset associated with a publication should be deposited to public archieves. There are two major public archives we use to deposit data. One of them is Gene expression Omnibus(GEO) hosted at http://www.ncbi.nlm.nih.gov/geo/, and the other one is European nucleotide archive(ENA) hosted at http://www.ebi.ac.uk/ena. These repositories accept high-throughput datasets and users can freely download and use these public data sets for their own research. Many data sets in these repositories are in their raw format, for example the format the sequencer provides mostly. Some data sets will also have processed data but that is not a norm. Apart from these repositories, there are multiple multi-national consortia that is dedicated to certain genome biology or disease related problems and they maintain their own databases and provide access to processed and raw data. Some of these consortia is mentioned below. Consortium what is it for? ENCODE Transcription factor binding sites, gene expression and epigenomics data for cell lines Epigenomics Roadmap Epigenomics data for multiple cell types The cancer genome atlas (TCGA) Expression, mutation and epigenomics data for multiple cancer types 1000 genomes project Human genetic variation data obtained by sequencing 1000s of individuals "],
["Rintro.html", "Chapter 2 Introduction to R for genomic data analysis", " Chapter 2 Introduction to R for genomic data analysis The aim of computational genomics is to provide biological interpretation and insights from high dimensional genomics data. Generally speaking, it is similar to any other kind of data analysis endeavor but often times doing computational genomics will require domain specific knowledge and tools. As new high-throughout experimental techniques on the rise, data analysis capabilities are sought-after features for researchers. The aim of this chapter is to first familiarize the readers with data analysis steps and then provide basics of R programming within the context of genomic data analysis.R is a free statistical programming language that is popular among researchers and data miners to build software and analyze data. Although basic R programming tutorials are easily accessible, we are aiming to introduce the subject with the genomic context in the background. The examples and narrative will always be from real-life situations when you try to analyze genomic data with R. We believe tailoring material to the context of genomics makes a difference when learning this programming language for sake of analyzing genomic data. "],
["steps-of-genomic-data-analysis.html", "2.1 Steps of (genomic) data analysis", " 2.1 Steps of (genomic) data analysis Regardless of the analysis type, the data analysis has a common pattern. We will discuss this general pattern and how it applies to genomics problems. The data analysis steps typically include data collection, quality check and cleaning, processing, modeling, visualization and reporting. Although, one expects to go through these steps in a linear fashion, it is normal to go back and repeat the steps with different parameters or tools. In practice, data analysis requires going through the same steps over and over again in order to be able to do a combination of the following: a) answering other related questions, b) dealing with data quality issues later realized, c) including new data sets to the analysis. We will now go through brief explanation of the steps within the context of genomic data analysis. 2.1.1 Data collection Data collection refers to any source, experiment or survey that provides data for the data analysis question you have. In genomics, data collection is done by high-throughput assays introduced in chapter 1. One can also use publicly available data sets and specialized data bases also mentioned in chapter 1. How much data and what type of data you should collect depends on the question you are trying to answer ant the technical and biological variability of the system you are studying. 2.1.2 Data quality check and cleaning In general, the data analysis almost always deals with imperfect data. It is common to have missing values or measurements that are noisy. Data quality check and cleaning aims to identify any data quality issue and clean it from the dataset. High-throughout genomics data is produced by technologies that could embed technical biases into the data. If we were to give an example from sequencing, the sequenced reads do not have the same quality of bases called. Towards the ends of the reads, you might have bases that might be called incorrectly. Identifying those low quality bases and removing them will improve read mapping step. 2.1.3 Data processing This step refers to processing the data to a format that is suitable for exploratory analysis and modeling. Often times, the data will not come in ready to analyze format. You may need to convert it to other formats by transforming data points (such as log transforming, normalizing etc), or subset the data set with some arbitrary or pre-defined condition. In terms of genomics, processing includes multiple steps. Following the sequencing analysis example above, processing will include aligning reads to the genome and quantification over genes or regions of interest. This simply counting how many reads are covering your regions of interest. This quantity can give you ideas about how much a gene expressed if your experimental protocol was RNA sequencing. This can be followed by some normalization to aid the next step. 2.1.4 Exploratory data analysis and modeling This phase usually takes in the processed or semi-processed data and applies machine-learning or statistical methods to explore the data. Typically, one needs to see relationship between variables measured, relationship between samples based on the variables measured. At this point, we might be looking to see if the samples group as expected by the experimental design, are there outliers or any other anomalies ? After this step you might want to do additional clean up or re-processing to deal with anomalies. Another related step is modeling. This generally refers to modelling your variable of interest based on other variables you measured. In the context of genomics, it could be that you are trying to predict disease status of the patients from expression of genes you measured from their tissue samples. Then your variable of interest is the disease status and . This is generally called predictive modeling and could be solved with regression based or any other machine-learning methods. This kind of approach is generally called “predictive modeling”. Statistical modeling would also be a part of this modeling step, this can cover predictive modeling as well where we use statistical methods such as linear regression. Other analyses such as hypothesis testing, where we have an expectation and we are trying to confirm that expectation is also related to statistical modeling. A good example of this in genomics is the differential gene expression analysis. This can be formulated as comparing two data sets, in this case expression values from condition A and condition B, with the expectation that condition A and condition B has similar expression values. You will see more on this on chapter 3. 2.1.5 Visualization and reporting Visualization is necessary for all the previous steps more or less. But in the final phase, we need final figures, tables and text that describes the outcome of your analysis. This will be your report. In genomics, we use common data visualization methods as well as specific visualization methods developed or popularized by genomic data analysis. You will see many popular visualization methods in chapters 3 and @{genomicIntervals} 2.1.6 Why use R for genomics ? R, with its statistical analysis heritage, plotting features and rich user-contributed packages is one of the best languages for the task of analyzing genomic data. High-dimensional genomics datasets are usually suitable to be analyzed with core R packages and functions. On top of that, Bioconductor and CRAN have an array of specialized tools for doing genomics specific analysis. Here is a list of computational genomics tasks that can be completed using R. 2.1.6.1 Data cleanup and processing Most of general data clean up, such as removing incomplete columns and values, reorganizing and transforming data, these tasks can be achieved using R. In addition, with the help of packages R can connect to databases in various formats such as mySQL, mongoDB, etc., and query and get the data to R environment using database specific tools. On top of these, genomic data specific processing and quality check can be achieved via R/Bioconductor packages. For example, sequencing read quality checks and even HT-read alignments can be achieved via R packages. 2.1.6.2 General data anaylsis and exploration Most genomics data sets are suitable for application of general data analysis tools. In some cases, you may need to preprocess the data to get it to a state that is suitable for application such tools. Here is a non-exhaustive list of what kind of things can be done via R. unsupervised data analysis: clustering (k-means, hierarchical), matrix factorization (PCA, ICA etc) supervised data analysis: generalized linear models, support vector machines, randomForests 2.1.6.3 Genomics specific data analysis methods R/Bioconductor gives you access to multitude of other bioinformatics specific algorithms. Here are some of the things you can do. Sequence analysis: TF binding motifs, GC content and CpG counts of a given DNA sequence Differential expression (or arrays and sequencing based measurements) Gene set/Pathway analysis: What kind of genes are enriched in my gene set Genomic Interval opertaions such as Overlapping CpG islands with transcription start sites, and filtering based on overlaps Overlapping aligned reads with exons and counting aligned reads per gene 2.1.6.4 Visualization Visualization is an important part of all data analysis techniques including computational genomics. Again, you can use core visualization techniques in R and also genomics specific ones with the help of specific packages. Here are some of the things you can do with R. Basic plots: Histograms, scatter plots, bar plots, box plots, heatmaps ideograms and circus plots for genomics provides visualization of different features over the whole genome. meta-profiles of genomic features, such as read enrichment over all promoters Visualization of quantitative assays for given locus in the genome "],
["getting-started-with-r.html", "2.2 Getting started with R", " 2.2 Getting started with R Download and install R http://cran.r-project.org/ and RStudio http://www.rstudio.com/ if you do not have them already. Rstudio is optional but it is a great tool if you are just starting to learn R. You will need specific data sets to run the codes in this document. Download the data.zip[URL to come] and extract it to your directory of choice. The folder name should be “data” and your R working directory should be level above the data folder. That means in your R console, when you type “dir(“data”)” you should be able to see the contents of the data folder. You can change your working directory by setwd() command and get your current working directory with getwd() command in R. In RStudio, you can click on the top menu and change the location of your working directory via user interface. 2.2.1 Installing packages R packages are add-ons to base R that help you achieve additional tasks that are not directly supported by base R. It is by the action of these extra functionality that R excels as a tool for computational genomics. Bioconductor project (http://bioconductor.org/) is a dedicated package repository for computational biology related packages. However main package repository of R, called CRAN, has also computational biology related packages. In addition, R-Forge(http://r-forge.r-project.org/), GitHub(https://github. com/), and googlecode(http://code.google.com) are other locations where R packages might be hosted. You can install CRAN packages using install.packages(). (# is the comment character in R) # install package named &quot;randomForests&quot; from CRAN install.packages(&quot;randomForests&quot;) You can install bioconductor packages with a specific installer script # get the installer package source(&quot;http://bioconductor.org/biocLite.R&quot;) # install bioconductor package &quot;rtracklayer&quot; biocLite(&quot;rtracklayer&quot;) You can install packages from github using install_github() function from devtools library(devtools) install_github(&quot;hadley/stringr&quot;) Another way to install packages are from the source. # download the source file download.file(&quot;http://goo.gl/3pvHYI&quot;, destfile=&quot;methylKit_0.5.7.tar.gz&quot;) # install the package from the source file install.packages(&quot;methylKit_0.5.7.tar.gz&quot;, repos=NULL,type=&quot;source&quot;) # delete the source file unlink(&quot;methylKit_0.5.7.tar.gz&quot;) You can also update CRAN and Bioconductor packages. # updating CRAN packages update.packages() # updating bioconductor packages source(&quot;http://bioconductor.org/biocLite.R&quot;) biocLite(&quot;BiocUpgrade&quot;) 2.2.2 Installing packages in custom locations If you will be using R on servers or computing clusters rather than your personal computer it is unlikely that you will have administrator access to install packages. In that case, you can install packages in custom locations by telling R where to look for additional packages. This is done by setting up an .Renviron file in your home directory and add the following line: R_LIBS=~/Rlibs This tells R that “Rlibs” directory at your home directory will be the first choice of locations to look for packages and install packages (The directory name and location is up to you above is just an example). You should go and create that directory now. After that, start a fresh R session and start installing packages. From now on, packages will be installed to your local directory where you have read-write access. 2.2.3 Getting help on functions and packages You can get help on functions by help() and help.search() functions. You can list the functions in a package with ls() function library(MASS) ls(&quot;package:MASS&quot;) # functions in the package ls() # objects in your R enviroment # get help on hist() function ?hist help(&quot;hist&quot;) # search the word &quot;hist&quot; in help pages help.search(&quot;hist&quot;) ??hist 2.2.3.1 More help needed? In addition, check package vignettes for help and practical understanding of the functions. All Bionconductor packages have vignettes that walk you through example analysis. Google search will always be helpful as well, there are many blogs and web pages that have posts about R. R-help, Stackoverflow and R-bloggers are usually source of good and reliable information. "],
["computations-in-r.html", "2.3 Computations in R", " 2.3 Computations in R R can be used as an ordinary calculator, some say it is an over-grown calculator. Here are some examples. Remember # is the comment character. The comments give details about the operations in case they are not clear. 2 + 3 * 5 # Note the order of operations. log(10) # Natural logarithm with base e 5^2 # 5 raised to the second power 3/2 # Division sqrt(16) # Square root abs(3-7) # Absolute value of 3-7 pi # The number exp(2) # exponential function # This is a comment line "],
["data-structures.html", "2.4 Data structures", " 2.4 Data structures R has multiple data structures. If you are familiar with excel you can think of a single excel sheet as a table and data structures as building blocks of that table. Most of the time you will deal with tabular data sets or you will want to transform your raw data to a tabular data set, and you will try to manipulate this tabular data set in some way. For example, you may want to take sub-sections of the table or extract all the values in a column. For these and similar purposes, it is essential to know what are the common data structures in R and how they can be used. R deals with named data structures, this means you can give names to data structures and manipulate or operate on them using those names. It will be clear soon what we mean by this if “named data structures” does not ring a bell. 2.4.1 Vectors Vectors are one the core R data structures. It is basically a list of elements of the same type (numeric,character or logical). Later you will see that every column of a table will be represented as a vector. R handles vectors easily and intuitively. You can create vectors with c() function, however that is not the only way. The operations on vectors will propagate to all the elements of the vectors. x&lt;-c(1,3,2,10,5) #create a vector named x with 5 components x = c(1,3,2,10,5) x ## [1] 1 3 2 10 5 y&lt;-1:5 #create a vector of consecutive integers y y+2 #scalar addition ## [1] 3 4 5 6 7 2*y #scalar multiplication ## [1] 2 4 6 8 10 y^2 #raise each component to the second power ## [1] 1 4 9 16 25 2^y #raise 2 to the first through fifth power ## [1] 2 4 8 16 32 y #y itself has not been unchanged ## [1] 1 2 3 4 5 y&lt;-y*2 y #it is now changed ## [1] 2 4 6 8 10 r1&lt;-rep(1,3) # create a vector of 1s, length 3 length(r1) #length of the vector ## [1] 3 class(r1) # class of the vector ## [1] &quot;numeric&quot; a&lt;-1 # this is actually a vector length one 2.4.2 Matrices A matrix refers to a numeric array of rows and columns. You can think of it as a stacked version of vectors where each row or column is a vector. One of the easiest ways to create a matrix is to combine vectors of equal length using cbind(), meaning ‘column bind’. x&lt;-c(1,2,3,4) y&lt;-c(4,5,6,7) m1&lt;-cbind(x,y);m1 ## x y ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 ## [4,] 4 7 t(m1) # transpose of m1 ## [,1] [,2] [,3] [,4] ## x 1 2 3 4 ## y 4 5 6 7 dim(m1) # 2 by 5 matrix ## [1] 4 2 You can also directly list the elements and specify the matrix: m2&lt;-matrix(c(1,3,2,5,-1,2,2,3,9),nrow=3) m2 ## [,1] [,2] [,3] ## [1,] 1 5 2 ## [2,] 3 -1 3 ## [3,] 2 2 9 Matrices and the next data structure data frames are tabular data structures. You can subset them using and providing desired rows and columns to subset. Figure 2.1 shows how that works conceptually. FIGURE 2.1: slicing/subsetting of a matrix and a data frame 2.4.3 Data Frames A data frame is more general than a matrix, in that different columns can have different modes (numeric, character, factor, etc.). A data frame can be constructed by data.frame() function. For example, we illustrate how to construct a data frame from genomic intervals or coordinates. chr &lt;- c(&quot;chr1&quot;, &quot;chr1&quot;, &quot;chr2&quot;, &quot;chr2&quot;) strand &lt;- c(&quot;-&quot;,&quot;-&quot;,&quot;+&quot;,&quot;+&quot;) start&lt;- c(200,4000,100,400) end&lt;-c(250,410,200,450) mydata &lt;- data.frame(chr,start,end,strand) #change column names names(mydata) &lt;- c(&quot;chr&quot;,&quot;start&quot;,&quot;end&quot;,&quot;strand&quot;) mydata # OR this will work too ## chr start end strand ## 1 chr1 200 250 - ## 2 chr1 4000 410 - ## 3 chr2 100 200 + ## 4 chr2 400 450 + mydata &lt;- data.frame(chr=chr,start=start,end=end,strand=strand) mydata ## chr start end strand ## 1 chr1 200 250 - ## 2 chr1 4000 410 - ## 3 chr2 100 200 + ## 4 chr2 400 450 + There are a variety of ways to extract the elements of a data frame. You can extract certain columns using column numbers or names, or you can extract certain rows by using row numbers. You can also extract data using logical arguments, such as extracting all rows that has a value in a column larger than your threshold. mydata[,2:4] # columns 2,3,4 of data frame ## start end strand ## 1 200 250 - ## 2 4000 410 - ## 3 100 200 + ## 4 400 450 + mydata[,c(&quot;chr&quot;,&quot;start&quot;)] # columns chr and start from data frame ## chr start ## 1 chr1 200 ## 2 chr1 4000 ## 3 chr2 100 ## 4 chr2 400 mydata$start # variable start in the data frame ## [1] 200 4000 100 400 mydata[c(1,3),] # get 1st and 3rd rows ## chr start end strand ## 1 chr1 200 250 - ## 3 chr2 100 200 + mydata[mydata$start&gt;400,] # get all rows where start&gt;400 ## chr start end strand ## 2 chr1 4000 410 - 2.4.4 Lists An ordered collection of objects (components). A list allows you to gather a variety of (possibly unrelated) objects under one name. # example of a list with 4 components # a string, a numeric vector, a matrix, and a scalar w &lt;- list(name=&quot;Fred&quot;, mynumbers=c(1,2,3), mymatrix=matrix(1:4,ncol=2), age=5.3) w ## $name ## [1] &quot;Fred&quot; ## ## $mynumbers ## [1] 1 2 3 ## ## $mymatrix ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## $age ## [1] 5.3 You can extract elements of a list using the [] convention using either its position in the list or its name. w[[3]] # 3rd component of the list ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 w[[&quot;mynumbers&quot;]] # component named mynumbers in list ## [1] 1 2 3 w$age ## [1] 5.3 2.4.5 Factors Factors are used to store categorical data. They are important for statistical modeling since categorical variables are treated differently in statistical models than continuous variables. This ensures categorical data treated accordingly in statistical models. features=c(&quot;promoter&quot;,&quot;exon&quot;,&quot;intron&quot;) f.feat=factor(features) Important thing to note is that when you are reading a data.frame with read.table() or creating a data frame with data.frame() character columns are stored as factors by default, to change this behavior you need to set stringsAsFactors=FALSE in read.table() and/or data.frame() function arguments. "],
["data-types.html", "2.5 Data types", " 2.5 Data types There are four common data types in R, they are numeric, logical, character and integer. All these data types can be used to create vectors natively. #create a numeric vector x with 5 components x&lt;-c(1,3,2,10,5) x ## [1] 1 3 2 10 5 #create a logical vector x x&lt;-c(TRUE,FALSE,TRUE) x ## [1] TRUE FALSE TRUE # create a character vector x&lt;-c(&quot;sds&quot;,&quot;sd&quot;,&quot;as&quot;) x ## [1] &quot;sds&quot; &quot;sd&quot; &quot;as&quot; class(x) ## [1] &quot;character&quot; # create an integer vector x&lt;-c(1L,2L,3L) x ## [1] 1 2 3 class(x) ## [1] &quot;integer&quot; "],
["reading-and-writing-data.html", "2.6 Reading and writing data", " 2.6 Reading and writing data Most of the genomics data sets are in the form of genomic intervals associated with a score. That means mostly the data will be in table format with columns denoting chromosome, start positions, end positions, strand and score. One of the popular formats is BED format used primarily by UCSC genome browser but most other genome browsers and tools will support BED format. We have all the annotation data in BED format. In R, you can easily read tabular format data with read.table() function. enhancerFilePath=system.file(&quot;extdata&quot;, &quot;subset.enhancers.hg18.bed&quot;, package=&quot;compGenomRData&quot;) cpgiFilePath=system.file(&quot;extdata&quot;, &quot;subset.cpgi.hg18.bed&quot;, package=&quot;compGenomRData&quot;) # read enhancer marker BED file enh.df &lt;- read.table(enhancerFilePath, header = FALSE) # read CpG island BED file cpgi.df &lt;- read.table(cpgiFilePath, header = FALSE) # check first lines to see how the data looks like head(enh.df) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 ## 1 chr20 266275 267925 . 1000 . 9.11 13.17 -1 ## 2 chr20 287400 294500 . 1000 . 10.53 13.02 -1 ## 3 chr20 300500 302500 . 1000 . 9.10 13.39 -1 ## 4 chr20 330400 331800 . 1000 . 6.39 13.51 -1 ## 5 chr20 341425 343400 . 1000 . 6.20 12.99 -1 ## 6 chr20 437975 439900 . 1000 . 6.31 13.52 -1 head(cpgi.df) ## V1 V2 V3 V4 ## 1 chr20 195575 195851 CpG:_28 ## 2 chr20 207789 208148 CpG:_32 ## 3 chr20 219055 219437 CpG:_33 ## 4 chr20 225831 227155 CpG:_135 ## 5 chr20 252826 256323 CpG:_286 ## 6 chr20 275376 276977 CpG:_116 You can save your data by writing it to disk as a text file. A data frame or matrix can be written out by using write.table() function. Now let us write out cpgi.df, we will write it out as a tab-separated file, pay attention to the arguments. write.table(cpgi.df,file=&quot;cpgi.txt&quot;,quote=FALSE, row.names=FALSE,col.names=FALSE,sep=&quot;\\t&quot;) You can save your R objects directly into a file using save() and saveRDS() and load them back in with load() and readRDS(). By using these functions you can save any R object whether or not they are in data frame or matrix classes. save(cpgi.df,enh.df,file=&quot;mydata.RData&quot;) load(&quot;mydata.RData&quot;) # saveRDS() can save one object at a type saveRDS(cpgi.df,file=&quot;cpgi.rds&quot;) x=readRDS(&quot;cpgi.rds&quot;) head(x) One important thing is that with save() you can save many objects at a time and when they are loaded into memory with load( they retain their variable names. For example, in the above code when you use load(&quot;mydata.RData&quot;) in a fresh R session, an object names “cpg.df” will be created. That means you have to figure out what name you gave it to the objects before saving them. On the contrary to that, when you save an object by saveRDS() and read by readRDS() the name of the object is not retained, you need to assign the output of readRDS() to a new variable (“x” in the above code chunk). "],
["plotting-in-r.html", "2.7 Plotting in R", " 2.7 Plotting in R R has great support for plotting and customizing plots. We will show only a few below. Let us sample 50 values from normal distribution and plot them as a histogram. # sample 50 values from normal distribution # and store them in vector x x&lt;-rnorm(50) hist(x) # plot the histogram of those values We can modify all the plots by providing certain arguments to the plotting function. Now let’s give a title to the plot using ‘main’ argument. We can also change the color of the bars using ‘col’ argument. You can simply provide the name of the color. Below, we are using ‘red’ for the color. See Figure below for the result this chunk. hist(x,main=&quot;Hello histogram!!!&quot;,col=&quot;red&quot;) Next, we will make a scatter plot. Scatter plots are one the most common plots you will encounter in data analysis. We will sample another set of 50 values and plotted those against the ones we sampled earlier. Scatterplot shows values of two variables for a set of data points. It is useful to visualize relationships between two variables. It is frequently used in connection with correlation and linear regression. There are other variants of scatter plots which show density of the points with different colors. We will show examples of those that in following chapters. The scatter plot from our sampling experiment is shown in the figure. Notice that, in addition to main we used “xlab” and “ylab” arguments to give labels to the plot. You can customize the plots even more than this. See ?plot and ?par for more arguments that can help you customize the plots. # randomly sample 50 points from normal distribution y&lt;-rnorm(50) #plot a scatter plot # control x-axis and y-axis labels plot(x,y,main=&quot;scatterplot of random samples&quot;, ylab=&quot;y values&quot;,xlab=&quot;x values&quot;) we can also plot boxplots for vectors x and y. Boxplots depict groups of numerical data through their quartiles. The edges of the box denote 1st and 3rd quartile, and the line that crosses the box is the median. Whiskers usually are defined using interquantile range: lowerWhisker=Q1-1.5[IQR] and upperWhisker=Q1+1.5[IQR] In addition, outliers can be depicted as dots. In this case, outliers are the values that remain outside the whiskers. boxplot(x,y,main=&quot;boxplots of random samples&quot;) Next up is bar plot which you can plot by barplot() function. We are going to plot four imaginary percentage values and color them with two colors, and this time we will also show how to draw a legend on the plot using legend() function. perc=c(50,70,35,25) barplot(height=perc, names.arg=c(&quot;CpGi&quot;,&quot;exon&quot;,&quot;CpGi&quot;,&quot;exon&quot;), ylab=&quot;percentages&quot;,main=&quot;imagine %s&quot;, col=c(&quot;red&quot;,&quot;red&quot;,&quot;blue&quot;,&quot;blue&quot;)) legend(&quot;topright&quot;,legend=c(&quot;test&quot;,&quot;control&quot;), fill=c(&quot;red&quot;,&quot;blue&quot;)) "],
["saving-plots.html", "2.8 Saving plots", " 2.8 Saving plots If you want to save your plots to an image file there are couple of ways of doing that. Normally, you will have to do the following: 1. Open a graphics device 2. Create the plot 3. Close the graphics device pdf(&quot;mygraphs/myplot.pdf&quot;,width=5,height=5) plot(x,y) dev.off() Alternatively, you can first create the plot then copy the plot to a graphic device. plot(x,y) dev.copy(pdf,&quot;mygraphs/myplot.pdf&quot;,width=7,height=5) dev.off() "],
["functions-and-control-structures-for-ifelse-etc-.html", "2.9 Functions and control structures (for, if/else etc.)", " 2.9 Functions and control structures (for, if/else etc.) 2.9.1 User defined functions Functions are useful for transforming larger chunks of code to re-usable pieces of code. Generally, if you need to execute certain tasks with variable parameters then it is time you write a function. A function in R takes different arguments and returns a definite output, much like mathematical functions. Here is a simple function takes two arguments, x and y, and returns the sum of their squares. sqSum&lt;-function(x,y){ result=x^2+y^2 return(result) } # now try the function out sqSum(2,3) ## [1] 13 Functions can also output plots and/or messages to the terminal. Here is a function that prints a message to the terminal: sqSumPrint&lt;-function(x,y){ result=x^2+y^2 cat(&quot;here is the result:&quot;,result,&quot;\\n&quot;) } # now try the function out sqSumPrint(2,3) ## here is the result: 13 Sometimes we would want to execute a certain part of the code only if certain condition is satisfied. This condition can be anything from the type of an object (Ex: if object is a matrix execute certain code), or it can be more complicated such as if object value is between certain thresholds. Let us see how they can be used3. They can be used anywhere in your code, now we will use it in a function. cpgi.df &lt;- read.table(&quot;intro2R_data/data/subset.cpgi.hg18.bed&quot;, header = FALSE) # function takes input one row # of CpGi data frame largeCpGi&lt;-function(bedRow){ cpglen=bedRow[3]-bedRow[2]+1 if(cpglen&gt;1500){ cat(&quot;this is large\\n&quot;) } else if(cpglen&lt;=1500 &amp; cpglen&gt;700){ cat(&quot;this is normal\\n&quot;) } else{ cat(&quot;this is short\\n&quot;) } } largeCpGi(cpgi.df[10,]) largeCpGi(cpgi.df[100,]) largeCpGi(cpgi.df[1000,]) 2.9.2 Loops and looping structures in R When you need to repeat a certain task or a execute a function multiple times, you can do that with the help of loops. A loop will execute the task until a certain condition is reached. The loop below is called a “for-loop” and it executes the task sequentially 10 times. for(i in 1:10){ # number of repetitions cat(&quot;This is iteration&quot;) # the task to be repeated print(i) } ## This is iteration[1] 1 ## This is iteration[1] 2 ## This is iteration[1] 3 ## This is iteration[1] 4 ## This is iteration[1] 5 ## This is iteration[1] 6 ## This is iteration[1] 7 ## This is iteration[1] 8 ## This is iteration[1] 9 ## This is iteration[1] 10 The task above is a bit pointless, normally in a loop, you would want to do something meaningful. Let us calculate the length of the CpG islands we read in earlier. Although this is not the most efficient way of doing that particular task, it serves as a good example for looping. The code below will be execute hundred times, and it will calculate the length of the CpG islands for the first 100 islands in the data frame (by subtracting the end coordinate from the start coordinate). Note:If you are going to run a loop that has a lot of repetitions, it is smart to try the loop with few repetitions first and check the results. This will help you make sure the code in the loop works before executing it for thousands of times. # this is where we will keep the lenghts # for now it is an empty vector result=c() # start the loop for(i in 1:100){ #calculate the length len=cpgi.df[i,3]-cpgi.df[i,2]+1 #append the length to the result result=c(result,len) } # check the results head(result) ## [1] 277 360 383 1325 3498 1602 2.9.2.1 apply family functions instead of loops R has other ways of repeating tasks that tend to be more efficient than using loops. They are known as the “apply” family of functions, which include apply, lapply, mapply and tapply (and some other variants). All of these functions apply a given function to a set of instances and returns the result of those functions for each instance. The differences between them is that they take different type of inputs. For example apply works on data frames or matrices and applies the function on each row or column of the data structure. lapply works on lists or vectors and applies a function which takes the list element as an argument. Next we will demonstrate how to use apply() on a matrix. The example applies the sum function on the rows of a matrix, it basically sums up the values on each row of the matrix, which is conceptualized in Figure 2.2. FIGURE 2.2: apply concept in R mat=cbind(c(3,0,3,3),c(3,0,0,0),c(3,0,0,3),c(1,1,0,0),c(1,1,1,0),c(1,1,1,0)) result&lt;-apply(mat,1,sum) result ## [1] 12 3 5 6 # OR you can define the function as an argument to apply() result&lt;-apply(mat,1,function(x) sum(x)) result ## [1] 12 3 5 6 Notice that we used a second argument which equals to 1, that indicates that rows of the matrix/ data frame will be the input for the function. If we change the second argument to 2, this will indicate that columns should be the input for the function that will be applied. See Figure 2.3 for the visualization of apply() on columns. FIGURE 2.3: apply function on columns result&lt;-apply(mat,2,sum) result ## [1] 9 3 6 2 3 3 Next, we will use lapply(), which applies a function on a list or a vector. The function that will be applied is a simple function that takes the square of a given number. input=c(1,2,3) lapply(input,function(x) x^2) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 9 mapply() is another member of apply family, it can apply a function on an unlimited set of vectors/lists, it is like a version of lapply that can handle multiple vectors as arguments. In this case, the argument to the mapply() is the function to be applied and the sets of parameters to be supplied as arguments of the function. This conceptualized Figure 2.4, the function to be applied is a function that takes to arguments and sums them up. The arguments to be summed up are in the format of vectors, Xs and Ys. mapply() applies the summation function to each pair in Xs and Ys vector. Notice that the order of the input function and extra arguments are different for mapply. FIGURE 2.4: mapply concept Xs=0:5 Ys=c(2,2,2,3,3,3) result&lt;-mapply(function(x,y) sum(x,y),Xs,Ys) result ## [1] 2 3 4 6 7 8 2.9.2.2 apply family functions on multiple cores If you have large data sets apply family functions can be slow (although probably still better than for loops). If that is the case, you can easily use the parallel versions of those functions from parallel package. These functions essentially divide your tasks to smaller chunks run them on separate CPUs and merge the results from those parallel operations. This concept is visualized at Figure below , mcapply runs the summation function on three different processors. Each processor executes the summation function on a part of the data set, and the results are merged and returned as a single vector that has the same order as the input parameters Xs and Ys. FIGURE 2.5: mcapplyconcept 2.9.2.3 Vectorized Functions in R The above examples have been put forward to illustrate functions and loops in R because functions using sum() are not complicated and easy to understand. You will probably need to use loops and looping structures with more complicated functions. In reality, most of the operations we used do not need the use of loops or looping structures because there are already vectorized functions that can achieve the same outcomes, meaning if the input arguments are R vectors the output will be a vector as well, so no need for loops or vectorization. For example, instead of using mapply() and sum() functions we can just use + operator and sum up Xs and Ys. result=Xs+Ys result ## [1] 2 3 4 6 7 8 In order to get the column or row sums, we can use the vectorized functions colSums() and rowSums(). colSums(mat) ## [1] 9 3 6 2 3 3 rowSums(mat) ## [1] 12 3 5 6 However, remember that not every function is vectorized in R, use the ones that are. But sooner or later, apply family functions will come in handy. "],
["exercises.html", "2.10 Exercises", " 2.10 Exercises 2.10.1 Computations in R Sum 2 and 3, use + Take the square root of 36, use sqrt() Take the log10 of 1000, use function log10() Take the log2 of 32, use function log2() Assign the sum of 2,3 and 4 to variable x Find the absolute value of 5 - 145 using abs() function Calculate the square root of 625, divide it by 5 and assign it to variable x. Ex: y= log10(1000)/5, the previous statement takes log10 of 1000, divides it by 5 and assigns the value to variable y Multiply the value you get from previous exercise with 10000, assign it variable x Ex: y=y*5, multiplies y with 5 and assigns the value to y. KEY CONCEPT: results of computations or arbitrary values can be stored in variables we can re-use those variables later on and over-write them with new values 2.10.2 Data structures in R Make a vector of 1,2,3,5 and 10 using c(), assign it to vec variable. Ex: vec1=c(1,3,4) makes a vector out of 1,3,4. Check the length of your vector with length(). Ex: length(vec1) should return 3 Make a vector of all numbers between 2 and 15. Ex: vec=1:6 makes a vector of numbers between 1 and 6, assigns to vec variable Make a vector of 4s repeated 10 times using rep() function. Ex: rep(x=2,times=5) makes a vector of 2s repeated 5 times Make a logical vector with TRUE, FALSE values of length 4, use c(). Ex: c(TRUE,FALSE) Make a character vector of gene names PAX6,ZIC2,OCT4 and SOX2. Ex: avec=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) a makes a character vector of a,b and c Subset the vector using [] notation, get 5th and 6th elements. Ex: vec1[1] gets the first element. vec1[c(1,3)] gets 1st and 3rd elements You can also subset any vector using a logical vector in []. Run the following: myvec=1:5 myvec[c(TRUE,TRUE,FALSE,FALSE,FALSE)] # the length of the logical vector should be equal to length(myvec) myvec[c(TRUE,FALSE,FALSE,FALSE,TRUE)] ==,&gt;,&lt;, &gt;=, &lt;= operators create logical vectors. See the results of the following operations: myvec &gt; 3 myvec == 4 myvec &lt;= 2 myvec != 4 Use &gt; operator in myvec[ ] to get elements larger than 2 in myvec whic is described above make a 5x3 matrix (5 rows, 3 columns) using matrix(). Ex: matrix(1:6,nrow=3,ncol=2) makes a 3x2 matrix using numbers between 1 and 6 What happens when you use byrow = TRUE in your matrix() as an additional argument? Ex: mat=matrix(1:6,nrow=3,ncol=2,byrow = TRUE) Extract first 3 columns and first 3 rows of your matrix using [] notation. Extract last two rows of the matrix you created earlier. Ex: mat[2:3,] or mat[c(2,3),] extracts 2nd and 3rd rows. Extract the first two columns and run class() on the result. Extract first column and run class() on the result, compare with the above exercise. Make a data frame with 3 columns and 5 rows, make sure first column is sequence of numbers 1:5, and second column is a character vector. Ex: df=data.frame(col1=1:3,col2=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;),col3=3:1) # 3x3 data frame. Remember you need to make 3x5 data frame Extract first two columns and first two rows. HINT: Same notation as matrices Extract last two rows of the data frame you made. HINT: Same notation as matrices Extract last two columns using column names of the data frame you made. Extract second column using column names. You can use [] or $ as in lists, use both in two different answers. Extract rows where 1st column is larger than 3. HINT: you can get a logical vector using &gt; operator ,logical vectors can be used in [] when subsetting. Extract rows where 1st column is larger than or equal to 3. Convert data frame to the matrix. HINT: use as.matrix(). Observe what happens to numeric values in the data frame. Make a list using list() function, your list should have 4 elements the one below has 2. Ex: mylist= list(a=c(1,2,3),b=c(&quot;apple,&quot;orange&quot;)) Select the 1st element of the list you made using $ notation. Ex: mylist$a selects first element named “a” Select the 4th element of the list you made earlier using $ notation. Select the 1st element of your list using [ ] notation. Ex: mylist[1] selects first element named “a”, you get a list with one element. Ex: mylist[&quot;a&quot;] selects first element named “a”, you get a list with one element. select the 4th element of your list using ‘’ notation. Make a factor using factor(), with 5 elements. Ex: fa=factor(c(&quot;a&quot;,&quot;a&quot;,&quot;b&quot;)) Convert a character vector to factor using as.factor(). First, make a character vector using c() then use as.factor(). Convert the factor you made above to character using as.character(). 2.10.3 Reading in and writing data out in R Read CpG island (CpGi) data from the compGenomRData package CpGi.table.hg18.txt, this is a tab-separated file, store it in a variable called “cpgi”. Use cpgtFilePath=system.file(&quot;extdata&quot;, &quot;CpGi.table.hg18.txt&quot;, package=&quot;compGenomRData&quot;) to get the file path within the installed compGenomRData package. Use head() on CpGi to see first few rows. Why doesn’t the following work? see ‘sep’ argument at help(read.table). cpgtFilePath=system.file(&quot;extdata&quot;, &quot;CpGi.table.hg18.txt&quot;, package=&quot;compGenomRData&quot;) cpgtFilePath ## [1] &quot;/home/travis/R/Library/compGenomRData/extdata/CpGi.table.hg18.txt&quot; cpgiSepComma=read.table(cpgtFilePath,header=TRUE,sep=&quot;,&quot;) head(cpgiSepComma) ## chrom.chromStart.chromEnd.name.length.cpgNum.gcNum.perCpg.perGc.obsExp ## 1 chr1\\t18598\\t19673\\tCpG: 116\\t1075\\t116\\t787\\t21.6\\t73.2\\t0.83 ## 2 chr1\\t124987\\t125426\\tCpG: 30\\t439\\t30\\t295\\t13.7\\t67.2\\t0.64 ## 3 chr1\\t317653\\t318092\\tCpG: 29\\t439\\t29\\t295\\t13.2\\t67.2\\t0.62 ## 4 chr1\\t427014\\t428027\\tCpG: 84\\t1013\\t84\\t734\\t16.6\\t72.5\\t0.64 ## 5 chr1\\t439136\\t440407\\tCpG: 99\\t1271\\t99\\t777\\t15.6\\t61.1\\t0.84 ## 6 chr1\\t523082\\t523977\\tCpG: 94\\t895\\t94\\t570\\t21\\t63.7\\t1.04 What happens when stringsAsFactors=FALSE ? cpgiHF=read.table(&quot;intro2R_data/data/CpGi.table.hg18.txt&quot;,header=FALSE,sep=&quot;\\t&quot;, stringsAsFactors=FALSE) Read only first 10 rows of the CpGi table. Use cpgtFilePath=system.file(&quot;extdata&quot;,&quot;CpGi.table.hg18.txt&quot;,package=&quot;compGenomRData&quot;) to get the file path, then use read.table() with argument header=FALSE. Use head() to see the results. Write CpG islands to a text file called “my.cpgi.file.txt”. Write the file to your home folder, you can use file=&quot;~/my.cpgi.file.txt&quot; in linux. ~/ denotes home folder. Same as above but this time make sure use quote=FALSE,sep=&quot;\\t&quot; and row.names=FALSE arguments. Save the file to “my.cpgi.file2.txt” and compare it with “my.cpgi.file.txt” Write out the first 10 rows of ‘cpgi’ data frame. HINT: use subsetting for data frames we learned before. Write the first 3 columns of ‘cpgi’ data frame. Write CpG islands only on chr1. HINT: use subsetting with [], feed a logical vector using == operator. Read two other data sets “rn4.refseq.bed” and “rn4.refseq2name.txt” with header=FALSE, assign them to df1 and df2 respectively. They are again included in the compGenomRData package, and you can use system.file() function to get the file paths. Use head() to see what is inside of the the data frames above. Merge data sets using merge() and assign the results to variable named ‘new.df’, and use head() to see the results. 2.10.4 Plotting in R Please run the following for the rest of the exercises. set.seed(1001) x1=1:100+rnorm(100,mean=0,sd=15) y1=1:100 Make a scatter plot using x1 and y1 vectors generated above. Use main argument to give a title to plot() as in plot(x,y,main=&quot;title&quot;) Use xlab argument to set a label to x-axis.Use ylab argument to set a label to y-axis. See what mtext(side=3,text=&quot;hi there&quot;) does. HINT: mtext stands for margin text. See what mtext(side=2,text=&quot;hi there&quot;) does.check your plot after execution. You can use paste() as ‘text’ argument in mtext() try that, you need to re-plot. your plot first. HINT: mtext(side=3,text=paste(...)) See what paste() is used for. paste(&quot;Text&quot;,&quot;here&quot;) ## [1] &quot;Text here&quot; myText=paste(&quot;Text&quot;,&quot;here&quot;) myText ## [1] &quot;Text here&quot; Use mtext() and paste() to put a margin text on the plot. cor() calculates correlation between two vectors. Pearson correlation is a measure of the linear correlation (dependence) between two variables X and Y. corxy=cor(x1,y1) # calculates pearson correlation Try use mtext(),cor() and paste() to display correlation coefficient on your scatterplot ? Change the colors of your plot using col argument. Ex: plot(x,y,col=&quot;red&quot;) Use pch=19 as an argument in your plot() command. Use pch=18 as an argument to your plot() command. Make histogram of x1 with hist() function.Histogram is a graphical representation of the data distribution. You can change colors with ‘col’, add labels with ‘xlab’, ‘ylab’, and add a ‘title’ with ‘main’ arguments. Try all these in a histogram. Make boxplot of y1 with boxplot(). Make boxplots of x1 and y1 vectors in the same plot. In boxplot use horizontal = TRUE argument make multiple plots with par(mfrow=c(2,1)) run par(mfrow=c(2,1)) make a boxplot make a histogram Do the same as above but this time with par(mfrow=c(1,2)) Save your plot using “Export” button in Rstudio Save your plot by running : dev.copy(pdf,file=&quot;plot.file.pdf&quot;);dev.off() Save your plot running : dev.copy(png,filename=&quot;plot.file.png&quot;);dev.off() Another way to save the plot is the following Open a graphics device Create the plot Close the graphics device EXTRA: Making color density scatterplot. You can make a scatter plot showing density of points rather than points themselves. If you use points it looks like this: x2=1:1000+rnorm(1000,mean=0,sd=200) y2=1:1000 plot(x2,y2,pch=19,col=&quot;blue&quot;) If you use smoothScatter() function, you get the densities. smoothScatter(x2,y2,colramp=colorRampPalette(c(&quot;white&quot;,&quot;blue&quot;, &quot;green&quot;,&quot;yellow&quot;,&quot;red&quot;))) Now, plot with colramp=heat.colors argument and then use a custom color scale using the following argument. colramp = colorRampPalette(c(&quot;white&quot;,&quot;blue&quot;, &quot;green&quot;,&quot;yellow&quot;,&quot;red&quot;))) 2.10.5 Functions and control structures (for, if/else etc.) Read CpG island data as shown below for the rest of the exercises. cpgtFilePath=system.file(&quot;extdata&quot;, &quot;CpGi.table.hg18.txt&quot;, package=&quot;compGenomRData&quot;) cpgi=read.table(cpgtFilePath,header=TRUE,sep=&quot;\\t&quot;) head(cpgi) ## chrom chromStart chromEnd name length cpgNum ## 1 chr1 18598 19673 CpG: 116 1075 116 ## 2 chr1 124987 125426 CpG: 30 439 30 ## 3 chr1 317653 318092 CpG: 29 439 29 ## 4 chr1 427014 428027 CpG: 84 1013 84 ## 5 chr1 439136 440407 CpG: 99 1271 99 ## 6 chr1 523082 523977 CpG: 94 895 94 ## gcNum perCpg perGc obsExp ## 1 787 21.6 73.2 0.83 ## 2 295 13.7 67.2 0.64 ## 3 295 13.2 67.2 0.62 ## 4 734 16.6 72.5 0.64 ## 5 777 15.6 61.1 0.84 ## 6 570 21.0 63.7 1.04 Check values at perGc column using a histogram. ‘perGc’ stands for GC percent =&gt; percentage of C+G nucleotides Make a boxplot for ‘perGc’ column Use if/else structure to decide if given GC percent high, low or medium. If it is low, high, or medium. low &lt; 60, high&gt;75, medium is between 60 and 75 use greater or less than operators &lt; or &gt; . Fill in the values in the in code below, where it is written ‘YOU_FILL_IN’ GCper=65 # check if GC value is lower than 60, # assign &quot;low&quot; to result if(&#39;YOU_FILL_IN&#39;){ result=&quot;low&quot; cat(&quot;low&quot;) } else if(&#39;YOU_FILL_IN&#39;){ # check if GC value is higher than 75, assign &quot;high&quot; to result result=&quot;high&quot; cat(&quot;high&quot;) }else{ # if those two conditions fail then it must be &quot;medium&quot; result=&quot;medium&quot; } result Write a function that takes a value of GC percent and decides if it is low, high, or medium. low &lt; 60, high&gt;75, medium is between 60 and 75. Fill in the values in the in code below, where it is written ‘YOU_FILL_IN’ GCclass&lt;-function(my.gc){ YOU_FILL_IN return(result) } GCclass(10) # should return &quot;low&quot; GCclass(90) # should return &quot;high&quot; GCclass(65) # should return &quot;medium&quot; Use a for loop to get GC percentage classes for gcValues below. Use the function you wrote above. gcValues=c(10,50,70,65,90) for( i in YOU_FILL_IN){ YOU_FILL_IN } Use lapply to get to get GC percentage classes for gcValues. Example: vec=c(1,2,4,5) power2=function(x){ return(x^2) } lapply(vec,power2) Use sapply to get values to get GC percentage classes for gcValues Is there a way to decide on the GC percentage class of given vector of GCpercentages without using if/else structure and loops ? if so, how can you do it? HINT: subsetting using &lt; and &gt; operators "],
["stats.html", "Chapter 3 Statistics and Exploratory Data Analysis for Genomics", " Chapter 3 Statistics and Exploratory Data Analysis for Genomics This chapter will summarize statistics and exploratory data analysis methods frequently used in computational genomics. As these fields are continuously evolving, the techniques introduced here do not form an exhaustive list but mostly corner stone methods that are often and still being used. In addition, we focused on giving intuitive and practical understanding of the methods with relevant examples from the field. If you want to dig deeper into statistics and math, beyond what is described here, we included appropriate references with annotation after each major section. "],
["how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html", "3.1 How to summarize collection of data points: The idea behind statistical distributions", " 3.1 How to summarize collection of data points: The idea behind statistical distributions In biology and many other fields data is collected via experimentation. The nature of the experiments and natural variation in biology makes it impossible to get the same exact measurements every time you measure something. For example, if you are measuring gene expression values for a certain gene, say PAX6, and let’s assume you are measuring expression per sample and cell with any method( microarrays, rt-qPCR, etc.). You will not get the same expression value even if your samples are homogeneous. Due to technical bias in experiments or natural variation in the samples. Instead, we would like to describe this collection of data some other way that represents the general properties of the data. The figure shows a sample of 20 expression values from PAX6 gene. FIGURE 3.1: Expression of PAX6 gene in 20 replicate experiments 3.1.1 Describing the central tendency: mean and median As seen in the figure above, the points from this sample are distributed around a central value and the histogram below the dot plot shows number of points in each bin. Another observation is that there are some bins that have more points than others. If we want to summarize what we observe, we can try to represent the collection of data points with an expression value that is typical to get, something that represents the general tendency we observe on the dot plot and the histogram. This value is sometimes called central value or central tendency, and there are different ways to calculate such a value. In the figure above, we see that all the values are spread around 6.13 (red line), and that is indeed what we call mean value of this sample of expression values. It can be calculated with the following formula \\(\\overline{X}=\\sum_{i=1}^n x_i/n\\), where \\(x_i\\) is the expression value of an experiment and \\(n\\) is the number of expression value obtained from the experiments. In R, mean() function will calculate the mean of a provided vector of numbers. This is called a “sample mean”. In reality the possible values of PAX6 expression for all cells (provided each cell is of the identical cell type and is in identical conditions) are much much more than 20. If we had the time and the funding to sample all cells and measure PAX6 expression we would get a collection values that would be called, in statistics, a “population”. In our case the population will look like the left hand side of the figure below. What we have done with our 20 data points is that we took a sample of PAX6 expression values from this population, and calculated the sample mean. FIGURE 3.2: Expression of all possible PAX6 gene expressions measures on all available biological samples (left). Expression of PAX6 gene from statistical sample, a random subset, from the population of biological samples (Right). The mean of the population is calculated the same way but traditionally Greek letter \\(\\mu\\) is used to denote the population mean. Normally, we would not have access to the population and we will use sample mean and other quantities derived from the sample to estimate the population properties. This is the basic idea behind statistical inference which we will see this in action in later sections as well. We estimate the population parameters from the sample parameters and there is some uncertainty associated with those estimates. We will be trying to assess those uncertainties and make decisions in the presence of those uncertainties. We are not yet done with measuring central tendency. There are other ways to describe it, such as the median value. Mean can be affected by outliers easily. If certain values are very high or low from the bulk of the sample this will shift mean towards those outliers. However, median is not affected by outliers. It is simply the value in a distribution where half of the values are above and the other half is below. In R, median() function will calculate the mean of a provided vector of numbers. Let’s create a set of random numbers and calculate their mean and median using R. #create 10 random numbers from uniform distribution x=runif(10) # calculate mean mean(x) ## [1] 0.3739 # calculate median median(x) ## [1] 0.3278 3.1.2 Describing the spread: measurements of variation Another useful way to summarize a collection of data points is to measure how variable the values are. You can simply describe the range of the values , such as minimum and maximum values. You can easily do that in R with range() function. A more common way to calculate variation is by calculating something called “standard deviation” or the related quantity called “variance”. This is a quantity that shows how variable the values are, a value around zero indicates there is not much variation in the values of the data points, and a high value indicates high variation in the values. The variance is the squared distance of data points from the mean. Population variance is again a quantity we usually do not have access to and is simply calculate as follows \\(\\sigma^2=\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{n}\\), where \\(\\mu\\) is the population mean, \\(x_i\\) is the ith data point in the population and \\(n\\) is the population size. However, when the we have only access to a sample this formulation is biased. It means that it underestimates the population variance, so we make a small adjustment when we calculate the sample variance, denoted as \\(s^2\\): \\[ \\begin{aligned} s^2=\\sum_{i=1}^n \\frac{(x_i-\\overline{X})^2}{n-1} &amp;&amp; \\text{ where $x_i$ is the ith data point and $\\overline{X}$ is the sample mean.} \\end{aligned} \\] The sample standard deviation is simply the square-root of the sample variance. The good thing about standard deviation is that it has the same unit as the mean so it is more intuitive. \\[s=\\sqrt{\\sum_{i=1}^n \\frac{(x_i-\\overline{X})^2}{n-1}}\\] We can calculate sample standard deviation and variation with sd() and var() functions in R. These functions take vector of numeric values as input and calculate the desired quantities. Below we use those functions on a randomly generated vector of numbers. x=rnorm(20,mean=6,sd=0.7) var(x) ## [1] 0.2531 sd(x) ## [1] 0.5031 One potential problem with the variance is that it could be affected by outliers. The points that are too far away from the mean will have a large affect on the variance even though there might be few of them. A way to measure variance that could be less affected by outliers is looking at where bulk of the distribution is. How do we define where the bulk is? One common way is to look at the the difference between 75th percentile and 25th percentile, this effectively removes a lot of potential outliers which will be towards the edges of the range of values. This is called interquartile range , and can be easily calculated using R via IQR() function and the quantiles of a vector is calculated with quantile() function. Let us plot the boxplot for a random vector and also calculate IQR using R. In the boxplot below, 25th and 75th percentiles are the edges of the box, and the median is marked with a thick line going through roughly middle the box. x=rnorm(20,mean=6,sd=0.7) IQR(x) ## [1] 0.5011 quantile(x) ## 0% 25% 50% 75% 100% ## 5.437 5.743 5.860 6.244 6.558 boxplot(x,horizontal = T) FIGURE 3.3: Boxplot showing 25th percentile and 75th percentile and median for a set of points sample from a normal distribution with mean=6 and standard deviation=0.7 3.1.2.1 Frequently used statistical distributions The distributions have parameters (such as mean and variance) that summarizes them but also they are functions that assigns each outcome of a statistical experiment to its probability of occurrence. One distribution that you will frequently encounter is the normal distribution or Gaussian distribution. The normal distribution has a typical “bell-curve” shape and, characterized by mean and standard deviation. A set of data points that follow normal distribution mostly will be close to the mean but spread around it controlled by the standard deviation parameter. That means if we sample data points from a normal distribution we are more likely to sample nearby the mean and sometimes away from the mean. Probability of an event occurring is higher if it is nearby the mean. The effect of the parameters for normal distribution can be observed in the following plot. FIGURE 3.4: Different parameters for normal distribution and effect of those on the shape of the distribution The normal distribution is often denoted by \\(\\mathcal{N}(\\mu,\\,\\sigma^2)\\) When a random variable \\(X\\) is distributed normally with mean \\(\\mu\\) and variance \\(\\sigma^2\\), we write: \\[X\\ \\sim\\ \\mathcal{N}(\\mu,\\,\\sigma^2).\\] The probability density function of Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is as follows \\[P(x)=\\frac{1}{\\sigma\\sqrt{2\\pi} } \\; e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} } \\] The probability density function gives the probability of observing a value on a normal distribution defined by \\(\\mu\\) and \\(\\sigma\\) parameters. Often times, we do not need the exact probability of a value but we need the probability of observing a value larger or smaller than a critical value or reference point. For example, we might want to know the probability of \\(X\\) being smaller than or equal to -2 for a normal distribution with mean 0 and standard deviation 2. ,\\(P(X &lt;= -2 \\; | \\; \\mu=0,\\sigma=2)\\). In this case, what we want is the are under the curve shaded in blue. To be able to that we need to integrate the probability density function but we will usually let software do that. Traditionally, one calculates a Z-score which is simply \\((X-\\mu)/\\sigma=(-2-0)/2= -1\\), and corresponds to how many standard deviations you are away from the mean. This is also called “standardization”, the corresponding value is distributed in “standard normal distribution” where \\(\\mathcal{N}(0,\\,1)\\). After calculating the Z-score, we can go look up in a table, that contains the area under the curve for the left and right side of the Z-score, but again we use software for that tables are outdated. Below we are showing the Z-score and the associated probabilities derived from the calculation above for \\(P(X &lt;= -2 \\; | \\; \\mu=0,\\sigma=2)\\). FIGURE 3.5: Z-score and associated probabilities for Z= -1 In R, family of *norm functions (rnorm,dnorm,qnorm and pnorm) can be used to operate with normal distribution, such as calculating probabilities and generating random numbers drawn from normal distribution. # get the value of probability density function when X= -2, # where mean=0 and sd=2 dnorm(-2, mean=0, sd=2) ## [1] 0.121 # get the probability of P(X =&lt; -2) where mean=0 and sd=2 pnorm(-2, mean=0, sd=2) ## [1] 0.1587 # get the probability of P(X &gt; -2) where mean=0 and sd=2 pnorm(-2, mean=0, sd=2,lower.tail = FALSE) ## [1] 0.8413 # get 5 random numbers from normal dist with mean=0 and sd=2 rnorm(5, mean=0 , sd=2) ## [1] -1.8109 -1.9221 -0.5147 0.8217 -0.7901 # get y value corresponding to P(X &gt; y) = 0.15 with mean=0 and sd=2 qnorm( 0.15, mean=0 , sd=2) ## [1] -2.073 There are many other distribution functions in R that can be used the same way. You have to enter the distribution specific parameters along with your critical value, quantiles or number of random numbers depending on which function you are using in the family.We will list some of those functions below. dbinom is for binomial distribution. This distribution is usually used to model fractional data and binary data. Examples from genomics includes methylation data. dpois is used for Poisson distribution and dnbinom is used for negative binomial distribution. These distributions are used to model count data such as sequencing read counts. df (F distribution) and dchisq (Chi-Squared distribution) are used in relation to distribution of variation. F distribution is used to model ratios of variation and Chi-Squared distribution is used to model distribution of variations. You will frequently encounter these in linear models and generalized linear models. 3.1.3 Precision of estimates: Confidence intervals When we take a random sample from a population and compute a statistic, such as the mean, we are trying to approximate the mean of the population. How well this sample statistic estimates the population value will always be a concern. A confidence interval addresses this concern because it provides a range of values which is plausible to contain the population parameter of interest. Normally, we would not have access to a population. If we did, we would not have to estimate the population parameters and its precision. When we do not have access to the population, one way to estimate intervals is to repeatedly take samples from the original sample with replacement, that is we take a data point from the sample we replace, and we take another data point until we have sample size of the original sample. Then, we calculate the parameter of interest, in this case mean, and repeat this step a large number of times, such as 1000. At this point, we would have a distribution of re-sampled means, we can then calculate the 2.5th and 97.5th percentiles and these will be our so-called 95% confidence interval. This procedure, resampling with replacement to estimate the precision of population parameter estimates, is known as the bootstrap. Let’s see how we can do this in practice. We simulate a sample coming from a normal distribution (but we pretend we don’t know the population parameters). We will try to estimate the precision of the mean of the sample using bootstrap to build confidence intervals. set.seed(21) sample1= rnorm(50,20,5) # simulate a sample # do bootstrap resampling, sampling with replacement boot.means=do(1000) * mean(resample(sample1)) # get percentiles from the bootstrap means q=quantile(boot.means[,1],p=c(0.025,0.975)) # plot the histogram hist(boot.means[,1],col=&quot;cornflowerblue&quot;,border=&quot;white&quot;, xlab=&quot;sample means&quot;) abline(v=c(q[1], q[2] ),col=&quot;red&quot;) text(x=q[1],y=200,round(q[1],3),adj=c(1,0)) text(x=q[2],y=200,round(q[2],3),adj=c(0,0)) FIGURE 3.6: Precision estimate of the sample mean using 1000 bootstrap samples. Confidence intervals derived from the bootstrap samples are shown with red lines. If we had a convenient mathematical method to calculate confidence interval we could also do without resampling methods. It turns out that if we take repeated samples from a population of with sample size \\(n\\), the distribution of means ( \\(\\overline{X}\\)) of those samples will be approximately normal with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{n}\\). This is also known as Central Limit Theorem(CLT) and is one of the most important theorems in statistics. This also means that \\(\\frac{\\overline{X}-\\mu}{\\sigma\\sqrt{n}}\\) has a standard normal distribution and we can calculate the Z-score and then we can get the percentiles associated with the Z-score. Below, we are showing the Z-score calculation for the distribution of \\(\\overline{X}\\), and then we are deriving the confidence intervals starting with the fact that probability of Z being between -1.96 and 1.96 is 0.95. We then use algebra to show that the probability that unknown \\(\\mu\\) is captured between \\(\\overline{X}-1.96\\sigma/\\sqrt{n}\\) and \\(\\overline{X}+1.96\\sigma/\\sqrt{n}\\) is 0.95, which is commonly known as 95% confidence interval. \\[\\begin{array}{ccc} Z=\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}\\\\ P(-1.96 &lt; Z &lt; 1.96)=0.95 \\\\ P(-1.96 &lt; \\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}} &lt; 1.96)=0.95\\\\ P(\\mu-1.96\\sigma/\\sqrt{n} &lt; \\overline{X} &lt; \\mu+1.96\\sigma/\\sqrt{n})=0.95\\\\ P(\\overline{X}-1.96\\sigma/\\sqrt{n} &lt; \\mu &lt; \\overline{X}+1.96\\sigma/\\sqrt{n})=0.95\\\\ confint=[\\overline{X}-1.96\\sigma/\\sqrt{n},\\overline{X}+1.96\\sigma/\\sqrt{n}] \\end{array}\\] A 95% confidence interval for population mean is the most common common interval to use, and would mean that we would expect 95% of the interval estimates to include the population parameter, in this case the mean. However, we can pick any value such as 99% or 90%. We can generalize the confidence interval for \\(100(1-\\alpha)\\) as follows: \\[\\overline{X} \\pm Z_{\\alpha/2}\\sigma/\\sqrt{n}\\] In R, we can do this using qnorm() function to get Z-scores associated with \\({\\alpha/2}\\) and \\({1-\\alpha/2}\\). As you can see, the confidence intervals we calculated using CLT are very similar to the ones we got from bootstrap for the same sample. For bootstrap we got \\([19.21, 21.989]\\) and for the CLT based estimate we got \\([19.23638, 22.00819]\\). alpha=0.05 sd=5 n=50 mean(sample1)+qnorm(c(alpha/2,1-alpha/2))*sd/sqrt(n) ## [1] 19.24 22.01 The good thing about CLT as long as the sample size is large regardless of the population distribution, the distribution of sample means drawn from that population will always be normal. Here we are repeatedly drawing samples 1000 times with sample size \\(n\\)=10,30, and 100 from a bimodal, exponential and a uniform distribution and we are getting sample mean distributions following normal distribution. FIGURE 3.7: Sample means are normally distributed regardless of the population distribution they are drawn from. However, we should note that how we constructed the confidence interval using standard normal distribution, \\(N(0,1)\\), only works when the when we know the population standard deviation. In reality, we usually have only access to a sample and have no idea about the population standard deviation. If this is the case we should use estimate the standard deviation using sample standard deviation and use something called t distribution instead of standard normal distribution in our interval calculation. Our confidence interval becomes \\(\\overline{X} \\pm t_{\\alpha/2}s/\\sqrt{n}\\), with t distribution parameter \\(d.f=n-1\\), since now the following quantity is t distributed \\(\\frac{\\overline{X}-\\mu}{s/\\sqrt{n}}\\) instead of standard normal distribution. The t distribution is similar to standard normal distribution has mean 0 but its spread is larger than the normal distribution especially when sample size is small, and has one parameter \\(v\\) for the degrees of freedom, which is \\(n-1\\) in this case. Degrees of freedom is simply number of data points minus number of parameters estimated. Here we are estimating the mean from the data and the distribution is for the means, therefore degrees of freedom is \\(n-1\\). FIGURE 3.8: Normal distribution and t distribution with different degrees of freedom. With increasing degrees of freedom, t distribution approximates the normal distribution better. "],
["how-to-test-for-differences-between-samples.html", "3.2 How to test for differences between samples", " 3.2 How to test for differences between samples Often times we would want to compare sets of samples. Such comparisons include if wild-type samples have different expression compared to mutants or if healthy samples are different from disease samples in some measurable feature (blood count, gene expression, methylation of certain loci). Since there is variability in our measurements, we need to take that into account when comparing the sets of samples. We can simply subtract the means of two samples, but given the variability of sampling, at the very least we need to decide a cutoff value for differences of means, small differences of means can be explained by random chance due to sampling. That means we need to compare the difference we get to a value that is typical to get if the difference between two group means were only due to sampling. If you followed the logic above, here we actually introduced two core ideas of something called “hypothesis testing”, this is simply using statistics to determine the probability that a given hypothesis (if two sample sets are from the same population or not) is true. Formally, those two core ideas are as follows: Decide on a hypothesis to test, often called “null hypothesis” (\\(H_0\\)). In our case, the hypothesis is there is no difference between sets of samples. An the “Alternative hypothesis” (\\(H_1\\)) is there is a difference between the samples. Decide on a statistic to test the truth of the null hypothesis. Calculate the statistic Compare it to a reference value to establish significance, the P-value. Based on that either reject or not reject the null hypothesis, \\(H_0\\) 3.2.1 randomization based testing for difference of the means There is one intuitive way to go about this. If we believe there are no differences between samples that means the sample labels (test-control or healthy-disease) has no meaning. So, if we randomly assign labels to the samples that and calculate the difference of the mean, this creates a null distribution for the \\(H_0\\) where we can compare the real difference and measure how unlikely it is to get such a value under the expectation of the null hypothesis. We can calculate all possible permutations to calculate the null distribution. However, sometimes that is not very feasible and equivalent approach would be generating the null distribution by taking a smaller number of random samples with shuffled group membership. Below, we are doing this process in R. We are first simulating two samples from two different distributions. These would be equivalent to gene expression measurements obtained under different conditions. Then, we calculate the differences in the means and do the randomization procedure to get a null distribution when we assume there is no difference between samples, \\(H_0\\). We then calculate how often we would get the original difference we calculated under the assumption that \\(H_0\\) is true. set.seed(100) gene1=rnorm(30,mean=4,sd=2) gene2=rnorm(30,mean=2,sd=2) org.diff=mean(gene1)-mean(gene2) gene.df=data.frame(exp=c(gene1,gene2), group=c( rep(&quot;test&quot;,30),rep(&quot;control&quot;,30) ) ) exp.null &lt;- do(1000) * diff(mosaic::mean(exp ~ shuffle(group), data=gene.df)) hist(exp.null[,1],xlab=&quot;null distribution | no difference in samples&quot;, main=expression(paste(H[0],&quot; :no difference in means&quot;) ), xlim=c(-2,2),col=&quot;cornflowerblue&quot;,border=&quot;white&quot;) abline(v=quantile(exp.null[,1],0.95),col=&quot;red&quot; ) abline(v=org.diff,col=&quot;blue&quot; ) text(x=quantile(exp.null[,1],0.95),y=200,&quot;0.05&quot;,adj=c(1,0),col=&quot;red&quot;) text(x=org.diff,y=200,&quot;org. diff.&quot;,adj=c(1,0),col=&quot;blue&quot;) FIGURE 3.9: The null distribution for differences of means obtained via randomization. The original difference is marked via blue line. The red line marks the value that corresponds to P-value of 0.05 p.val=sum(exp.null[,1]&gt;org.diff)/length(exp.null[,1]) p.val ## [1] 0 After doing random permutations and getting a null distribution, it is possible to get a confidence interval for the distribution of difference in means. This is simply the 2.5th and 97.5th percentiles of the null distribution, and directly related to the P-value calculation above. 3.2.2 Using t-test for difference of the means between two samples We can also calculate the difference between means using a t-test. Sometimes we will have too few data points in a sample to do meaningful randomization test, also randomization takes more time than doing a t-test. This is a test that depends on the t distribution. The line of thought follows from the CLT and we can show differences in means are t distributed. There are couple of variants of the t-test for this purpose. If we assume the variances are equal we can use the following version \\[t = \\frac{\\bar {X}_1 - \\bar{X}_2}{s_{X_1X_2} \\cdot \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\\] where \\[s_{X_1X_2} = \\sqrt{\\frac{(n_1-1)s_{X_1}^2+(n_2-1)s_{X_2}^2}{n_1+n_2-2}}\\] In the first equation above the quantity is t distributed with \\(n_1+n_2-2\\) degrees of freedom. We can calculate the quantity then use software to look for the percentile of that value in that t distribution, which is our P-value. When we can not assume equal variances we use “Welch’s t-test” which is the default t-test in R and also works well when variances and the sample sizes are the same. For this test we calculate the following quantity: \\[t = {\\overline{X}_1 - \\overline{X}_2 \\over s_{\\overline{X}_1 - \\overline{X}_2}}\\] where \\[s_{\\overline{X}_1 - \\overline{X}_2} = \\sqrt{{s_1^2 \\over n_1} + {s_2^2 \\over n_2}} \\] and the degrees of freedom equals to: \\[\\mathrm{d.f.} = \\frac{(s_1^2/n_1 + s_2^2/n_2)^2}{(s_1^2/n_1)^2/(n_1-1) + (s_2^2/n_2)^2/(n_2-1)} \\] Luckily, R does all those calculations for us. Below we will show the use of t.test() function in R. We will use it on the samples we simulated above. # Welch&#39;s t-test stats::t.test(gene1,gene2) ## ## Welch Two Sample t-test ## ## data: gene1 and gene2 ## t = 3.8, df = 48, p-value = 5e-04 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.8724 2.8728 ## sample estimates: ## mean of x mean of y ## 4.058 2.185 # t-test with equal varience assumption stats::t.test(gene1,gene2,var.equal=TRUE) ## ## Two Sample t-test ## ## data: gene1 and gene2 ## t = 3.8, df = 58, p-value = 4e-04 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.8771 2.8681 ## sample estimates: ## mean of x mean of y ## 4.058 2.185 A final word on t-tests: they generally assume population where samples coming from have normal distribution, however it is been shown t-test can tolerate deviations from normality. Especially, when two distributions are moderately skewed in the same direction. This is due to central limit theorem which says means of samples will be distributed normally no matter the population distribution if sample sizes are large. 3.2.3 multiple testing correction We should think of hypothesis testing as a non-error-free method of making decisions. There will be times when we declare something significant and accept \\(H_1\\) but we will be wrong. These decisions are also called “false positives” or “false discoveries”, this is also known as “type I error”. Similarly, we can fail to reject a hypothesis when we actually should. These cases are known as “false negatives”, also known as “type II error”. The ratio of true negatives to the sum of true negatives and false positives (\\(\\frac{TN}{FP+TN}\\)) is known as specificity. And we usually want to decrease the FP and get higher specificity. The ratio of true positives to the sum of true positives and false negatives (\\(\\frac{TP}{TP+FN}\\)) is known as sensitivity. And, again we usually want to decrease the FN and get higher sensitivity. Sensitivity is also known as “power of a test” in the context of hypothesis testing. More powerful tests will be highly sensitive and will do less type II errors. For the t-test the power is positively associated with sample size and the effect size. Higher the sample size, smaller the standard error and looking for the larger effect sizes will similarly increase the power. The general summary of these the different combination of the decisions are included in the table below. \\(H_0\\) is TRUE, [Gene is NOT differentially expressed] \\(H_1\\) is TRUE, [Gene is differentially expressed] Accept \\(H_0\\) (claim that the gene is not differentially expressed) True Negatives (TN) False Negatives (FN) ,type II error \\(m_0\\): number of truly null hypotheses reject \\(H_0\\) (claim that the gene is differentially expressed) False Positives (FP) ,type I error True Positives (TP) \\(m-m_0\\): number of truly alternative hypotheses We expect to make more type I errors as the number of tests increase, that means we will reject the null hypothesis by mistake. For example, if we perform a test the 5% significance level, there is a 5% chance of incorrectly rejecting the null hypothesis if the null hypothesis is true. However, if we make 1000 tests where all null hypotheses are true for each of them, the average number of incorrect rejections is 50. And if we apply the rules of probability, there are is almost a 100% chance that we will have at least one incorrect rejection. There are multiple statistical techniques to prevent this from happening. These techniques generally shrink the P-values obtained from multiple tests to higher values, if the individual P-value is low enough it survives this process. The most simple method is just to multiply the individual, P-value (\\(p_i\\)) with the number of tests (\\(m\\)): \\(m \\cdot p_i\\), this is called “Bonferroni correction”. However, this is too harsh if you have thousands of tests. Other methods are developed to remedy this. Those methods rely on ranking the P-values and dividing \\(m \\cdot p_i\\) by the rank,\\(i\\), :\\(\\frac{m \\cdot p_i }{i}\\), this is derived from Benjamini–Hochberg procedure. This procedure is developed to control for “False Discovery Rate (FDR)” , which is proportion of false positives among all significant tests. And in practical terms, we get the “FDR adjusted P-value” from the procedure described above. This gives us an estimate of proportion of false discoveries for a given test. To elaborate, p-value of 0.05 implies that 5% of all tests will be false positives. An FDR adjusted p-value of 0.05 implies that 5% of significant tests will be false positives. The FDR adjusted P-values will result in a lower number of false positives. One final method that is also popular is called the “q-value” method and related to the method above. This procedure relies on estimating the proportion of true null hypotheses from the distribution of raw p-values and using that quantity to come up with what is called a “q-value”, which is also an FDR adjusted P-value . That can be practically defined as “the proportion of significant features that turn out to be false leads.” A q-value 0.01 would mean 1% of the tests called significant at this level will be truly null on average. Within the genomics community q-value and FDR adjusted P-value are synonymous although they can be calculated differently. In R, the base function p.adjust() implements most of the p-value correction methods described above. For the q-value, we can use the qvalue package from Bioconductor. Below we are demonstrating how to use them on a set of simulated p-values.The plot shows that Bonferroni correction does a terrible job. FDR(BH) and q-value approach are better but q-value approach is more permissive than FDR(BH). library(qvalue) data(hedenfalk) qvalues &lt;- qvalue(hedenfalk$p)$q bonf.pval=p.adjust(hedenfalk$p,method =&quot;bonferroni&quot;) fdr.adj.pval=p.adjust(hedenfalk$p,method =&quot;fdr&quot;) plot(hedenfalk$p,qvalues,pch=19,ylim=c(0,1), xlab=&quot;raw P-values&quot;,ylab=&quot;adjusted P-values&quot;) points(hedenfalk$p,bonf.pval,pch=19,col=&quot;red&quot;) points(hedenfalk$p,fdr.adj.pval,pch=19,col=&quot;blue&quot;) legend(&quot;bottomright&quot;,legend=c(&quot;q-value&quot;,&quot;FDR (BH)&quot;,&quot;Bonferroni&quot;), fill=c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;)) FIGURE 3.10: Adjusted P-values via different methods and their relationship to raw P-values 3.2.4 moderated t-tests: using information from multiple comparisons In genomics, we usually do not do one test but many, as described above. That means we may be able to use the information from the parameters obtained from all comparisons to influence the individual parameters. For example, if you have many variances calculated for thousands of genes across samples, you can force individual variance estimates to shrunk towards the mean or the median of the distribution of variances. This usually creates better performance in individual variance estimates and therefore better performance in significance testing which depends on variance estimates. How much the values be shrunk towards a common value comes in many flavors. These tests in general are called moderated t-tests or shrinkage t-tests. One approach popularized by Limma software is to use so-called “Empirical Bayesian methods”. The main formulation in these methods is \\(\\hat{V_g} = aV_0 + bV_g\\), where \\(V_0\\) is the background variability and \\(V_g\\) is the individual variability. Then, these methods estimate \\(a\\) and \\(b\\) in various ways to come up with shrunk version of variability, \\(\\hat{V_g}\\). In a Bayesian viewpoint, the prior knowledge is used to calculate the variability of an individual gene. In this case, \\(V_0\\) would be the prior knowledge we have on variability of the genes and we use that knowledge to influence our estimate for the individual genes. Below we are simulating a gene expression matrix with 1000 genes, and 3 test and 3 control groups. Each row is a gene and in normal circumstances we would like to find out differentially expressed genes. In this case, we are simulating them from the same distribution so in reality we do not expect any differences. We then use the adjusted standard error estimates in empirical Bayesian spirit but in a very crude way. We just shrink the gene-wise standard error estimates towards the median with equal \\(a\\) and \\(b\\) weights. That is to say, we add individual estimate to the median of standard error distribution from all genes and divide that quantity by 2. So if we plug that in the to the above formula what we do is: \\[ \\hat{V_g} = (V_0 + V_g)/2 \\] In the code below, we are avoiding for loops or apply family functions by using vectorized operations. set.seed(100) #sample data matrix from normal distribution gset=rnorm(3000,mean=200,sd=70) data=matrix(gset,ncol=6) # set groups group1=1:3 group2=4:6 n1=3 n2=3 dx=rowMeans(data[,group1])-rowMeans(data[,group2]) require(matrixStats) # get the esimate of pooled variance stderr &lt;- sqrt( (rowVars(data[,group1])*(n1-1) + rowVars(data[,group2])*(n2-1)) / (n1+n2-2) * ( 1/n1 + 1/n2 )) # do the shrinking towards median mod.stderr &lt;- (stderr + median(stderr)) / 2 # moderation in variation # esimate t statistic with moderated variance t.mod = dx / mod.stderr # calculate P-value of rejecting null p.mod = 2*pt( -abs(t.mod), n1+n2-2 ) # esimate t statistic without moderated variance t = dx / stderr # calculate P-value of rejecting null p = 2*pt( -abs(t), n1+n2-2 ) par(mfrow=c(1,2)) hist(p,col=&quot;cornflowerblue&quot;,border=&quot;white&quot;,main=&quot;&quot;,xlab=&quot;P-values t-test&quot;) mtext(paste(&quot;signifcant tests:&quot;,sum(p&lt;0.05)) ) hist(p.mod,col=&quot;cornflowerblue&quot;,border=&quot;white&quot;,main=&quot;&quot;,xlab=&quot;P-values mod. t-test&quot;) mtext(paste(&quot;signifcant tests:&quot;,sum(p.mod&lt;0.05)) ) FIGURE 3.11: The distributions of P-values obtained by t-tests and moderated t-tests Want to know more ? basic statistical concepts “Cartoon guide to statistics” by Gonick &amp; Smith “Introduction to statistics” by Mine Rundel, et al. (Free e-book) Hands-on statistics recipes with R “The R book” by Crawley moderated tests comparison of moderated tests for differential expression http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-17 limma method: Smyth, G. K. (2004). Linear models and empirical Bayes methods for assessing differential expression in microarray experiments. Statistical Applications in Genetics and Molecular Biology, 3, No. 1, Article 3. http://www.statsci.org/smyth/pubs/ebayes.pdf "],
["relationship-between-variables-linear-models-and-correlation.html", "3.3 Relationship between variables: linear models and correlation", " 3.3 Relationship between variables: linear models and correlation In genomics, we would often need to measure or model the relationship between variables. We might want to know about expression of a particular gene in liver in relation to the dosage of a drug that patient receives. Or, we may want to know DNA methylation of certain locus in the genome in relation to age of the sample donor’s. Or, we might be interested in the relationship between histone modifications and gene expression. Is there a linear relationship, the more histone modification the more the gene is expressed ? In these situations and many more, linear regression or linear models can be used to model the relationship with a “dependent” or “response” variable (expression or methylation in the above examples) and one or more “independent”&quot; or “explanatory” variables (age, drug dosage or histone modification in the above examples). Our simple linear model has the following components. \\[ Y= \\beta_0+\\beta_1X + \\epsilon \\] In the equation above, \\(Y\\) is the response variable and \\(X\\) is the explanatory variable. \\(\\epsilon\\) is the mean-zero error term. Since, the line fit will not be able to precisely predict the \\(Y\\) values, there will be some error associated with each prediction when we compare it to the original \\(Y\\) values. This error is captured in \\(\\epsilon\\) term. We can alternatively write the model as follows to emphasize that the model approximates \\(Y\\), in this case notice that we removed the \\(\\epsilon\\) term: \\(Y \\sim \\beta_0+\\beta_1X\\) The graph below shows the relationship between histone modification (trimethylated forms of histone H3 at lysine 4, aka H3K4me3) and gene expression for 100 genes. The blue line is our model with estimated coefficients (\\(\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1X\\), where \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) the estimated values of \\(\\beta_0\\) and \\(\\beta_1\\), and \\(\\hat{y}\\) indicates the prediction). The red lines indicate the individual errors per data point, indicated as \\(\\epsilon\\) in the formula above. FIGURE 3.12: Relationship between histone modification score and gene expression. Increasing histone modification, H3K4me3, seems to be associated with increasing gene expression. Each dot is a gene There could be more than one explanatory variable, we then simply add more \\(X\\) and \\(\\beta\\) to our model. If there are two explanatory variables our model will look like this: \\[ Y= \\beta_0+\\beta_1X_1 +\\beta_2X_2 + \\epsilon \\] In this case, we will be fitting a plane rather than a line. However, the fitting process which we will describe in the later sections will not change. For our gene expression problem. We can introduce one more histone modification, H3K27me3. We will then have a linear model with 2 explanatory variables and the fitted plane will look like the one below. The gene expression values are shown as dots below and above the fitted plane. FIGURE 3.13: Association of Gene expression with H3K4me3 and H27Kme3 histone modifications. 3.3.0.1 Matrix notation for linear models We can naturally have more explanatory variables than just two.The formula below has \\(n\\) explanatory variables. \\[Y= \\beta_0+\\beta_1X_1+\\beta_2X_2 + \\beta_3X_3 + .. + \\beta_nX_n +\\epsilon\\] If there are many variables, it would be easier to write the model in matrix notation. The matrix form of linear model with two explanatory variables will look like the one below. First matrix would be our data matrix. This contains our explanatory variables and a column of 1s. The second term is a column vector of \\(\\beta\\) values. We add a vector of error terms,\\(\\epsilon\\)s to the matrix multiplication. \\[ \\mathbf{Y} = \\left[\\begin{array}{rrr} 1 &amp; X_{1,1} &amp; X_{1,2} \\\\ 1 &amp; X_{2,1} &amp; X_{2,2} \\\\ 1 &amp; X_{3,1} &amp; X_{3,2} \\\\ 1 &amp; X_{4,1} &amp; X_{4,2} \\end{array}\\right] % \\left[\\begin{array}{rrr} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{array}\\right] % + \\left[\\begin{array}{rrr} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_0 \\end{array}\\right] \\] The multiplication of data matrix and \\(\\beta\\) vector and addition of the error terms simply results in the the following set of equations per data point: \\[ \\begin{aligned} Y_1= \\beta_0+\\beta_1X_{1,1}+\\beta_2X_{1,2} +\\epsilon_1 \\\\ Y_2= \\beta_0+\\beta_1X_{2,1}+\\beta_2X_{2,2} +\\epsilon_2 \\\\ Y_3= \\beta_0+\\beta_1X_{3,1}+\\beta_2X_{3,2} +\\epsilon_3 \\\\ Y_4= \\beta_0+\\beta_1X_{4,1}+\\beta_2X_{4,2} +\\epsilon_4 \\end{aligned} \\] This expression involving the multiplication of the data matrix, the \\(\\beta\\) vector and vector of error terms (\\(\\epsilon\\)) could be simply written as follows. \\[Y=X\\beta + \\epsilon\\] In the equation above \\(Y\\) is the vector of response variables and \\(X\\) is the data matrix and \\(\\beta\\) is the vector of coefficients. This notation is more concise and often used in scientific papers. However, this also means you need some understanding of linear algebra to follow the math laid out in such resources. 3.3.1 How to fit a line At this point a major questions is left unanswered: How did we fit this line? We basically need to define \\(\\beta\\) values in a structured way. There are multiple ways or understanding how to do this, all of which converges to the same end point. We will describe them one by one. 3.3.1.1 The cost or loss function approach This is the first approach and in my opinion is easiest to understand. We try to optimize a function, often called “cost function” or “loss function”. The cost function is the sum of squared differences between the predicted \\(\\hat{Y}\\) values from our model and the original \\(Y\\) values. The optimization procedure tries to find \\(\\beta\\) values that minimizes this difference between reality and the predicted values. \\[min \\sum{(y_i-(\\beta_0+\\beta_1x_i))^2}\\] Note that this is related to the the error term, \\(\\epsilon\\), we already mentioned above, we are trying to minimize the squared sum of \\(\\epsilon_i\\) for each data point. We can do this minimization by a bit of calculus. The rough algorithm is as follows: Pick a random starting point, random \\(\\beta\\) values Take the partial derivatives of the cost function to see which direction is the way to go in the cost function. Take a step toward the direction that minimizes the cost function. step size is parameter to choose, there are many variants. repeat step 2,3 until convergence. This is the basis of “gradient descent” algorithm. With the help of partial derivatives we define a “gradient” on the cost function and follow that through multiple iterations and until convergence, meaning until the results do not improve defined by a margin. The algorithm usually converges to optimum \\(\\beta\\) values. Below, we show the cost function over various \\(\\beta_0\\) and \\(\\beta_1\\) values for the histone modification and gene expression data set. The algorithm will pick a point on this graph and traverse it incrementally based on the derivatives and converge on the bottom of the cost function “well”. FIGURE 3.14: Cost function landscape for linear regression with changing beta values. The optimization process tries to find the lowest point in this landscape by implementing a strategy for updating beta values towards the lowest point in the landscape. 3.3.1.2 Not cost function but maximum likelihood function We can also think of this problem from more a statistical point of view. In essence, we are looking for best statistical parameters, in this case \\(\\beta\\) values, for our model that are most likely to produce such a scatter of data points given the explanatory variables.This is called “Maximum likelihood” approach. Probability of observing a \\(Y\\) value, given that the distribution of it on a given \\(X\\) value follows a normal distribution with mean \\(\\beta_0+\\beta_1x_i\\) and variance \\(s^2\\) , and is shown below. Note that this assumes variance is constant and \\(s^2=\\frac{\\sum{\\epsilon_i}}{n-2}\\) is an unbiased estimation for population variance, \\(\\sigma^2\\). \\[P(y_{i})=\\frac{1}{s\\sqrt{2\\pi} }e^{-\\frac{1}{2}\\left(\\frac{y_i-(\\beta_0 + \\beta_1x_i)}{s}\\right)^2}\\] Following from this, then the likelihood function ,shown as \\(L\\) below, for linear regression is multiplication of \\(P(y_{i})\\) for all data points. \\[L=P(y_1)P(y_2)P(y_3)..P(y_n)=\\prod\\limits_{i=1}^n{P_i}\\] This can be simplified to this by some algebra and taking logs (since it is easier to add than multiply) \\[ln(L) = -nln(s\\sqrt{2\\pi}) - \\frac{1}{2s^2} \\sum\\limits_{i=1}^n{(y_i-(\\beta_0 - \\beta_1x_i))^2} \\] As you can see, the right part of the function is the negative of the cost function defined above. If we wanted to optimize this function we would need to take derivative of the function with respect to \\(\\beta\\) parameters. That means we can ignore the first part since there is no \\(\\beta\\) terms there. This simply reduces to the negative of the cost function. Hence, this approach produces exactly the same result as the cost function approach. The difference is that we defined our problem within the domain of statistics. This particular function has still to be optimized. This can be done with some calculus without the need for an iterative approach. 3.3.1.3 Linear algebra and closed-form solution to linear regression The last approach we will describe is the minimization process using linear algebra. If you find this concept challenging, feel free to skip it but scientific publications and other books frequently use matrix notation and linear algebra to define and solve regression problems. In this case, we do not use an iterative approach. Instead, we will minimize cost function by explicitly taking its derivatives with respect to \\(\\beta\\)’s and setting them to zero. This is doable by employing linear algebra and matrix calculus. This approach is also called “ordinary least squares”. We will not show the whole derivation here but the following expression is what we are trying to minimize in matrix notation, this is basically a different notation of the same minimization problem defined above. Remember \\(\\epsilon_i=Y_i-(\\beta_0+\\beta_1x_i)\\) \\[ \\begin{aligned} \\sum\\epsilon_{i}^2=\\epsilon^T\\epsilon=(Y-{\\beta}{X})^T(Y-{\\beta}{X}) \\\\ =Y^T{Y}-2{\\beta}^T{Y}+{\\beta}^TX^TX{\\beta} \\end{aligned} \\] After rearranging the terms, we take the derivative of \\(\\epsilon^T\\epsilon\\) with respect to \\(\\beta\\), and equalize that to zero. We then arrive at the following for estimated \\(\\beta\\) values, \\(\\hat{\\beta}\\): \\[\\hat{\\beta}=(X^TX)^{-1}X^TY\\] This requires for you to calculate the inverse of the \\(X^TX\\) term, which could be slow for large matrices. Iterative approach over the cost function derivatives will be faster for larger problems. The linear algebra notation is something you will see in the papers or other resources often. If you input the data matrix X and solve the \\((X^TX)^{-1}\\) , you get the following values for \\(\\beta_0\\) and \\(\\beta_1\\) for simple regression . However, we should note that this simple linear regression case can easily be solved algebraically without the need for matrix operations. This can be done by taking the derivative of \\(\\sum{(y_i-(\\beta_0+\\beta_1x_i))^2}\\) with respect to \\(\\beta_1\\), rearranging the terms and equalizing the derivative to zero. \\[\\hat{\\beta_1}=\\frac{\\sum{(x_i-\\overline{X})(y_i-\\overline{Y})}}{ \\sum{(x_i-\\overline{X})^2} }\\] \\[\\hat{\\beta_0}=\\overline{Y}-\\hat{\\beta_1}\\overline{X}\\] 3.3.1.4 Fitting lines in R After all this theory, you will be surprised how easy it is to fit lines in R. This is achieved just by lm() command, stands for linear models. Let’s do this for a simulated data set and plot the fit. First step is to simulate the data, we will decide on \\(\\beta_0\\) and \\(\\beta_1\\) values. The we will decide on the variance parameter,\\(\\sigma\\) to be used in simulation of error terms, \\(\\epsilon\\). We will first find \\(Y\\) values, just using the linear equation \\(Y=\\beta0+\\beta_1X\\), for a set of \\(X\\) values. Then, we will add the error terms get our simulated values. # set random number seed, so that the random numbers from the text # is the same when you run the code. set.seed(32) # get 50 X values between 1 and 100 x = runif(50,1,100) # set b0,b1 and varience (sigma) b0 = 10 b1 = 2 sigma = 20 # simulate error terms from normal distribution eps = rnorm(50,0,sigma) # get y values from the linear equation and addition of error terms y = b0 + b1*x+ eps Now let us fit a line using lm() function. The function requires a formula, and optionally a data frame. We need the pass the following expression within the lm function, y~x, where y is the simulated \\(Y\\) values and x is the explanatory variables \\(X\\). We will then use abline() function to draw the fit. mod1=lm(y~x) # plot the data points plot(x,y,pch=20, ylab=&quot;Gene Expression&quot;,xlab=&quot;Histone modification score&quot;) # plot the linear fit abline(mod1,col=&quot;blue&quot;) FIGURE 3.15: Gene expression and histone modification score modelled by linear regression 3.3.2 How to estimate the error of the coefficients Since we are using a sample to estimate the coefficients they are not exact, with every random sample they will vary. Below, we are taking multiple samples from the population and fitting lines to each sample, with each sample the lines slightly change.We are overlaying the points and the lines for each sample on top of the other samples .When we take 200 samples and fit lines for each of them,the lines fit are variable. And, we get a normal-like distribution of \\(\\beta\\) values with a defined mean and standard deviation a, which is called standard error of the coefficients. FIGURE 3.16: Regression coefficients vary with every random sample. The figure illustrates the variability of regression coefficients when regression is done using a sample of data points. Histograms depict this variability for \\(b_0\\) and \\(b_1\\) coefficients. As usually we will not have access to the population to do repeated sampling, model fitting and estimation of the standard error for the coefficients. But there is statistical theory that helps us infer the population properties from the sample. When we assume that error terms have constant variance and mean zero , we can model the uncertainty in the regression coefficients, \\(\\beta\\)s. The estimates for standard errors of \\(\\beta\\)s for simple regression are as follows and shown without derivation. \\[ \\begin{aligned} s=RSE=\\sqrt{\\frac{\\sum{(y_i-(\\beta_0+\\beta_1x_i))^2}}{n-2} } =\\sqrt{\\frac{\\sum{\\epsilon^2}}{n-2} } \\\\ SE(\\hat{\\beta_1})=\\frac{s}{\\sqrt{\\sum{(x_i-\\overline{X})^2}}} \\\\ SE(\\hat{\\beta_0})=s\\sqrt{ \\frac{1}{n} + \\frac{\\overline{X}^2}{\\sum{(x_i-\\overline{X})^2} } } \\end{aligned} \\] Notice that that \\(SE(\\beta_1)\\) depends on the estimate of variance of residuals shown as \\(s\\) or Residual Standard Error (RSE). Notice alsos standard error depends on the spread of \\(X\\). If \\(X\\) values have more variation, the standard error will be lower. This intuitively makes sense since if the spread of the \\(X\\) is low, the regression line will be able to wiggle more compared to a regression line that is fit to the same number of points but covers a greater range on the X-axis. The standard error estimates can also be used to calculate confidence intervals and test hypotheses, since the following quantity called t-score approximately follows a t-distribution with \\(n-p\\) degrees of freedom, where \\(n\\) is the number of data points and \\(p\\) is the number of coefficients estimated. \\[ \\frac{\\hat{\\beta_i}-\\beta_test}{SE(\\hat{\\beta_i})}\\] Often, we would like to test the null hypothesis if a coefficient is equal to zero or not. For simple regression this could mean if there is a relationship between explanatory variable and response variable. We would calculate the t-score as follows \\(\\frac{\\hat{\\beta_i}-0}{SE(\\hat{\\beta_i})}\\), and compare it t-distribution with \\(d.f.=n-p\\) to get the p-value. We can also calculate the uncertainty of the regression coefficients using confidence intervals, the range of values that are likely to contain \\(\\beta_i\\). The 95% confidence interval for \\(\\hat{\\beta_i}\\) is \\(\\hat{\\beta_i}\\) ± \\(t_{0.975}SE(\\hat{\\beta_i})\\). \\(t_{0.975}\\) is the 97.5% percentile of the t-distribution with \\(d.f. = n – p\\). In R, summary() function will test all the coefficients for the null hypothesis \\(\\beta_i=0\\). The function takes the model output obtained from the lm() function. To demonstrate this, let us first get some data. The procedure below simulates data to be used in a regression setting and it is useful to examine what the linear model expect to model the data. Since we have the data, we can build our model and call the summary function. We will then use confint() function to get the confidence intervals on the coefficients and coef() function to pull out the estimated coefficients from the model. mod1=lm(y~x) summary(mod1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -77.11 -18.44 0.33 16.06 57.23 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.2454 6.2887 2.11 0.038 * ## x 0.4995 0.0513 9.74 4.5e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.8 on 98 degrees of freedom ## Multiple R-squared: 0.492, Adjusted R-squared: 0.486 ## F-statistic: 94.8 on 1 and 98 DF, p-value: 4.54e-16 # get confidence intervals confint(mod1) ## 2.5 % 97.5 % ## (Intercept) 0.7657 25.7251 ## x 0.3977 0.6014 # pull out coefficients from the model coef(mod1) ## (Intercept) x ## 13.2454 0.4995 The summary() function prints out an extensive list of values. The “Coefficients” section has the estimates, their standard error, t score and the p-value from the hypothesis test \\(H_0:\\beta_i=0\\). As you can see, the estimate we get for the coefficients and their standard errors are close to the ones we get from the repeatedly sampling and getting a distribution of coefficients. This is statistical inference at work, we can estimate the population properties within a certain error using just a sample. 3.3.3 Accuracy of the model If you have observed the table output by summary() function, you must have noticed there are some other outputs, such as “Residual standard error”, “Multiple R-squared” and “F-statistic”. These are metrics that are useful for assessing the accuracy of the model. We will explain them one by one. _ (RSE)_ simply is the square-root of the the sum of squared error terms, divided by degrees of freedom, \\(n-p\\), for simple linear regression case, \\(n-2\\). Sum of of the squares of the error terms is also called “Residual sum of squares”, RSS. So RSE is calculated as follows: \\[ s=RSE=\\sqrt{\\frac{\\sum{(y_i-\\hat{Y_i})^2 }}{n-p}}=\\sqrt{\\frac{RSS}{n-p}}\\] RSE is a way of assessing the model fit. The larger the RSE the worse the model is. However, this is an absolute measure in the units of \\(Y\\) and we have nothing to compare against. One idea is that we divide it by RSS of a simpler model for comparative purposes. That simpler model is in this case is the model with the intercept,\\(\\beta_0\\). A very bad model will have close zero coefficients for explanatory variables, and the RSS of that model will be close to the RSS of the model with only the intercept. In such a model intercept will be equal to \\(\\overline{Y}\\). As it turns out, RSS of the the model with just the intercept is called “Total Sum of Squares” or TSS. A good model will have a low \\(RSS/TSS\\). The metric \\(R^2\\) uses these quantities to calculate a score between 0 and 1, and closer to 1 the better the model. Here is how it is calculated: \\[R^2=1-\\frac{RSS}{TSS}=\\frac{TSS-RSS}{TSS}=1-\\frac{RSS}{TSS}\\] \\(TSS-RSS\\) part of the formula often referred to as “explained variability” in the model. The bottom part is for “total variability”. With this interpretation, higher the “explained variability” better the model. For simple linear regression with one explanatory variable, the square root of \\(R^2\\) is a quantity known as absolute value of the correlation coefficient, which can be calculated for any pair of variables, not only the response and the explanatory variables. Correlation is a general measure of linear relationship between two variables. One of the most popular flavors of correlation is the Pearson correlation coefficient. Formally, It is the covariance of X and Y divided by multiplication of standard deviations of X and Y. In R, it can be calculated with cor() function. \\[ r_{xy}=\\frac{cov(X,Y)}{\\sigma_x\\sigma_y} =\\frac{\\sum\\limits_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})} {\\sqrt{\\sum\\limits_{i=1}^n (x_i-\\bar{x})^2 \\sum\\limits_{i=1}^n (y_i-\\bar{y})^2}} \\] In the equation above, cov is the covariance, this is again a measure of how much two variables change together, like correlation. If two variables show similar behavior they will usually have positive covariance value, if they have opposite behavior, the covariance will have negative value. However, these values are boundless. A normalized way of looking at covariance is to divide covariance by the multiplication of standard errors of X and Y. This bounds the values to -1 and 1, and as mentioned above called Pearson correlation coefficient. The values that change in a similar manner will have a positive coefficient, the values that change in opposite manner will have negative coefficient, and pairs do not have a linear relationship will have 0 or near 0 correlation. In the figure below, we are showing \\(R^2\\), correlation coefficient and covariance for different scatter plots. FIGURE 3.17: Correlation and covariance for different scatter plots For simple linear regression, correlation can be used to asses the model. However, this becomes useless as a measure of general accuracy if the there are more than one explanatory variable as in multiple linear regression. In that case, \\(R^2\\) is a measure of accuracy for the model. Interestingly, square of the correlation of predicted values and original response variables (\\((cor(Y,\\hat{Y}))^2\\) ) equals to \\(R^2\\) for multiple linear regression. The last accuracy measure or the model fit in general we are going to explain is F-statistic. This is a quantity that depends on RSS and TSS again. It can also answer one important question that other metrics can not easily answer. That question is whether or not any of the explanatory variables have predictive value or in other words if all the explanatory variables are zero. We can write the null hypothesis as follows: \\[H_0: \\beta_1=\\beta_2=\\beta_3=...=\\beta_p=0 \\] where the alternative is: \\[H_1: \\text{at least one } \\beta_i \\neq 0 \\] Remember \\(TSS-RSS\\) is analogous to “explained variability” and the RSS is analogous to “unexplained variability”. For the F-statistic, we divide explained variance to unexplained variance. Explained variance is just the \\(TSS-RSS\\) divided by degrees of freedom, and unexplained variance is the RSE. The ratio will follow the F-distribution with two parameters, the degrees of freedom for the explained variance and the degrees of freedom for the the unexplained variance.F-statistic for a linear model is calculated as follows. \\[F=\\frac{(TSS-RSS)/(p-1)}{RSS/(n-p)}=\\frac{(TSS-RSS)/(p-1)}{RSE} \\sim F(p-1,n-p)\\] If the variances are the same, the ratio will be 1, and when \\(H_0\\) is true, then it can be shown that expected value of \\((TSS-RSS)/(p-1)\\) will be \\(\\sigma^2\\) which is estimated by RSE. So, if the variances are significantly different, the ratio will need to be significantly bigger than 1. If the ratio is large enough we can reject the null hypothesis. To asses that we need to use software or look up the tables for F statistics with calculated parameters. In R, function qf() can be used to calculate critical value of the ratio. Benefit of the F-test over looking at significance of coefficients one by one is that we circumvent multiple testing problem. If there are lots of explanatory variables at least 5% of the time (assuming we use 0.05 as P-value significance cutoff), p-values from coefficient t-tests will be wrong. In summary, F-test is a better choice for testing if there is any association between the explanatory variables and the response variable. 3.3.4 Regression with categorical variables An important feature of linear regression is that categorical variables can be used as explanatory variables, this feature is very useful in genomics where explanatory variables often could be categorical. To put it in context, in our histone modification example we can also include if promoters have CpG islands or not as a variable. In addition, in differential gene expression, we usually test the difference between different condition which can be encoded as categorical variables in a linear regression. We can sure use t-test for that as well if there are only 2 conditions, but if there are more conditions and other variables to control for such as Age or sex of the samples, we need to take those into account for our statistics, and t-test alone can not handle such complexity. In addition, when we have categorical variables we can also have numeric variables in the model and we certainly do not have to include only one type of variable in a model. The simplest model with categorical variables include two levels that can be encoded in 0 and 1. set.seed(100) gene1=rnorm(30,mean=4,sd=2) gene2=rnorm(30,mean=2,sd=2) gene.df=data.frame(exp=c(gene1,gene2), group=c( rep(1,30),rep(0,30) ) ) mod2=lm(exp~group,data=gene.df) summary(mod2) ## ## Call: ## lm(formula = exp ~ group, data = gene.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.729 -1.066 0.012 1.384 4.563 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.185 0.352 6.21 6e-08 *** ## group 1.873 0.497 3.77 0.00039 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.93 on 58 degrees of freedom ## Multiple R-squared: 0.196, Adjusted R-squared: 0.183 ## F-statistic: 14.2 on 1 and 58 DF, p-value: 0.000391 require(mosaic) plotModel(mod2) FIGURE 3.18: Linear model with a categorical variable coded as 0 and 1 we can even compare more levels, we do not even have to encode them ourselves. We can pass categorical variables to lm() function. gene.df=data.frame(exp=c(gene1,gene2,gene2), group=c( rep(&quot;A&quot;,30),rep(&quot;B&quot;,30),rep(&quot;C&quot;,30) ) ) mod3=lm(exp~group,data=gene.df) summary(mod3) ## ## Call: ## lm(formula = exp ~ group, data = gene.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.729 -1.079 -0.098 1.484 4.563 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.058 0.378 10.7 &lt; 2e-16 *** ## groupB -1.873 0.535 -3.5 0.00073 *** ## groupC -1.873 0.535 -3.5 0.00073 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.07 on 87 degrees of freedom ## Multiple R-squared: 0.158, Adjusted R-squared: 0.139 ## F-statistic: 8.17 on 2 and 87 DF, p-value: 0.000558 3.3.5 Regression pitfalls In most cases one should look at the error terms (residuals) vs fitted values plot. Any structure in this plot indicates problems such as non-linearity, correlation of error terms, non-constant variance or unusual values driving the fit. Below we briefly explain the potential issues with the linear regression. 3.3.5.0.1 non-linearity If the true relationship is far from linearity, prediction accuracy is reduced and all the other conclusions are questionable. In some cases, transforming the data with \\(logX\\), \\(\\sqrt{X}\\) and \\(X^2\\) could resolve the issue. 3.3.5.0.2 correlation of explanatory variables If the explanatory variables are correlated that could lead to something known as multicolinearity. When this happens SE estimates of the coefficients will be too large. This is usually observed in time-course data. 3.3.5.0.3 correlation of error terms This assumes that the errors of the response variables are uncorrelated with each other. If they are confidence intervals in the coefficients might too narrow. 3.3.5.0.4 Non-constant variance of error terms This means that different response variables have the same variance in their errors, regardless of the values of the predictor variables. If the errors are not constant, if for the errors grow as X grows this will result in unreliable estimates in standard errors as the model assumes constant variance. Transformation of data, such as \\(logX\\) and \\(\\sqrt{X}\\) could help in some cases. 3.3.5.0.5 outliers and high leverage points Outliers are extreme values for Y and high leverage points are unusual X values. Both of these extremes have power to affect the fitted line and the standard errors. In some cases (measurement error), they can be removed from the data for a better fit. Want to know more ? linear models and derivations of equations including matrix notation Applied Linear Statistical Models by Kutner, Nachtsheim, et al. Elements of statistical learning by Hastie &amp; Tibshirani An Introduction to statistical learning by James, Witten, et al. "],
["clustering-grouping-samples-based-on-their-similarity.html", "3.4 Clustering: grouping samples based on their similarity", " 3.4 Clustering: grouping samples based on their similarity In genomics, we would very frequently want to assess how our samples relate to each other. Are our replicates similar to each other? Do the samples from the same treatment group have the similar genome-wide signals ? Do the patients with similar diseases have similar gene expression profiles ? Take the last question for example. We need to define a distance or similarity metric between patients’ expression profiles and use that metric to find groups of patients that are more similar to each other than the rest of the patients. This, in essence, is the general idea behind clustering. We need a distance metric and a method to utilize that distance metric to find self-similar groups. Clustering is a ubiquitous procedure in bioinformatics as well as any field that deals with high-dimensional data. It is very likely every genomics paper containing multiple samples have some sort of clustering. Due to this ubiquity and general usefulness, it is an essential technique to learn. 3.4.1 Distance metrics The first required step for clustering is the distance metric. This is simply a measurement of how similar gene expressions to each other are. There are many options for distance metrics and the choice of the metric is quite important for clustering. Consider a simple example where we have four patients and expression of three genes measured. Which patients look similar to each other based on their gene expression profiles ? TABLE 3.1: Gene expressions from patients IRX4 OCT4 PAX6 patient1 11 10 1 patient2 13 13 3 patient3 2 4 10 patient4 1 3 9 It may not be obvious from the table at first sight but if we plot the gene expression profile for each patient, we will see that expression profiles of patient 1 and patient 2 is more similar to each other than patient 3 or patient 4. FIGURE 3.19: Gene expression values for different patients. Certain patients have similar gene expression values to each other. But how can we quantify what see by eye ? A simple metric for distance between gene expression vectors between a given patient pair is the sum of absolute difference between gene expression values This can be formulated as follows: \\(d_{AB}={\\sum _{i=1}^{n}|e_{Ai}-e_{Bi}|}\\), where \\(d_{AB}\\) is the distance between patient A and B, and \\(e_{Ai}\\) and \\(e_{Bi}\\) expression value of the \\(i\\)th gene for patient A and B. This distance metric is called “Manhattan distance” or “L1 norm”. Another distance metric using sum of squared distances and taking a square root of resulting value, that can be formulaized as: \\(d_{AB}={{\\sqrt {\\sum _{i=1}^{n}(e_{Ai}-e_{Bi})^{2}}}}\\). This distance is called “Euclidean Distance” or “L2 norm”. This is usually the default distance metric for many clustering algorithms. due to squaring operation values that are very different get higher contribution to the distance. Due to this, compared to Manhattan distance it can be more affected by outliers but generally if the outliers are rare this distance metric works well. The last metric we will introduce is the “correlation distance”. This is simply \\(d_{AB}=1-\\rho\\), where \\(\\rho\\) is the pearson correlation coefficient between two vectors, in our case those vectors are gene expression profiles of patients. Using this distance the gene expression vectors that have a similar pattern will have a small distance whereas when the vectors have different patterns they will have a large distance. In this case, the linear correlation between vectors matters, the the scale of the vectors might be different. Now let’s see how we can calculate these distance in R. First, we have our gene expression per patient table. df ## IRX4 OCT4 PAX6 ## patient1 11 10 1 ## patient2 13 13 3 ## patient3 2 4 10 ## patient4 1 3 9 Next, we calculate the distance metrics using dist function and 1-cor(). dist(df,method=&quot;manhattan&quot;) ## patient1 patient2 patient3 ## patient2 7 ## patient3 24 27 ## patient4 25 28 3 dist(df,method=&quot;euclidean&quot;) ## patient1 patient2 patient3 ## patient2 4.123 ## patient3 14.071 15.843 ## patient4 14.595 16.733 1.732 as.dist(1-cor(t(df))) ## patient1 patient2 patient3 ## patient2 0.004129 ## patient3 1.988522 1.970725 ## patient4 1.988522 1.970725 0.000000 3.4.1.1 Scaling before calculating the distance Before we proceed to the clustering, one more thing we need to take care. Should we normalize our data ? Scale of the vectors in our expression matrix can affect the distance calculation. Gene expression tables are usually have some sort of normalization, so the values are in comparable scales. But somehow if a gene’s expression values were on much higher scale than the other genes, that gene will effect the distance more than other when using Euclidean or Manhattan distance. If that is the case we can scale the variables.The traditional way of scaling variables is to subtract their mean, and divide by their standard deviation, this operation is also called “standardization”. If this is done on all genes, each gene will have the same affect on distance measures. The decision to apply scaling ultimately depends on our data and what you want to achieve. If the gene expression values are previously normalized between patients, having genes that dominate the distance metric could have a biological meaning and therefore it may not be desireable to further scale variables. In R, the standardization is done via scale() function. Here we scale the gene expression values. df ## IRX4 OCT4 PAX6 ## patient1 11 10 1 ## patient2 13 13 3 ## patient3 2 4 10 ## patient4 1 3 9 scale(df) ## IRX4 OCT4 PAX6 ## patient1 0.6933 0.5213 -1.0734 ## patient2 1.0195 1.1468 -0.6214 ## patient3 -0.7748 -0.7298 0.9604 ## patient4 -0.9379 -0.9383 0.7344 ## attr(,&quot;scaled:center&quot;) ## IRX4 OCT4 PAX6 ## 6.75 7.50 5.75 ## attr(,&quot;scaled:scale&quot;) ## IRX4 OCT4 PAX6 ## 6.131 4.796 4.425 3.4.2 Hiearchical clustering This is one of the most ubiqutous clustering algorithms. Using this algorithm you can see the relationship of individual data points and relationships of clusters. This is achieved successively joining small clusters to each other based on the intercluster distance. Eventually, you get a tree structure or a dendrogram that shows the relationship between the individual data points and clusters. The height of the dendrogram is the distance between clusters. Here we can show how to use this on our toy data set from four patients. The base function in R to do hierarchical clustering in hclust(). Below, we apply that function on Euclidean distances between patients. d=dist(df) hc=hclust(d,method=&quot;complete&quot;) plot(hc) FIGURE 3.20: Dendrogram of distance matrix In the above code snippet, we have used method=&quot;complete&quot; argument without explaining it. The method argument defines the criteria that directs how the sub-clusters are merged. During clustering starting with single-member clusters, the clusters are merged based on the distance between them. There are many different ways to define distance between clusters and based on which definition you use the hierarchical clustering results change. So the method argument controls that. There are a couple of values this argument can take, we list them and their description below: “complete” stands for “Complete Linkage” and the distance between two clusters is defined as largest distance between any members of the two clusters. “single” stands for “Single Linkage” and the distance between two clusters is defined as smallest distance between any members of the two clusters. “average” stands for “Average Linkage” or more precisely UPGMA (Unweighted Pair Group Method with Arithmetic Mean) method. In this case, the distance between two clusters is defined as average distance between any members of the two clusters. “ward.D2” and “ward.D” stands for different implementations of Ward’s minimum variance method. This method aims to find compact, spherical clusters by selecting clusters to merge based on the change in the cluster variances. The clusters are merged if the increase in the combined variance over the sum of the cluster specific variances is minimum compared to alternative merging operations. In real life, we would get expression profiles from thousands of genes and we will typically have many more patients than our toy example. One such data set is gene expression values from 60 bone marrow samples of patients with one of the four main types of leukemia (ALL, AML, CLL, CML) or no-leukemia controls. We trimmed that data set down to top 1000 most variable genes to be able to work with it easier and in addition genes that are not very variable do not contribute much to the distances between patients. We will now use this data set to cluster the patients and display the values as a heatmap and a dendrogram. The heatmap shows the expression values of genes across patients in a color coded manner. The heatmap function, pheatmap(), we will use performs the clustering as well. The matrix that contains gene expressions has the genes in the rows and the patients in the columns. Therefore, we will also use a column-side color code to mark the patients based on their leukemia type. For the hierarchical clustering, we will use Ward’s method designated by clustering_method argument to pheatmap() function. library(pheatmap) expFile=system.file(&quot;extdata&quot;,&quot;leukemiaExpressionSubset.rds&quot;,package=&quot;compGenomRData&quot;) mat=readRDS(expFile) # set the leukemia type annotation for each sample annotation_col = data.frame( LeukemiaType =substr(colnames(mat),1,3)) rownames(annotation_col)=colnames(mat) pheatmap(mat,show_rownames=FALSE,show_colnames=FALSE,annotation_col=annotation_col,scale = &quot;none&quot;,clustering_method=&quot;ward.D2&quot;,clustering_distance_cols=&quot;euclidean&quot;) As we can observe in the heatmap each cluster has a distinct set of expression values. The main clusters almost perfectly distinguish the leukemia types. Only one CML patient is clustered as a non-leukemia sample. This could mean that gene expression profiles are enough to classify leukemia type. More detailed analysis and experiments are needed to verify that but by looking at this exploratory analysis we can decide where to focus our efforts next. 3.4.2.1 where to cut the tree ? The example above seems like a clear cut example where we can pick by eye clusters from the dendrogram. This is mostly due to the Ward’s method where compact clusters are preffered. However, as it is usually the case we do not have patient labels and it would be difficult to tell which leaves (patients) in the dendrogram we should consider as part of the same cluster. In other words, how deep we should cut the dendrogram so that every patient sample still connected via the remaining sub-dendrograms constitute clusters. The cutree() function provides the functionality to output either desired number of clusters or clusters obtained from cutting the dendrogram at a certain height. Below, we will cluster the patients with hierarchical clustering using the default method “complete linkage” and cut the dendrogram at a certain height. In this case, you will also observe that, changing from Ward’s distance to complete linkage had an effect on clustering. Now two clusters that are defined by Ward’s distance are closer to each other and harder to separate from each other. hcl=hclust(dist(t(mat))) plot(hcl,labels = FALSE, hang= -1) rect.hclust(hcl, h = 80, border = &quot;red&quot;) clu.k5=cutree(hcl,k=5) # cut tree so that there are 4 clusters clu.h80=cutree(hcl,h=80) # cut tree/dendrogram from height 80 table(clu.k5) # number of samples for each cluster ## clu.k5 ## 1 2 3 4 5 ## 12 3 9 12 24 Apart from the arbitrary values for the height or the number of the clusters, how can we define clusters more systematically? As this is a general question, we will show later how to decide the optimal number of clusters later in this chapter. 3.4.3 K-means clustering Another, very common clustering algorithm is k-means.This method divides or partitions the data points, our working example patients, into a pre-determined, “k” number of clusters. Hence, this type of methods are generally called “partioning” methods. The algorithm is initialized with randomly choosen \\(k\\) centers or centroids. In a sense, a centroid is a data point with multiple values. In our working example, it is a hypothetical patient with gene expression values. But in the initialization phase, those gene expression values are choosen randomly within the boundaries of the gene expression distributions from real patients. As the next step in the algorithm, each patient is assigned to the closest centroid and in the next iteration centroids are set to the mean of values of the genes in the cluster. This process of setting centroids and assigning patients to the clusters repeats itself until sum of squared distances to cluster centroids is minimized. As you might see, the cluster algorithm starts with random initial centroids. This feature might yield different results for each run of the algorithm. We will know show how to use k-means method on the gene expression data set. We will use set.seed() for reproducbility. In the wild, you might want to run this algorithm multiple times to see if your clustering results are stable. set.seed(101) # we have to transpore the matrix t() # so that we calculate distances between patients kclu=kmeans(t(mat),centers=5) # number of data points in each cluster table(kclu$cluster) ## ## 1 2 3 4 5 ## 12 12 14 11 11 Now let us check the percentage of each leukemia type in each cluster. We can visualize this as a table. Looking at the table below, we see that each of the 5 clusters are predominantly representing one of the 4 leukemia types or the control patients without leukemia. type2kclu = data.frame( LeukemiaType =substr(colnames(mat),1,3), cluster=kclu$cluster) table(type2kclu) ## cluster ## LeukemiaType 1 2 3 4 5 ## ALL 12 0 0 0 0 ## AML 0 0 1 0 11 ## CLL 0 12 0 0 0 ## CML 0 0 1 11 0 ## NoL 0 0 12 0 0 Another related and maybe more robust algorithm is called “k-medoids” clustering. The procedure is almost identical to k-means clustering with a couple of differences. In this case, centroids choosen are real data points in our case patients, and the metric we are trying to optimize in each iteration is based on manhattan distance to the centroid. In k-means this was based on sum of squared distances so euclidean distance. Below we are showing how to use k-medoids clustering function pam() from the cluster package. kmclu=cluster::pam(t(mat),k=5) # cluster using k-medoids # make a data frame with Leukemia type and cluster id type2kmclu = data.frame( LeukemiaType =substr(colnames(mat),1,3), cluster=kmclu$cluster) table(type2kmclu) ## cluster ## LeukemiaType 1 2 3 4 5 ## ALL 12 0 0 0 0 ## AML 0 10 1 1 0 ## CLL 0 0 0 0 12 ## CML 0 0 0 12 0 ## NoL 0 0 12 0 0 We can not visualize the clustering from partioning methods with a tree like we did for hierarchical clustering. Even if we can get the distances between patients the algorithm does not return the distances between clusters out of the box. However, if we had a way to visualize the distances between patients in 2 dimensions we could see the how patients and clusters relate each other. It turns out, that there is a way to compress between patient distances to a 2-dimensional plot. There are many ways to do this and we introduce these dimension reduction methods including the one we will use now later in this chapter. For now, we are going to use a method called “multi-dimensional scaling” and plot the patients in a 2D plot color coded by their cluster assignments. # Calculate distances dists=dist(t(mat)) # calculate MDS mds=cmdscale(dists) # plot the patients in the 2D space plot(mds,pch=19,col=rainbow(5)[kclu$cluster]) # set the legend for cluster colors legend(&quot;bottomright&quot;, legend=paste(&quot;clu&quot;,unique(kclu$cluster)), fill=rainbow(5)[unique(kclu$cluster)], border=NA,box.col=NA) The plot we obtained shows the separetion between clusters. However, it does not do a great job showing the separation between cluster 3 and 4, which represent CML and “no leukemia” patients. We might need another dimension to properly visualize that separation. In addition, those two clusters were closely related in the hierarhical clustering as well. 3.4.4 how to choose “k”, the number of clusters Up to this point, we have avoided the question of selecting optimal number clusters. How do we know where to cut our dendrogram or which k to choose ? First of all, this is a difficult question. Usually, clusters have different granuality. Some clusters are tight and compact and some are wide,and both these types of clusters can be in the same data set. When visualized, some large clusters may look like they may have sub-clusters. So should we consider the large cluster as one cluster or should we consider the sub-clusters as individual clusters ? There are some metrics to help but there is no definite answer. We will show a couple of them below. 3.4.4.1 Silhouhette One way to determine how well the clustering is to measure the expected self-similar nature of the points in a set of clusters. The silhouette value does just that and it is a measure of how similar a data point is to its own cluster compared to other clusters. The silhouette value ranges from -1 to +1, where values that are positive indicates that the data point is well matched to its own cluster, if the value is zero it is a borderline case and if the value is minus it means that the data point might be mis-clustered because it is more simialr to a neighboring cluster. If most data points have a high value, then the clustering is appropriate. Ideally, one can create many different clusterings with different parameters such as \\(k\\),number of clusters and assess their appropriateness using the average silhouette values. In R, silhouette values are refered to as silhouette widths in the documentation. A silhouette value is calculated for each data point. In our working example, each patient will get silhouette values showing how well they are matched to their assigned clusters. Formally this calculated as follows. For each data point \\(i\\), we calculate \\({\\displaystyle a(i)}\\), which denotes the average distance between \\(i\\) and all other data points within the same cluster. This shows how well the point fits into that cluster. For the same data point, we also calculate \\({\\displaystyle b(i)}\\) b(i) denotes the lowest average distance of \\({\\displaystyle i}\\) to all points in any other cluster, of which \\({\\displaystyle i}\\) is not a member. The cluster with this lowest average \\(b(i)\\) is the “neighbouring cluster” of data point \\({\\displaystyle i}\\) since it is the next best fit cluster for that data point. Then, the silhouette value for a given data point is: \\(s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i),b(i)\\}}\\) As described, this quantity is positive when \\(b(i)\\) is high and \\(a(i)\\) is low, meaning that the data point \\(i\\) is self-similar to its cluster. And the silhouette value, \\(s(i)\\), is negative if it is more similar to its neighbours than its assigned cluster. In R, we can calculate silhouette values using cluster::silhouette() function. Below, we calculate the silhouette values for k-medoids clustering with pam() function with k=5. library(cluster) set.seed(101) pamclu=cluster::pam(t(mat),k=5) plot(silhouette(pamclu),main=NULL) Now, let us calculate average silhouette value different \\(k\\) values and compare. We will use sapply() function to get average silhouette values accross \\(k\\) values between 2 and 7. Within sapply() there is an anonymous function that that does the clustering and calculates average silhouette values for each \\(k\\). Ks=sapply(2:7, function(i) summary(silhouette(pam(t(mat),k=i)))$avg.width) plot(2:7,Ks,xlab=&quot;k&quot;,ylab=&quot;av. silhouette&quot;,type=&quot;b&quot;, pch=19) In this case, it seems the best value for \\(k\\) is 4. The k-medoids function pam() will usually cluster CML and noLeukemia cases together when k=4, which are also related clusters according to hierarchical clustering we did earlier. 3.4.4.2 Gap statistic As clustering aims to find self-similar data points, it would be reasonable to expect with the correct number of clusters the total within-cluster variation is minimized. Within-cluster variation for a single cluster can simply be defined as sum of squares from the cluster mean, which in this case is the centroid we defined in k-means algorithm. The total within-cluster variation is then sum of within-cluster variations for each cluster. This can be formally defined as follows: \\(\\displaystyle W_k = \\sum_{k=1}^K \\sum_{\\mathrm{x}_i \\in C_k} (\\mathrm{x}_i - \\mu_k )^2\\) Where \\(\\mathrm{x}_i\\) is data point in cluster \\(k\\), and \\(\\mu_k\\) is the cluster mean, and \\(W_k\\) is the total within-cluster variation quantity we described. However, the problem is that the variation quantity decreases with number of clusters. The more centroids we have, the smaller the distances to the centroids get. A more reliable approach would be somehow calculating the expected variation from a reference null distribution and compare that to the observed variation for each \\(k\\). In gap statistic approach, the expected distribution is calculated via sampling points from the boundaries of the original data and calculating within-cluster variation quantity for multiple rounds of sampling. This way we have an expectation how about the variability when there is no expected clustering, and then compare that expected variation to the observed within-cluster variation. The expected variation should also go down with increasing number of clusters, but for the optimal number of clusters the expected variation will be furthest away from observed variation. This distance is called the “gap statistic” and defined as follows: \\(\\displaystyle \\mathrm{Gap}_n(k) = E_n^*\\{\\log W_k\\} - \\log W_k\\), where \\(E_n^*\\{\\log W_k\\}\\) is the expected variation in log-scale under a sample size \\(n\\) from the reference distribution and \\(\\log W_k\\) is the observed variation. Our aim is choose the \\(k\\), number of clusters, that maximizes \\(\\mathrm{Gap}_n(k)\\). We can easily calculate the gap statistic with cluster::clusGap() function. We will now use that function to calculate the gap statistic for our patient gene expression data. library(cluster) set.seed(101) # define the clustering function pam1 &lt;- function(x,k) list(cluster = pam(x,k, cluster.only=TRUE)) # calculate the gap statistic pam.gap= clusGap(t(mat), FUN = pam1, K.max = 8,B=50) # plot the gap statistic accross k values plot(pam.gap, main = &quot;Gap statistic for the &#39;Leukemia&#39; data&quot;) In this case, gap statistic shows \\(k=7\\) is the best. However, after \\(K=6\\) the statistic has more or less a stable curve. In this case, we know that there are 5 main patient categories but this does not mean there is no sub-categories or sub-types for the cancers we are looking at. https://statweb.stanford.edu/~gwalther/gap 3.4.4.3 Other methods There are several other methods that provide insight into how many clusters. In fact, the package NbClust provides 30 different ways to determine the number of optimal clusters and can offer a voting mechanism to pick the best number. Below, we are showing how to use this function for some of the optimal number of cluster detection methods. library(NbClust) nb = NbClust(data=t(mat), distance = &quot;euclidean&quot;, min.nc = 2, max.nc = 7, method = &quot;kmeans&quot;, index=c(&quot;kl&quot;,&quot;ch&quot;,&quot;cindex&quot;,&quot;db&quot;,&quot;silhouette&quot;, &quot;duda&quot;,&quot;pseudot2&quot;,&quot;beale&quot;,&quot;ratkowsky&quot;, &quot;gap&quot;,&quot;gamma&quot;,&quot;mcclain&quot;,&quot;gplus&quot;, &quot;tau&quot;,&quot;sdindex&quot;,&quot;sdbw&quot;)) table(nb$Best.nc[1,]) # consensus seems to be 3 clusters However, the readers should keep in mind that clustering is an exploratory technique. If you have solid labels for your data points maybe clustering is just a sanity check, and you should just do predictive modeling instead. However, in biology there are rarely solid labels and things have different granularity. Take the leukemia patients case we have been using for example, it is know that leukemia types have subtypes and those sub-types that have different mutation profiles and consequently have different molecular signatures. Because of this, it is not surprising that some optimal cluster number techniques will find more clusters to be appropriate. On the other hand, CML (Chronic myeloid leukemia ) is a slow progressing disease and maybe as molecular signatures goes could be the closest to no leukemia patients, clustering algorithms may confuse the two depending on what granuality they are operating with. It is always good to look at the heatmaps after clustering, if you have meaningful self-similar data points even if the labels you have do not agree that there can be different clusters you can perform downstream analysis to understand the sub-clusters better. As we have seen, we can estimate optimal number of clusters but we can not take that estimation as the absolute truth, given more data points or different set of expression signatures you may have different optimal clusterings, or the supposed optimal clustering might overlook previously known sub-groups of your data. "],
["dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html", "3.5 Dimensionality reduction techniques: visualizing complex data sets in 2D", " 3.5 Dimensionality reduction techniques: visualizing complex data sets in 2D In statistics, dimension reduction techniques are a set of processes for reducing the number of random variables by obtaining a set of principal variables. For example, in the context of a gene expression matrix accross different patient samples, this might mean getting a set of new variables that cover the variation in sets of genes. This way samples can be represented by a couple of principal variables instead of thousands of genes. This is useful for visualization, clustering and predictive modeling. 3.5.1 Principal component analysis Principal component analysis (PCA) is maybe the most popular technique to examine high-dimensional data. There are multiple interpretations of how PCA reduces dimensionality. We will first focus on geometrical interpretation, where this operation can be interpreted as rotating the orignal dimensions of the data. For this, we go back to our example gene expression data set. In this example, we will represent our patients with expression profiles of just two genes, CD33 (ENSG00000105383) and PYGL (ENSG00000100504) genes. This way we can visualize them in a scatterplot. plot(mat[rownames(mat)==&quot;ENSG00000100504&quot;,], mat[rownames(mat)==&quot;ENSG00000105383&quot;,],pch=19, ylab=&quot;CD33 (ENSG00000105383)&quot;, xlab=&quot;PYGL (ENSG00000100504)&quot;) PCA rotates the original data space such that the axes of the new coordinate system point into the directions of highest variance of the data. The axes or new variables are termed principal components (PCs) and are ordered by variance: The first component, PC 1, represents the direction of the highest variance of the data. The direction of the second component, PC 2, represents the highest of the remaining variance orthogonal to the first component. This can be naturally extended to obtain the required number of components which together span a component space covering the desired amount of variance. In our toy example with only two genes, the principal componets are drawn over the original scatter plot and in the next plot we show the new coordinate system based on the pricinpal components. We will calculate the PCA with the princomp() function, this function returns the new coordinates as well. These new coordinates are simply a projection of data over the new coordinates. We will decorate the scatter plots with eigenvectors showing the direction of greatest variation. Then, we will plot the new coordinates. These are automatically calculated by princomp() function. Notice that we are using the scale() function when plotting coordinates and also before calculating PCA. This function centers the data, meaning substracts the mean of the each column vector from the elements in the vector. This essentially gives the columns a zero mean. It also divides the data by the standard deviation of the centered columns. These two operations helps bring the data to a common scale which is important for PCA not to be affected by different scales in the data. par(mfrow=c(1,2)) # create the subset of the data with two genes only # notice that we transpose the matrix so samples are # on the columns sub.mat=t(mat[rownames(mat) %in% c(&quot;ENSG00000100504&quot;,&quot;ENSG00000105383&quot;),]) # ploting our genes of interest as scatter plots plot(scale(mat[rownames(mat)==&quot;ENSG00000100504&quot;,]), scale(mat[rownames(mat)==&quot;ENSG00000105383&quot;,]), pch=19, ylab=&quot;CD33 (ENSG00000105383)&quot;, xlab=&quot;PYGL (ENSG00000100504)&quot;, col=annotation_col$LeukemiaType, xlim=c(-2,2),ylim=c(-2,2)) # create the legend for the Leukemia types legend(&quot;bottomright&quot;, legend=unique(annotation_col$LeukemiaType), fill =palette(&quot;default&quot;), border=NA,box.col=NA) # calculate the PCA only for our genes and all the samples pr=princomp(scale(sub.mat)) # plot the direction of eigenvectors # pr$loadings returned by princomp has the eigenvectors arrows(x0=0, y0=0, x1 = pr$loadings[1,1], y1 = pr$loadings[2,1],col=&quot;pink&quot;,lwd=3) arrows(x0=0, y0=0, x1 = pr$loadings[1,2], y1 = pr$loadings[2,2],col=&quot;gray&quot;,lwd=3) # plot the samples in the new coordinate system plot(-pr$scores,pch=19, col=annotation_col$LeukemiaType, ylim=c(-2,2),xlim=c(-4,4)) # plot the new coordinate basis vectors arrows(x0=0, y0=0, x1 =-2, y1 = 0,col=&quot;pink&quot;,lwd=3) arrows(x0=0, y0=0, x1 = 0, y1 = -1,col=&quot;gray&quot;,lwd=3) FIGURE 3.21: Geometric interpretation of PCA finding eigenvectors that point to direction of highest variance. Eigenvectors can be used as a new coordinate system. As you can see, the new coordinate system is useful by itself.The X-axis which represents the first component separates the data along the lympoblastic and myeloid leukemias. PCA in this case is obtained by calculating eigenvectors of the covariance matrix via an operation called eigen decomposition. Covariance matrix is obtained by covariance of pairwise variables of our expression matrix, which is simply \\({ \\operatorname{cov} (X,Y)={\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-\\mu_X)(y_{i}-\\mu_Y)}\\), where \\(X\\) and \\(Y\\) expression values of genes in a sample in our example. This is a measure of how things vary together, if high expressed genes in sample A are also highly expressed in sample B and lowly expressed in sample A are also lowly expressed in sample B, then sample A and B will have positive covariance. If the opposite is true then they will have negative covariance. This quantity is related to correlation and in fact correlation is standardized covariance. Covariance of variables can be obtained with cov() function, and eigen decomposition of such a matrix will produce a set of ortahogonal vectors that span the directions of highest variation. In 2D, you can think of this operation as rotating two perpendicular lines together until they point to the directions where most of the variation in the data lies on, similar to the figure 3.21. An important intuition is that, after the rotation prescribed by eigenvectors is complete the covariance between variables in this rotated dataset will be zero. There is a proper mathematical relationship between covariances of the rotated dataset and the original dataset. That’s why operating on covariance matrix is related to the rotation of the original dataset. cov.mat=cov(sub.mat) # calculate covariance matrix cov.mat eigen(cov.mat) # obtain eigen decomposition for eigen values and vectors Eigenvectors and eigenvalues of the covariance matrix indicates the direction and the magnitute of variation of the data. In our visual example the eigenvectors are so-called principal components. The eigenvector indicates the direction and the eigen values indicate the variation in that direction. Eigenvectors and values exist in pairs: every eigenvector has a corresponding eigenvalue and the eigenvectors are linearly independent from each other, this means they are orthogonal or uncorrelated in the our working example above. The eigenvectors are ranked by their corresponding eigen value, the higher the eigen value the more important the eigenvector is, because it explains more of the variation compared to the other eigenvectors. This feature of PCA makes the dimension reduction possible. We can sometimes display data sets that have many variables only in 2D or 3D because the these top eigenvectors are sometimes enough to capture most of variation in the data. 3.5.1.1 Singular value decomposition and principal component analysis A more common way to calculate PCA is through something called singular value decomposition (SVD). This results in another interpretation of PCA, which is called “latent factor” or “latent component” interpretation. In a moment, it will be more clear what we mean by “latent factors”. SVD is a matrix factorization or decomposition algorithm that decomposes an input matrix,\\(X\\), to three matrices as follows: \\(\\displaystyle \\mathrm{X} = USV^T\\). In essence many matrices can be decomposed as a product of multiple matrices and we will come to other techniques later in this chapter. Singular Value Decomposition is shown in figure 3.22. \\(U\\) is the matrix with eigenarrays on the columns and this has the same dimensions as the input matrix, you might see elsewhere the columns are named as eigenassays. \\(S\\) is the matrix that contain the singular values on the diagonal. The singular values are also known as eigenvalues and their square is proportional to explained variation by each eigenvector. Finally, the matrix \\(V^T\\) contains the eigenvectors on its rows. It is interpretation is still the same. Geometrically, eigenvectors point to the direction of highest variance in the data. They are uncorrolated or geometrically orthogonal to each other. These interpretations are identical to the ones we made before. The slight difference is that the decomposition seem to output \\(V^T\\) which is just the transpose of the matrix \\(V\\). However, the SVD algorithms in R usually return the matrix \\(V\\). If you want the eigenvectors, you either simply use the columns of matrix \\(V\\) or rows of \\(V^T\\). FIGURE 3.22: Singular Value Decomposition (SVD) explained in a diagram. One thing that is new in the figure 3.22 is the concept of eigenarrays. The eigenarrays or sometimes called eigenassays reprensent the sample space and can be used to plot the relationship between samples rather than genes. In this way, SVD offers additional information than the PCA using the covariance matrix. It offers us a way to summarize both genes and samples. As we can project the gene expression profiles over the top two eigengenes and get a 2D representation of genes, but with SVD we can also project the samples over the the top two eigenarrays and get a representation of samples in 2D scatterplot. Eigenvector could represent independent expression programs across samples, such as cell-cycle if we had time-based expression profiles. However, there is no guarantee that each eigenvector will be biologically meaningful. Similarly each eigenarray represent samples with specific expression characteristics. For example, the samples that have a particular pathway activated might be corrolated to an eigenarray returned by SVD. Previously, in order to map samples to the reduced 2D space we had to transpose the genes-by-samples matrix when using princomp() function. We will now first use SVD on genes-by-samples matrix to get eigenarrays and use that to plot samples on the reduced dimensions. We will project the columns in our original expression data on eigenarrays and use the first two dimensions in the scatter plot. If you look at the code you will see that for the projection we use \\(U^T X\\) operation, which is just \\(V^T\\) if you follow the linear algebra. We will also perform the PCA this time with prcomp() function on the transposed genes-by-samples matrix to get a similar information, and plot the samples on the reduced coordinates. par(mfrow=c(1,2)) d=svd(scale(mat)) # apply SVD assays=t(d$u) %*% scale(mat) # projection on eigenassays plot(assays[1,],assays[2,],pch=19, col=annotation_col$LeukemiaType) #plot(d$v[,1],d$v[,2],pch=19, # col=annotation_col$LeukemiaType) pr=prcomp(t(mat),center=TRUE,scale=TRUE) # apply PCA on transposed matrix # plot new coordinates from PCA, projections on eigenvectors # since the matrix is transposed eigenvectors represent plot(pr$x[,1],pr$x[,2],col=annotation_col$LeukemiaType) FIGURE 3.23: SVD on matrix and its transpose As you can see in the figure 3.23, the two approaches yield separation of samples, although they are slightly different. The difference comes from the centering and scaling. In the first case, we scale and center columns and the second case we scale and center rows since the matrix is transposed. If we do not do any scaling or centering we would get identical plots. 3.5.1.1.1 Eigenvectors as latent factors/variables Finally, we can introduce the latent factor interpretation of PCA via SVD. As we have already mentioned eigenvectors can also be interpreted as expression programs that are shared by several genes such as cell cycle expression program when measuring gene expression accross samples taken in different time points. In this intrepretation, linear combination of expression programs makes up the expression profile of the genes. Linear combination simply means multiplying the expression program with a weight and adding them up. Our \\(USV^T\\) matrix multiplication can be rearranged to yield such an understanding, we can multiply eigenarrays \\(U\\) with the diagonal eigenvalues \\(S\\), to produce a m-by-n weights matrix called \\(W\\), so \\(W=US\\) and we can re-write the equation as just weights by eigenvectors matrix, \\(X=WV^T\\) as shown in figure 3.24. FIGURE 3.24: Singular Value Decomposition (SVD) reorgonized as multiplication of m-by-n weights matrix and eigenvectors This simple transformation now makes it clear that indeed if eigenvectors are representing expression programs, their linear combination is making up individual gene expression profiles. As an example, we can show the liner combination of the first two eigenvectors can approximate the expression profile of an hypothetical gene in the gene expression matrix. The figure 3.25 shows eigenvector 1 and eigenvector 2 combined with certain weights in \\(W\\) matrix can approximate gene expression pattern our example gene. FIGURE 3.25: Gene expression of a gene can be thought as linear combination of eigenvectors. However, SVD does not care about biology. The eigenvectors are just obtained from the data with constraints of ortagonality and the direction of variation. There are examples of eigenvectors representing real expression programs but that does not mean eigenvectors will always be biologically meaningful. Sometimes combination of them might make more sense in biology than single eigenvectors. This is also the same for the other matrix factorization techniques we describe below. 3.5.2 Other dimension reduction techniques using other matrix factorization methods We must mention a few other techniques that are similar to SVD in spirit. Remember we mentioned that every matrix can be decomposed to other matrices where matrix multiplication operations reconstruct the original matrix. In the case of SVD/PCA, the constraint is that eigenvectors/arrays are ortogonal, however there are other decomposition algorithms with other constraints. 3.5.2.1 Independent component analysis (ICA) We will first start with independent component analysis (ICA) which is an extension of PCA. ICA algorithm decomposes a given matrix \\(X\\) as follows: \\(X=SA\\). The rows of \\(A\\) could be interpreted similar to the eigengenes and columns of \\(S\\) could be interpreted as eigenarrays, these components are sometimes called metagenes and metasamples in the literature. Traditionally, \\(S\\) is called source matrix and \\(A\\) is called mixing matrix. ICA is developed for a problem called “blind-source separation”. In this problem, multiple microphones record sound from multiple instruments, and the task is disentagle sounds from original instruments since each microphone is recording a combination of sounds. In this respect, the matrix \\(S\\) contains the original signals (sounds from different instruments) and their linear combinations identified by the weights in \\(A\\), and the product of \\(A\\) and \\(S\\) makes up the matrix \\(X\\), which is the observed signal from different microphones. With this interpretation in mind, if the interest is strictly expression patterns similar that represent the hidden expression programs we see that genes-by-samples matrix is transposed to a samples-by-genes matrix, so that the columns of \\(S\\) represent these expression patterns , here refered to as “metagenes”, hopefully representing distinct expression programs (Figure 3.26 ). FIGURE 3.26: Independent Component Analysis (ICA) ICA requires that the columns of \\(S\\) matrix, the “metagenes” in our example above to be statistical independent. This is a stronger constraint than uncorrelatedness. In this case, there should be no relationship between non-linear transformation of the data either. There are different ways of ensuring this statistical indepedence and this is the main constraint when finding the optimal \\(A\\) and \\(S\\) matrices. The various ICA algorithms use different proxies for statistical independence, and the definition of that proxy is the main difference between many ICA algorithms. The algorithm we are going to use requires that metagenes or sources in the \\(S\\) matrix are non-gaussian as possible. Non-gaussianity is shown to be related to statistical independence [REF]. Below, we are using fastICA::fastICA() function to extract 2 components and plot the rows of matrix \\(A\\) which represents metagenes. This way, we can visualize samples in a 2D plot. If we wanted to plot the relationship between genes we would use the the columns of matrix \\(S\\). library(fastICA) ica.res=fastICA(t(mat),n.comp=2) # apply ICA # plot reduced dimensions plot(ica.res$S[,1],ica.res$S[,2],col=annotation_col$LeukemiaType) 3.5.2.2 Non-negative matrix factorization (NMF) Non-negative matrix factorization algorithms are series of algorithms that aim to decompose the matrix \\(X\\) into the product or matrices \\(W\\) and \\(H\\), \\(X=WH\\) (Figure 3.27). The constraint is that \\(W\\) and \\(H\\) must contain non-negative values, so must \\(X\\). This is well suited for data sets that can not contain negative values such as gene expression. This also implies addivity of components, in our example expression of a gene across samples are addition of multiple metagenes. Unlike ICA and SVD/PCA, the metagenes can never be combined in subtractive way. In this sense, expression programs potentially captured by metagenes are combined additively. FIGURE 3.27: Non-negative matrix factorization The algorithms that compute NMF tries to minimize the cost function \\(D(X,WH)\\), which is the distance between \\(X\\) and \\(WH\\). The early algorithms just use the euclidean distance which translates to \\(\\sum(X-WH)^2\\), this is also known as Frobenious norm and you will see in the literature it is written as :\\(\\||V-WH||_{F}\\) However this is not the only distance metric, other distance metrics are also used in NMF algorithms. In addition, there could be other parameters to optimize that relates to sparseness of the \\(W\\) and \\(H\\) matrices. With sparse \\(W\\) and \\(H\\), each entry in the \\(X\\) matrix is expressed as the sum of a small number of components. This makes the interpretation easier, if the weights are 0 than there is not contribution from the corresponding factors. Below, we are plotting the values of metagenes (rows of \\(H\\)) for component 1 and 3. In this context, these values can also be interpreted as relationship between samples. If we wanted to plot the relationship between genes we would plot the columns of \\(W\\) matrix. library(NMF) res=nmf(mat,rank=3,seed=123) # nmf with 3 components/factors w &lt;- basis(res) # get W h &lt;- coef(res) # get H # plot 1st factor against 3rd factor plot(h[1,],h[3,],col=annotation_col$LeukemiaType,pch=19) We should add the note that due to random starting points of the optimization algorithm, NMF is usually run multiple times and a consensus clustering approach is used when clustering samples. This simply means that samples are clustered together if they cluster together in multiple runs of the NMF. The NMF package we used above has built-in ways to achieve this. In addition, NMF is a family of algorithms the choice of cost function to optimize the difference between \\(X\\) and \\(WH\\) and the methods used for optimization creates multiple variants of NMF. The “method” parameter in the above nmf() function controls the which algorithm for NMF. 3.5.2.3 chosing the number of components and ranking components in importance In both ICA and NMF, there is no well-defined way to rank components or to select the number of components. There are couple of approaches that might suit to both ICA and NMF for ranking components. One can use the norms of columns/rows in mixing matrices. This could simply mean take the sum of absolute values in mixing matrices. In our examples above, For our ICA example above, ICA we would take the sum of the absolute values of the rows of \\(A\\) since we transposed the input matrix \\(X\\) before ICA. And for the NMF, we would use the columns of \\(W\\). These ideas assume that the larger coefficients in the weight or mixing matrices indicate more important components. For selecting the optimal number of components, NMF package provides different strategies. One way is to calculate RSS for each \\(k\\), number of components, and take the \\(k\\) where the RSS curve starts to stabilize.However, these strategies require that you run the algorithm with multiple possible component numbers. nmf function will run these automatically when the rank argument is a vector of numbers. For ICA there is no straightforward way to choose the right number of components, a common strategy is to start with as many components as variables and try to rank them by their usefullness. Want to know more ? NMF package vignette has extensive information on how to run NMF to get stable resuts and getting an estimate of components https://cran.r-project.org/web/packages/NMF/vignettes/NMF-vignette.pdf 3.5.3 Multi-dimensional scaling MDS is a set of data analysis techniques that display the structure of distance data in a high dimensional space into a lower dimensional space without much loss of information. The overall goal of MDS is to faithfully represent these distances with the lowest possible dimensions. So called “classical multi-dimensional scaling” algorithm, tries to minimize the following function: \\({\\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\\Biggl (}{\\frac {\\sum _{i,j}{\\bigl (}d_{ij}-\\|z_{i}-z_{j}\\|{\\bigr )}^{2}}{\\sum _{i,j}d_{ij}^{2}}}{\\Biggr )}^{1/2}}\\) Here the function compares the new data points on lower dimension \\((z_{1},z_{2},...,z_{N})\\) to the input distances between data points or distance between samples in our gene expression example. It turns out, this problem can be efficiently solved with SVD/PCA on the scaled distance matrix, the projection on eigenvectors will be the most optimal solution for the equation above. Therefore, classical MDS is sometimes called Principal Coordinates Analysis in the litereuature. However, later variants improve on classical MDS this by using this as a starting point and optimize a slightly different cost function that again measures how well the low-dimensional distances correspond to high-dimensional distances. This variant is called non-metric MDS and due to the nature of the cost function, it assumes a less stringent relationship between the low-dimensional distances $|z_{i}-z_{j}| and input distances \\(d_{ij}\\). Formally, this procedure tries to optimize the following function. \\({\\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\\Biggl (}{\\frac {\\sum _{i,j}{\\bigl (}\\|z_{i}-z_{j}\\|-\\theta(d_{ij}){\\bigr )}^{2}}{\\sum _{i,j}\\|z_{i}-z_{j}\\|^{2}}}{\\Biggr )}^{1/2}}\\) The core of a non-metric MDS algorithm is a twofold optimization process. First the optimal monotonic transformation of the distances has to be found, this is shown in the above formula as \\(\\theta(d_{ij})\\). Secondly, the points on a low dimension configuration have to be optimally arranged, so that their distances match the scaled distances as closely as possible. This two steps are repeated until some convergence criteria is reached. This usually means that the cost function does not improve much after certain number of iterations. The basic steps in a non-metric MDS algorithm are: 1. Find a random low dimensional configuration of points, or in the variant we will be using below we start with the configuration returned by classical MDS 2. Calculate the distances between the points in the low dimension $|z_{i}-z_{j}|, \\(z_{i}\\) and \\(z_{j}\\) are vector of positions for sample \\(i\\) and \\(j\\). 3. Find the optimal monotonic transformation of the input distance, \\({\\textstyle \\theta(d_{ij})}\\), to approximate input distances to low-dimensional distances. This is achieved by isotonic regression, where a monotonically increasing free-form function is fit. This step practically ensures that ranking of low-dimensional distances are similar to rankings of input distances. 4. Minimize the stress function by re-configuring low-dimensional space and keeping \\(\\theta\\) function constant. 5. repeat from step 2 until convergence. We will now demonstrate both classical MDS and Kruskal’s isometric MDS. mds=cmdscale(dist(t(mat))) isomds=MASS::isoMDS(dist(t(mat))) ## initial value 15.907414 ## final value 13.462986 ## converged # plot the patients in the 2D space par(mfrow=c(1,2)) plot(mds,pch=19,col=annotation_col$LeukemiaType, main=&quot;classical MDS&quot;) plot(isomds$points,pch=19,col=annotation_col$LeukemiaType, main=&quot;isotonic MDS&quot;) In this example, there is not much difference between isotonic MDS and classical MDS. However, there might be cases where different MDS methods provides visiable changes in the scatter plots. 3.5.4 t-Distributed Stochastic Neighbor Embedding (t-SNE) t-SNE maps the distances in high-dimensional space to lower dimensions and it is similar to MDS method in this respect. But the benefit of this particular method is that it tries to preserve the local structure of the data so the distances and grouping of the points we observe in a lower dimensions such as a 2D scatter plot is as close as possible to the distances we observe in the high-dimensional space. As with other dimension reduction methods, you can choose how many lower dimensions you need. The main difference of t-SNE is that it tries to preserve the local structure of the data. This kind of local structure embedding is missing in the MDS algorithm which also has a similar goal. MDS tries to optimize the distances as a whole, whereas t-SNE optimizes the distances with the local structure in mind. This is defined by the “perplexity” parameter in the arguments. This parameter controls how much the local structure influences the distance calculation. The lower the value the more the local structure is take into account. Similar to MDS, the process is an optimization algorithm. Here, we also try to minimize the divergence between observed distances and lower dimension distances. However, in the case of t-SNE, the observed distances and lower dimensional distances are transformed using a probabilistic framework with their local variance in mind. From here on, we will provide a bit more detail on how the algorithm works in case conceptual description above is too shallow. In t-SNE the euclidiean distances between data points are transformed into a conditional similarity between points. This is done by assuming a normal distribution on each data point with a variance calculated ultimately by the use of “perplexity” parameter. The perplexity paramater is, in a sense, a guess about the number of the closest neighbors each point has. Setting it to higher values gives more weight to global structure. Given \\(d_{ij}\\) is the euclidean distance between point \\(i\\) and \\(j\\), the similarity score \\(p_{ij}\\) is calculated as shown below. \\(p_{j | i} = \\frac{\\exp(-\\|d_{ij}\\|^2 / 2 σ_i^2)}{∑_{k \\neq i} \\exp(-\\|d_{ik}\\|^2 / 2 σ_i^2)}\\) This distance is symmetrized by incorparating $p_{i | j} as shown below. \\(p_{i j}=\\frac{p_{j|i} + p_{i|j}}{2n}\\) For the distances in the reduced dimension, we use t-distribution with one degree of freedom. In the formula below, \\(| y_i-y_j\\|^2\\) is euclidean distance between points \\(i\\) and \\(j\\) in the reduced dimensions. \\[ q_{i j} = \\frac{(1+ \\| y_i-y_j\\|^2)^{-1}}{(∑_{k \\neq l} 1+ \\| y_k-y_l\\|^2)^{-1} } \\] As most of the algorithms we have seen in this section, t-SNE is an optimization process in essence. In every iteration the points along lower dimensions are re-arranged to minimize the formulated difference between the the observed joint probabilities (\\(p_{i j}\\)) and low-dimensional joint probabilities (\\(q_{i j}\\)). Here we are trying to compare probability distributions. In this case, this is done using a method called Kullback-Leibler divergence, or KL-divergence. In the formula below, since the \\(p_{i j}\\) is pre-defined using original distances, only way to optimize is to play with \\(q_{i j}\\)) because it depends on the configuration of points in the lower dimensional space. This configuration is optimized to minimize the KL-divergence between \\(p_{i j}\\) and \\(q_{i j}\\). \\[ KL(P||Q) = \\sum_{i, j} p_{ij} \\, \\log \\frac{p_{ij}}{q_{ij}}. \\] Strictly speaking, KL-divergence measures how well the distribution \\(P\\) which is observed using the original data points can be aproximated by distribution \\(Q\\), which is modeled using points on the lower dimension. If the distributions are identical KL-divergence would be 0. Naturally, the more divergent the distributions are the higher the KL-divergence will be. We will now show how to use t-SNE on our gene expression data set. We are setting the random seed because again t-SNE optimization algorithm have random starting points and this might create non-identical results in every run. After calculating the t-SNE lower dimension embeddings we will plot the points in a 2D scatter plot. library(&quot;Rtsne&quot;) set.seed(42) # Set a seed if you want reproducible results tsne_out &lt;- Rtsne(t(mat),perplexity = 10) # Run TSNE #image(t(as.matrix(dist(tsne_out$Y)))) # Show the objects in the 2D tsne representation plot(tsne_out$Y,col=annotation_col$LeukemiaType, pch=19) # create the legend for the Leukemia types legend(&quot;bottomleft&quot;, legend=unique(annotation_col$LeukemiaType), fill =palette(&quot;default&quot;), border=NA,box.col=NA) As you might have noticed, we set again a random seed with set.seed() function. The optimization algorithm starts with random configuration of points in the lower dimension space, and each iteration it tries to improve on the previous lower dimension confugration, that is why starting points can result in different final outcomes. Want to know more ? How perplexity effects t-sne, interactive examples https://distill.pub/2016/misread-tsne/ more on perplexity: https://blog.paperspace.com/dimension-reduction-with-t-sne/ Intro to t-SNE https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm "],
["exercises-1.html", "3.6 Exercises", " 3.6 Exercises 3.6.1 How to summarize collection of data points: The idea behind statistical distributions 3.6.1.1 Calculate the means and variances of the rows of the following simulated data set, plot the distributions of means and variances using hist() and boxplot() functions. set.seed(100) #sample data matrix from normal distribution gset=rnorm(600,mean=200,sd=70) data=matrix(gset,ncol=6) 3.6.1.2 Using the data generated above, calculate the standard deviation of the distribution of the means using sd() function. Compare that to the expected standard error obtained from central limit theorem keeping in mind the population parameters were \\(\\sigma=70\\) and \\(n=6\\). How does the estimate from the random samples change if we simulate more data with data=matrix(rnorm(6000,mean=200,sd=70),ncol=6) 3.6.1.3 simulate 30 random variables using rpois() function, do this 1000 times and calculate means of sample. Plot the sampling distributions of the means using a histogram. Get the 2.5th and 97.5th percentiles of the distribution. Use t.test function to calculate confidence intervals of the first random sample pois1 simulated fromrpois() function below. Use bootstrap confidence interval for the mean on pois1 compare all the estimates set.seed(100) #sample 30 values from poisson dist with lamda paramater =30 pois1=rpois(30,lambda=5) 3.6.1.4 Optional exercise: Try to recreate the following figure, which demonstrates the CLT concept. 3.6.2 How to test for differences in samples 3.6.2.1 Test the difference of means of the following simulated genes using the randomization, t-test and wilcox.test() functions. Plot the distributions using histograms and boxplots. set.seed(101) gene1=rnorm(30,mean=4,sd=3) gene2=rnorm(30,mean=3,sd=3) 3.6.2.2 Test the difference of means of the following simulated genes using the randomization, t-test and wilcox.test() functions. Plot the distributions using histograms and boxplots. set.seed(100) gene1=rnorm(30,mean=4,sd=2) gene2=rnorm(30,mean=2,sd=2) 3.6.2.3 read the gene expression data set with data=readRDS(&quot;StatisticsForGenomics/geneExpMat.rds&quot;). The data has 100 differentially expressed genes.First 3 columns are the test samples, and the last 3 are the control samples. Do a t-test for each gene (each row is a gene), record the p-values. Then, do a moderated t-test, as shown in the lecture notes and record the p-values. Do a p-value histogram and compare two approaches in terms of the number of significant tests with 0.05 threshold. On the p-values use FDR (BH), bonferroni and q-value adjustment methods. Calculate how many adjusted p-values are below 0.05 for each approach. 3.6.3 Relationship between variables: linear models and correlation 3.6.3.1 Below we are going to simulate X and Y values. Run the code then fit a line to predict Y based on X. Plot the scatter plot and the fitted line. Calculate correlation and R^2. Run the summary() function and try to extract P-values for the model from the object returned by summary. see ?summary.lm Plot the residuals vs fitted values plot, by calling plot function with which=1 as the second argument. First argument is the model returned by lm. # set random number seed, so that the random numbers from the text # is the same when you run the code. set.seed(32) # get 50 X values between 1 and 100 x = runif(50,1,100) # set b0,b1 and varience (sigma) b0 = 10 b1 = 2 sigma = 20 # simulate error terms from normal distribution eps = rnorm(50,0,sigma) # get y values from the linear equation and addition of error terms y = b0 + b1*x+ eps 3.6.3.2 Read the data set histone modification data set with using a variation of: df=readRDS(&quot;StatisticsForGenomics_data/HistoneModeVSgeneExp.rds&quot;). There are 3 columns in the data set these are measured levels of H3K4me3, H3K27me3 and gene expression per gene. plot the scatter plot for H3K4me3 vs expression plot the scatter plot for H3K27me3 vs expression fit the model model for prediction of expression data using: only H3K4me3 as explanatory variable only H3K27me3 as explanatory variable using both H3K4me3 and H3K27me3 as explanatory variables inspect summary() function output in each case, which terms are significant Is using H3K4me3 and H3K27me3 better than the model with only H3K4me3. Plot H3k4me3 vs H3k27me3. Inspect the points that does not follow a linear trend. Are they clustered at certain segments of the plot. Bonus: Is there any biological or technical interpretation for those points ? "],
["genomicIntervals.html", "Chapter 4 Operations on Genomic Intervals and Genome Arithmetic", " Chapter 4 Operations on Genomic Intervals and Genome Arithmetic A considerable time in computational genomics is spent on overlapping different features of the genome. Each feature can be represented with a genomic interval within the chromosomal coordinate system. In addition, each interval can carry different sorts of information. An interval may for instance represent exon coordinates or a transcription factor binding site. On the other hand, you can have base-pair resolution, continuous scores over the genome such as read coverage or scores that could be associated with only certain bases such as in the case of CpG methylation (See Figure 4.1 ). Typically, you will need to overlap intervals on interest with other features of the genome, again represented as intervals. For example, you may want to overlap transcription factor binding sites with CpG islands or promoters to quantify what percentage of binding sites overlap with your regions of interest. Overlapping mapped reads from high-throughput sequencing experiments with genomic features such as exons, promoters, enhancers can also be classified as operations on genomic intervals. You can think of a million other ways that involves overlapping two sets of different features on the genome. This chapter aims to show how to do analysis involving operations on genomic intervals. FIGURE 4.1: Summary of genomic intervals with different kinds of information "],
["operations-on-genomic-intervals-with-genomicranges-package.html", "4.1 Operations on Genomic Intervals with GenomicRanges package", " 4.1 Operations on Genomic Intervals with GenomicRanges package The Bioconductor project has a dedicated package called GenomicRanges to deal with genomic intervals. In this section, we will provide use cases involving operations on genomic intervals. The main reason we will stick to this package is that it provides tools to do overlap operations. However package requires that users operate on specific data types that are conceptually similar to a tabular data structure implemented in a way that makes overlapping and related operations easier. The main object we will be using is called GRanges object and we will also see some other related objects from the GenomicRanges package. 4.1.1 How to create and manipulate a GRanges object GRanges (from GenomicRanges package) is the main object that holds the genomic intervals and extra information about those intervals. Here we will show how to create one. Conceptually, it is similar to a data frame and some operations such as using notation to subset the table will work also on GRanges, but keep in mind that not everything that works for data frames will work on GRanges objects. library(GenomicRanges) gr=GRanges(seqnames=c(&quot;chr1&quot;,&quot;chr2&quot;,&quot;chr2&quot;), ranges=IRanges(start=c(50,150,200), end=c(100,200,300)), strand=c(&quot;+&quot;,&quot;-&quot;,&quot;-&quot;) ) gr ## GRanges object with 3 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## [1] chr1 50-100 + ## [2] chr2 150-200 - ## [3] chr2 200-300 - ## ------- ## seqinfo: 2 sequences from an unspecified genome; no seqlengths # subset like a data frame gr[1:2,] ## GRanges object with 2 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## [1] chr1 50-100 + ## [2] chr2 150-200 - ## ------- ## seqinfo: 2 sequences from an unspecified genome; no seqlengths As you can see it looks a bit like a data frame. Also, note that the peculiar second argument “ranges” which basically contains start and end positions of the genomic intervals. However, you can not just give start and end positions you actually have to provide another object of IRanges. Do not let this confuse you, GRanges actually depends on another object that is very similar to itself called IRanges and you have to provide the “ranges” argument as an IRanges object. In its simplest for, an IRanges object can be constructed by providing start and end positions to IRanges() function. Think of it as something you just have to provide in order to construct the GRanges object. GRanges can also contain other information about the genomic interval such as scores, names, etc. You can provide extra information at the time of the construction or you can add it later. Here is how you can do those: gr=GRanges(seqnames=c(&quot;chr1&quot;,&quot;chr2&quot;,&quot;chr2&quot;), ranges=IRanges(start=c(50,150,200), end=c(100,200,300)), names=c(&quot;id1&quot;,&quot;id3&quot;,&quot;id2&quot;), scores=c(100,90,50) ) # or add it later (replaces the existing meta data) mcols(gr)=DataFrame(name2=c(&quot;pax6&quot;,&quot;meis1&quot;,&quot;zic4&quot;), score2=c(1,2,3)) gr=GRanges(seqnames=c(&quot;chr1&quot;,&quot;chr2&quot;,&quot;chr2&quot;), ranges=IRanges(start=c(50,150,200), end=c(100,200,300)), names=c(&quot;id1&quot;,&quot;id3&quot;,&quot;id2&quot;), scores=c(100,90,50) ) # or appends to existing meta data mcols(gr)=cbind(mcols(gr), DataFrame(name2=c(&quot;pax6&quot;,&quot;meis1&quot;,&quot;zic4&quot;)) ) gr ## GRanges object with 3 ranges and 3 metadata columns: ## seqnames ranges strand | names ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; ## [1] chr1 50-100 * | id1 ## [2] chr2 150-200 * | id3 ## [3] chr2 200-300 * | id2 ## scores name2 ## &lt;numeric&gt; &lt;character&gt; ## [1] 100 pax6 ## [2] 90 meis1 ## [3] 50 zic4 ## ------- ## seqinfo: 2 sequences from an unspecified genome; no seqlengths # elementMetadata() and values() do the same things elementMetadata(gr) ## DataFrame with 3 rows and 3 columns ## names scores name2 ## &lt;character&gt; &lt;numeric&gt; &lt;character&gt; ## 1 id1 100 pax6 ## 2 id3 90 meis1 ## 3 id2 50 zic4 values(gr) ## DataFrame with 3 rows and 3 columns ## names scores name2 ## &lt;character&gt; &lt;numeric&gt; &lt;character&gt; ## 1 id1 100 pax6 ## 2 id3 90 meis1 ## 3 id2 50 zic4 # you may also add metadata using the $ operator, as for data frames gr$name3 = c(&quot;A&quot;,&quot;C&quot;, &quot;B&quot;) gr ## GRanges object with 3 ranges and 4 metadata columns: ## seqnames ranges strand | names ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; ## [1] chr1 50-100 * | id1 ## [2] chr2 150-200 * | id3 ## [3] chr2 200-300 * | id2 ## scores name2 name3 ## &lt;numeric&gt; &lt;character&gt; &lt;character&gt; ## [1] 100 pax6 A ## [2] 90 meis1 C ## [3] 50 zic4 B ## ------- ## seqinfo: 2 sequences from an unspecified genome; no seqlengths 4.1.2 Getting genomic regions into R as GRanges objects There are multiple ways you can read in your genomic features into R and create a GRanges object. Most genomic interval data comes as a tabular format that has the basic information about the location of the interval and some other information. We already showed how to read BED files as data frame. Now we will show how to convert it to GRanges object. This is one way of doing it, but there are more convenient ways described further in the text. # read CpGi data set filePath=system.file(&quot;extdata&quot;, &quot;cpgi.hg19.chr21.bed&quot;, package=&quot;compGenomRData&quot;) cpgi.df = read.table(filePath, header = FALSE, stringsAsFactors=FALSE) # remove chr names with &quot;_&quot; cpgi.df =cpgi.df [grep(&quot;_&quot;,cpgi.df[,1],invert=TRUE),] cpgi.gr=GRanges(seqnames=cpgi.df[,1], ranges=IRanges(start=cpgi.df[,2], end=cpgi.df[,3])) You may need to do some pre-processing before/after reading in the BED file. Below is an example of getting transcription start sites from BED files containing refseq transcript locations. # read refseq file filePathRefseq=system.file(&quot;extdata&quot;, &quot;refseq.hg19.chr21.bed&quot;, package=&quot;compGenomRData&quot;) ref.df = read.table(filePathRefseq, header = FALSE, stringsAsFactors=FALSE) ref.gr=GRanges(seqnames=ref.df[,1], ranges=IRanges(start=ref.df[,2], end=ref.df[,3]), strand=ref.df[,6],name=ref.df[,4]) # get TSS tss.gr=ref.gr # end of the + strand genes must be equalized to start pos end(tss.gr[strand(tss.gr)==&quot;+&quot;,]) =start(tss.gr[strand(tss.gr)==&quot;+&quot;,]) # startof the - strand genes must be equalized to end pos start(tss.gr[strand(tss.gr)==&quot;-&quot;,])=end(tss.gr[strand(tss.gr)==&quot;-&quot;,]) # remove duplicated TSSes ie alternative transcripts # this keeps the first instance and removes duplicates tss.gr=tss.gr[!duplicated(tss.gr),] Another way of doing this is from a BED file is to use readTranscriptfeatures() function from the genomation package. This function takes care of the steps described in the code chunk above. Reading the genomic features as text files and converting to GRanges is not the only way to create GRanges object. With the help of the rtracklayer package we can directly import BED files. require(rtracklayer) # we are reading a BED file, the path to the file # is stored in filePathRefseq variable import.bed(filePathRefseq) Next, we will show how to use other methods to automatically obtain the data in GRanges format from online databases. But you will not be able to use these methods for every data set so it is good to now how to read data from flat files as well. We will use rtracklayer package to download data from UCSC browser. We will download CpG islands as GRanges objects. The rtracklayer workflow we show below works like using the UCSC table browser. You need to select which species you are working with, then you need to select which dataset you need to download and lastly you download the UCSC dataset or track as GRanges object. require(rtracklayer) session &lt;- browserSession(&quot;UCSC&quot;,url = &#39;http://genome-euro.ucsc.edu/cgi-bin/&#39;) genome(session) &lt;- &quot;mm9&quot; ## choose CpG island track on chr12 query &lt;- ucscTableQuery(session, track=&quot;CpG Islands&quot;,table=&quot;cpgIslandExt&quot;, range=GRangesForUCSCGenome(&quot;mm9&quot;, &quot;chr12&quot;)) ## get the GRanges object for the track track(query) There is also an interface to Ensembl database called biomaRt. This package will enable you to access and import all of the datasets included in Ensembl. Another similar package is AnnotationHub. . This package is an aggregator for different datasets from various sources. Using AnnotationHub one can access data sets from UCSC browser, Ensembl browser and data sets from genomics consortiums such as ENCODE and RoadmapEpigenomics. Although, we do not provide examples, we invite curious readers to read the vignettes of the aforementioned annotation packages. 4.1.2.1 Frequently used file formats and how to read them into R as a table There are multiple file formats in genomics but some of them you will see more frequently than others. We already mentioned some of them. Here is a list of files and functions to read them into R as GRanges objects or something coercible to GRanges objects. BED: These are used and popularized by UCSC browser, and can hold a variety of information including exon/intron structure of transcripts in a single line. genomation::readBed() genomation::readTranscriptFeatures() good for getting intron/exon/promoters from BED12 files rtracklayer::import.bed() GFF: GFF format is a tabular text format for genomic features similar to BED. However, it is a more flexible format than BED, which makes it harder to parse at times. Many gene annotation files are in this format. genomation::gffToGranges() rtracklayer::impot.gff() BAM: BAM format is compressed and indexed tabular file format designed for sequencing reads. GenomicAlignments::readGAlignments Rsamtools::scanBam returns a data frame with columns from SAM/BAM file. BigWig: This is used to for storing scores associated with genomic intervals. It is an indexed format. Similar to BAM, this makes it easier to query and only necessary portions of the file could be loaded into memory. rtracklayer::import.bw() Generic Text files: This represents any text file with the minimal information of chromosome, start and end coordinates. genomation::readGeneric() Tabix/Bcf: These are tabular file formats indexed and compressed similar to BAM. The following functions return lists rather than tabular data structures. These formats are mostly used to store genomic variation data such as SNPs and indels. Rsamtools::scanTabix Rsamtools::scanBcf 4.1.3 Finding regions that do/do not overlap with another set of regions This is one of the most common tasks in genomics. Usually, you have a set of regions that you are interested in and you want to see if they overlap with another set of regions or see how many of them overlap. A good example is transcription factor binding sites determined by ChIP-seq experiments. In these types of experiments and followed analysis, one usually ends up with genomic regions that are bound by transcription factors. One of the standard next questions would be to annotate binding sites with genomic annotations such as promoter,exon,intron and/or CpG islands. Below is a demonstration of how transcription factor binding sites can be annotated using CpG islands. First, we will get the subset of binding sites that overlap with the CpG islands. In this case, binding sites are ChIP-seq peaks. We can find the subset of peaks that overlap with the CpG islands using the subsetByoverlaps() function. You will also see another way of converting data frames to GRanges. library(genomation) filePathPeaks=system.file(&quot;extdata&quot;, &quot;wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep1.broadPeak.gz&quot;, package=&quot;compGenomRData&quot;) # read the peaks from a bed file pk1.gr=readBroadPeak(filePathPeaks) # get the peaks that overlap with CpG islands subsetByOverlaps(pk1.gr,cpgi.gr) ## GRanges object with 44 ranges and 5 metadata columns: ## seqnames ranges strand | name ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; ## [1] chr21 9825360-9826582 * | peak14562 ## [2] chr21 9968469-9968984 * | peak14593 ## [3] chr21 15755368-15755956 * | peak14828 ## [4] chr21 19191579-19192525 * | peak14840 ## [5] chr21 26979619-26980048 * | peak14854 ## ... ... ... ... . ... ## [40] chr21 46237464-46237809 * | peak15034 ## [41] chr21 46707702-46708084 * | peak15037 ## [42] chr21 46961552-46961875 * | peak15039 ## [43] chr21 47743587-47744125 * | peak15050 ## [44] chr21 47878412-47878891 * | peak15052 ## score signalValue pvalue qvalue ## &lt;integer&gt; &lt;numeric&gt; &lt;integer&gt; &lt;integer&gt; ## [1] 56 183.11 -1 -1 ## [2] 947 3064.92 -1 -1 ## [3] 90 291.9 -1 -1 ## [4] 290 940.03 -1 -1 ## [5] 32 104.67 -1 -1 ## ... ... ... ... ... ## [40] 32 106.36 -1 -1 ## [41] 67 217.02 -1 -1 ## [42] 38 124.31 -1 -1 ## [43] 353 1141.58 -1 -1 ## [44] 104 338.78 -1 -1 ## ------- ## seqinfo: 23 sequences from an unspecified genome; no seqlengths For each CpG island, we can count the number of peaks that overlap with a given CpG island with countOverlaps(). counts=countOverlaps(pk1.gr,cpgi.gr) head(counts) ## [1] 0 0 0 0 0 0 The findOverlaps() function can be used to see one-to-one overlaps between peaks and CpG islands. It returns a matrix showing which peak overlaps with which CpGi island. findOverlaps(pk1.gr,cpgi.gr) ## Hits object with 45 hits and 0 metadata columns: ## queryHits subjectHits ## &lt;integer&gt; &lt;integer&gt; ## [1] 14562 1 ## [2] 14593 3 ## [3] 14828 8 ## [4] 14840 13 ## [5] 14854 16 ## ... ... ... ## [41] 15034 155 ## [42] 15037 166 ## [43] 15039 176 ## [44] 15050 192 ## [45] 15052 200 ## ------- ## queryLength: 26121 / subjectLength: 205 Another interesting thing would be to look at the distances to nearest CpG islands for each peak. In addition, just finding the nearest CpG island could also be interesting. Often times, you will need to find nearest TSS or gene to your regions of interest, and the code below is handy for doing that. # find nearest CpGi to each TSS n.ind=nearest(pk1.gr,cpgi.gr) # get distance to nearest dists=distanceToNearest(pk1.gr,cpgi.gr,select=&quot;arbitrary&quot;) dists ## Hits object with 620 hits and 1 metadata column: ## queryHits subjectHits | distance ## &lt;integer&gt; &lt;integer&gt; | &lt;integer&gt; ## [1] 14440 1 | 384188 ## [2] 14441 1 | 382968 ## [3] 14442 1 | 381052 ## [4] 14443 1 | 379311 ## [5] 14444 1 | 376978 ## ... ... ... . ... ## [616] 15055 205 | 26212 ## [617] 15056 205 | 27402 ## [618] 15057 205 | 30468 ## [619] 15058 205 | 31611 ## [620] 15059 205 | 34090 ## ------- ## queryLength: 26121 / subjectLength: 205 # histogram of the distances to nearest TSS dist2plot=mcols(dists)[,1] hist(log10(dist2plot),xlab=&quot;log10(dist to nearest TSS)&quot;, main=&quot;Distances&quot;) FIGURE 4.2: histogram of distances "],
["dealing-with-mapped-high-throughput-sequencing-reads.html", "4.2 Dealing with mapped high-throughput sequencing reads", " 4.2 Dealing with mapped high-throughput sequencing reads The reads from sequencing machines are usually pre-proccessed and aligned to the genome with the help of specific bioinformatics tools. We have introduced the read processing , quality check and alignment methods in chapter 5. In this section we will deal with mapped reads. Since each mapped read has a start and end position the genome, mapped reads can be thought as genomic intervals stored in a file. After mapping, the next task is to quantify the enrichment of those aligned reads in the regions of interest. You may want to count how many reads overlapping with your promoter set of interest or you may want to quantify RNA-seq reads overlapping with exons. This is similar to operations on genomic intervals which are described previously. If you can read all your alignments into the memory and create a GRanges object, you can apply the previously described operations. However, most of the time we can not read all mapped reads into the memory, so we have to use specialized tools to query and quantify alignments on a given set of regions. One of the most common alignment formats is SAM/BAM format, most aligners will produce SAM/BAM output or you will be able to convert your specific alignment format to SAM/BAM format. The BAM format is a binary version of the human readable SAM format. The SAM format has specific columns that contain different kind of information about the alignment such as mismatches, qualities etc. (see [http://samtools.sourceforge.net/SAM1.pdf] for SAM format specification). 4.2.1 Counting mapped reads for a set of regions Rsamtools package has functions to query BAM files. The function we will use in the first example is countBam which takes input of the BAM file and param argument. “param” argument takes a ScanBamParam object. The object is instantiated using ScanBamParam() and contains parameters for scanning the BAM file. The example below is a simple example where ScanBamParam() only includes regions of interest, promoters on chr21. promoter.gr=tss.gr start(promoter.gr)=start(promoter.gr)-1000 end(promoter.gr) =end(promoter.gr)+1000 promoter.gr=promoter.gr[seqnames(promoter.gr)==&quot;chr21&quot;] library(Rsamtools) bamfilePath=system.file(&quot;extdata&quot;, &quot;wgEncodeHaibTfbsGm12878Sp1Pcr1xAlnRep1.chr21.bam&quot;, package=&quot;compGenomRData&quot;) # get reads for regions of interest from the bam file param &lt;- ScanBamParam(which=promoter.gr) counts=countBam(bamfilePath, param=param) Alternatively, aligned reads can be read in using GenomicAlignments package (which on this occasion relies on RSamtools package). library(GenomicAlignments) alns &lt;- readGAlignments(bamfilePath, param=param) "],
["dealing-with-continuous-scores-over-the-genome.html", "4.3 Dealing with continuous scores over the genome", " 4.3 Dealing with continuous scores over the genome Most high-throughput data can be viewed as a continuous score over the bases of the genome. In case of RNA-seq or ChIP-seq experiments the data can be represented as read coverage values per genomic base position. In addition, other information (not necessarily from high-throughput experiments) can be represented this way. The GC content and conservation scores per base are prime examples of other data sets that can be represented as scores. This sort of data can be stored as a generic text file or can have special formats such as Wig (stands for wiggle) from UCSC, or the bigWig format is which is indexed binary format of the wig files. The bigWig format is great for data that covers large fraction of the genome with varying scores, because the file is much smaller than regular text files that have the same information and it can be queried easier since it is indexed. In R/Bioconductor, the continuous data can also be represented in a compressed format, in a format called Rle vector, which stands for run-length encoded vector. This gives superior memory performance over regular vectors because repeating consecutive values are represented as one value in the Rle vector (See Figure 4.3 ). FIGURE 4.3: Rle encoding explained Typically, for genome-wide data you will have a RleList object which is a list of Rle vectors per chromosome. You can obtain such vectors by reading the reads in and calling coverage() function from GenomicRanges package. Let’s try that on the above data set. covs=coverage(alns) # get coverage vectors covs ## RleList of length 24 ## $chr1 ## integer-Rle of length 249250621 with 1 run ## Lengths: 249250621 ## Values : 0 ## ## $chr2 ## integer-Rle of length 243199373 with 1 run ## Lengths: 243199373 ## Values : 0 ## ## $chr3 ## integer-Rle of length 198022430 with 1 run ## Lengths: 198022430 ## Values : 0 ## ## $chr4 ## integer-Rle of length 191154276 with 1 run ## Lengths: 191154276 ## Values : 0 ## ## $chr5 ## integer-Rle of length 180915260 with 1 run ## Lengths: 180915260 ## Values : 0 ## ## ... ## &lt;19 more elements&gt; Alternatively, you can get the coverage from the Bam file directly. Below, we are getting the coverage directly from the Bam file for our previously defined promoters. covs=coverage(bamfilePath, param=param) # get coverage vectors One of the most common ways of storing score data is, as mentioned, wig or bigWig format. Most of the ENCODE project data can be downloaded in bigWig format. In addition, conservation scores can also be downloaded as wig/bigWig format. You can import bigWig files into R using import() function from rtracklayer package. However, it is generally not advisable to read the whole bigWig file in memory as it was the case with BAM files. Usually, you will be interested in only a fraction of the genome, such as promoters, exons etc. So it is best you extract the data for those regions and read those into memory rather than the whole file. Below we read the a bigWig file only for promoters. The operation returns an GRanges object with score column which indicates the scores in the BigWig file per genomic region. library(rtracklayer) # File from ENCODE ChIP-seq tracks bwFile=system.file(&quot;extdata&quot;, &quot;wgEncodeHaibTfbsA549.chr21.bw&quot;, package=&quot;compGenomRData&quot;) bw.gr=import(bwFile, which=promoter.gr) # get coverage vectors bw.gr ## GRanges object with 9205 ranges and 1 metadata column: ## seqnames ranges strand | score ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;numeric&gt; ## [1] chr21 9825456-9825457 * | 1 ## [2] chr21 9825458-9825464 * | 2 ## [3] chr21 9825465-9825466 * | 4 ## [4] chr21 9825467-9825470 * | 5 ## [5] chr21 9825471 * | 6 ## ... ... ... ... . ... ## [9201] chr21 48055809-48055856 * | 2 ## [9202] chr21 48055857-48055858 * | 1 ## [9203] chr21 48055872-48055921 * | 1 ## [9204] chr21 48055944-48055993 * | 1 ## [9205] chr21 48056069-48056118 * | 1 ## ------- ## seqinfo: 1 sequence from an unspecified genome Following this we can create an RleList object from the GRanges with coverage function. cov.bw=coverage(bw.gr,weight = &quot;score&quot;) # or get this directly from cov.bw=import(bwFile, which=promoter.gr,as = &quot;RleList&quot;) 4.3.1 Extracting subsections of Rle and RleList objects Frequently, we will need to extract subsections of the Rle vectors or RleList objects. We will need to do this to visualize that subsection or get some statistics out of those sections. For example, we could be interested in average coverage per base for the regions we are interested in. We have to extract those regions from RleList object and apply summary statistics. Below, we show how to extract subsections of RleList object. We are extracting promoter regions from ChIP-seq read coverage RleList. Following that, we will plot the one of the promoters associated coverage values. myViews=Views(cov.bw,as(promoter.gr,&quot;IRangesList&quot;)) # get subsets of coverage # there is a views object for each chromosome myViews ## RleViewsList of length 1 ## $chr21 ## Views on a 48129895-length Rle subject ## ## views: ## start end width ## [1] 42218039 42220039 2001 [2 0 0 0 0 0 0 0 0 ...] ## [2] 17441841 17443841 2001 [0 0 0 0 0 0 0 0 0 ...] ## [3] 17565698 17567698 2001 [0 0 0 0 0 0 0 0 0 ...] ## [4] 30395937 30397937 2001 [0 0 0 0 0 0 0 0 0 ...] ## [5] 27542138 27544138 2001 [1 1 1 1 1 1 1 1 1 ...] ## [6] 27511708 27513708 2001 [0 0 0 0 0 0 0 0 0 ...] ## [7] 32930290 32932290 2001 [0 0 0 0 0 0 0 0 0 ...] ## [8] 27542446 27544446 2001 [0 0 0 0 0 0 0 0 0 ...] ## [9] 28338439 28340439 2001 [0 0 0 0 0 0 0 0 0 ...] ## ... ... ... ... ... ## [370] 47517032 47519032 2001 [1 1 1 1 1 1 1 1 1 ...] ## [371] 47648157 47650157 2001 [1 1 1 1 1 1 1 1 1 ...] ## [372] 47603373 47605373 2001 [0 0 0 0 0 0 0 0 0 ...] ## [373] 47647738 47649738 2001 [2 2 2 2 2 2 2 2 2 ...] ## [374] 47704236 47706236 2001 [0 0 0 0 0 0 0 0 0 ...] ## [375] 47742785 47744785 2001 [0 0 0 0 0 0 0 0 0 ...] ## [376] 47881383 47883383 2001 [1 1 1 1 1 1 1 1 1 ...] ## [377] 48054506 48056506 2001 [0 0 0 0 0 0 0 0 0 ...] ## [378] 48024035 48026035 2001 [1 1 1 1 1 1 1 1 1 ...] myViews[[1]] ## Views on a 48129895-length Rle subject ## ## views: ## start end width ## [1] 42218039 42220039 2001 [2 0 0 0 0 0 0 0 0 ...] ## [2] 17441841 17443841 2001 [0 0 0 0 0 0 0 0 0 ...] ## [3] 17565698 17567698 2001 [0 0 0 0 0 0 0 0 0 ...] ## [4] 30395937 30397937 2001 [0 0 0 0 0 0 0 0 0 ...] ## [5] 27542138 27544138 2001 [1 1 1 1 1 1 1 1 1 ...] ## [6] 27511708 27513708 2001 [0 0 0 0 0 0 0 0 0 ...] ## [7] 32930290 32932290 2001 [0 0 0 0 0 0 0 0 0 ...] ## [8] 27542446 27544446 2001 [0 0 0 0 0 0 0 0 0 ...] ## [9] 28338439 28340439 2001 [0 0 0 0 0 0 0 0 0 ...] ## ... ... ... ... ... ## [370] 47517032 47519032 2001 [1 1 1 1 1 1 1 1 1 ...] ## [371] 47648157 47650157 2001 [1 1 1 1 1 1 1 1 1 ...] ## [372] 47603373 47605373 2001 [0 0 0 0 0 0 0 0 0 ...] ## [373] 47647738 47649738 2001 [2 2 2 2 2 2 2 2 2 ...] ## [374] 47704236 47706236 2001 [0 0 0 0 0 0 0 0 0 ...] ## [375] 47742785 47744785 2001 [0 0 0 0 0 0 0 0 0 ...] ## [376] 47881383 47883383 2001 [1 1 1 1 1 1 1 1 1 ...] ## [377] 48054506 48056506 2001 [0 0 0 0 0 0 0 0 0 ...] ## [378] 48024035 48026035 2001 [1 1 1 1 1 1 1 1 1 ...] # get the coverage vector from the 5th view and plot plot(myViews[[1]][[5]],type=&quot;l&quot;) FIGURE 4.4: Coverage vector extracted from RleList via Views() function is plotted as a line plot. Next, we are interested in average coverage per base for the promoters using summary functions that works on Views object. # get the mean of the views head( viewMeans(myViews[[1]]) ) ## [1] 0.2259 0.3498 1.2244 0.4998 2.0905 0.6997 # get the max of the views head( viewMaxs(myViews[[1]]) ) ## [1] 2 4 12 4 21 6 "],
["genomic-intervals-with-more-information-summarizedexperiment-class.html", "4.4 Genomic intervals with more information: SummarizedExperiment class", " 4.4 Genomic intervals with more information: SummarizedExperiment class As we have seen, genomic intervals can be mainly contained in a GRanges object. It can also contain additional columns associated with each interval, here you can save information such as read counts or other scores associated with the interval. However, genomics data is often have many layers. With GRanges you can have a table associated with the intervals, but what happens if you have many tables and each table has some metadata associated with it. In addition, rows and columns might have additional annotation that can not be contained by row or column names. For these cases, SummarizedExperiment class is ideal. It can hold multi-layered tabular data associated with each genomic interval and the meta-data associated with rows and columns, or associated with each table. For example, genomic intervals associated with the SummarizedExperiment object can be gene locations, and each tabular data structure can be RNA-seq read counts in a time course experiment. Each table could represent different conditions in which experiments performed. The SummarizedExperiment class is outlined in the figure below (Figure 4.5 ). FIGURE 4.5: Overview of SummarizedExperiment class and functions 4.4.1 Create a SummarizedExperiment object Here we show how to create a basic SummarizedExperiment object. We will first create a matrix of read counts. This matrix will represent read counts from a series RNA-seq experiments from different timepoints. Following that, we create GRanges object to represent the locations of the genes, and a table for column annotations. This will include the names for the columns and any other value we want to represent. Finally, we will create a SummarizedExperiment object by combining all those pieces. # simulate an RNA-seq read counts table nrows &lt;- 200 ncols &lt;- 6 counts &lt;- matrix(runif(nrows * ncols, 1, 1e4), nrows) # create gene locations rowRanges &lt;- GRanges(rep(c(&quot;chr1&quot;, &quot;chr2&quot;), c(50, 150)), IRanges(floor(runif(200, 1e5, 1e6)), width=100), strand=sample(c(&quot;+&quot;, &quot;-&quot;), 200, TRUE), feature_id=paste0(&quot;gene&quot;, 1:200)) # create table for the columns colData &lt;- DataFrame(timepoint=1:6, row.names=LETTERS[1:6]) # create SummarizedExperiment object se=SummarizedExperiment(assays=list(counts=counts), rowRanges=rowRanges, colData=colData) se ## class: RangedSummarizedExperiment ## dim: 200 6 ## metadata(0): ## assays(1): counts ## rownames: NULL ## rowData names(1): feature_id ## colnames(6): A B ... E F ## colData names(1): timepoint 4.4.2 Subset and manipulate the SummarizedExperiment object Now that we have a SummarizedExperiment object, we can subset it and extract/change parts of it. 4.4.2.1 Extracting parts of the object colData() and rowData() extract the column associated and row associated tables. metaData() extracts the meta-data table if there is any table associated. colData(se) # extract column associated data ## DataFrame with 6 rows and 1 column ## timepoint ## &lt;integer&gt; ## A 1 ## B 2 ## C 3 ## D 4 ## E 5 ## F 6 rowData(se) # extrac row associated data ## DataFrame with 200 rows and 1 column ## feature_id ## &lt;character&gt; ## 1 gene1 ## 2 gene2 ## 3 gene3 ## 4 gene4 ## 5 gene5 ## ... ... ## 196 gene196 ## 197 gene197 ## 198 gene198 ## 199 gene199 ## 200 gene200 To extract the main table or tables that contain the values of interest such as read counts. We must use the assays() function. This returns a list of DataFrame objects associated with the object. assays(se) # extract list of assays ## List of length 1 ## names(1): counts You can use names with $ or [] notaion to extract specific tables from the list. assays(se)$counts # get the table named &quot;counts&quot; assays(se)[[1]] # get the first table 4.4.2.2 subsetting Subsetting is easy using [ ] notation. This is similar to the way we subset data frames or matrices. # subset the first five transcripts and first three samples se[1:5, 1:3] ## class: RangedSummarizedExperiment ## dim: 5 3 ## metadata(0): ## assays(1): counts ## rownames: NULL ## rowData names(1): feature_id ## colnames(3): A B C ## colData names(1): timepoint One can also use $ operator to subset based on colData() columns. You can extract certain samples or in our case time points. se[, se$timepoint == 1] In addition, as SummarizedExperiment objects are GRanges objects on streoids, they support all of the findOverlaps() methods and associated functions that work on GRanges objects. # Subset for only rows which are in chr1:100,000-1,100,000 roi &lt;- GRanges(seqnames=&quot;chr1&quot;, ranges=100000:1100000) subsetByOverlaps(se, roi) ## class: RangedSummarizedExperiment ## dim: 50 6 ## metadata(0): ## assays(1): counts ## rownames: NULL ## rowData names(1): feature_id ## colnames(6): A B ... E F ## colData names(1): timepoint "],
["visualizing-and-summarizing-genomic-intervals.html", "4.5 Visualizing and summarizing genomic intervals", " 4.5 Visualizing and summarizing genomic intervals Data integration and visualization is corner stone of genomic data analysis. Below, we will show different ways of integrating and visualizing genomic intervals. These methods can be use to visualize large amounts of data in a locus-specific or multi-loci manner. 4.5.1 Visualizing intervals on a locus of interest Often times, we will be interested in particular genomic locus and try to visualize different genomic datasets over that locus. This is similar to looking at the data over one of the genome browsers. Below we will display genes, GpG islands and read coverage from a ChIP-seq experiment using Gviz package.For Gviz, we first need to set the tracks to display. The tracks can be in various formats. They can be R objects such as IRanges,GRanges and data.frame, or they can be in flat file formats such as BigWig,BED and BAM. After the tracks are set, we can display them with plotTracks function. library(Gviz) # set tracks to display # set CpG island track cpgi.track=AnnotationTrack(cpgi.gr, name = &quot;CpG&quot;) # set gene track # we will get this from EBI Biomart webservice gene.track &lt;- BiomartGeneRegionTrack(genome = &quot;hg19&quot;, chromosome = &quot;chr21&quot;, start = 27698681, end = 28083310, name = &quot;ENSEMBL&quot;) # set track for ChIP-seq coverage chipseqFile=system.file(&quot;extdata&quot;, &quot;wgEncodeHaibTfbsA549.chr21.bw&quot;, package=&quot;compGenomRData&quot;) cov.track=DataTrack(chipseqFile,type = &quot;l&quot;, name=&quot;coverage&quot;) # call the display function plotTracks track.list=list(cpgi.track,gene.track,cov.track) plotTracks(track.list,from=27698681,to=28083310,chromsome=&quot;chr21&quot;) FIGURE 4.6: tracks visualized using Gviz 4.5.2 Summaries of genomic intervals on multiple loci Looking at data one region at a time could be inefficient. One can summarize different data sets over thousands of regions of interest and identify patterns. This summaries can include different data types such as motifs, read coverage and other scores associated with genomic intervals. The genomation package can summarize and help identify patterns in the datasets. The datasets can have different kinds of information and multiple file types can be used such as BED, GFF, BAM and bigWig. We will look at H3K4me3 ChIP-seq and DNAse-seq signals from H1 embryonic stem cell line. H3K4me3 is usually associated with promoters and regions with high DNAse-seq signal are associated with accessible regions, that means mostly regulatory regions. We will summarize those datasets around the transcription start sites (TSS) of genes on chromosome 20 of human hg19 assembly. We will first read the genes and extract the region around TSS, 500bp upstream and downstream. We will then create a matrix of ChIP-seq scores for those regions, each row will represent a region around a specific TSS and columns will be the scores per base. We will then plot, average enrichment values around the TSSes of genes on chromosome 20. # get transcription start sites on chr20 library(genomation) transcriptFile=system.file(&quot;extdata&quot;, &quot;refseq.hg19.chr20.bed&quot;, package=&quot;compGenomRData&quot;) feat=readTranscriptFeatures(transcriptFile, remove.unusual = TRUE, up.flank = 500, down.flank = 500) prom=feat$promoters # get promoters from the features # get for H3K4me3 values around TSSes # we use strand.aware=TRUE so - strands will # be reversed H3K4me3File=system.file(&quot;extdata&quot;, &quot;H1.ESC.H3K4me3.chr20.bw&quot;, package=&quot;compGenomRData&quot;) sm=ScoreMatrix(H3K4me3File,prom, type=&quot;bigWig&quot;,strand.aware = TRUE) # look for the average enrichment plotMeta(sm, profile.names = &quot;H3K4me3&quot;, xcoords = c(-500,500), ylab=&quot;H3K4me3 enrichment&quot;,dispersion = &quot;se&quot;, xlab=&quot;bases around TSS&quot;) FIGURE 4.7: meta region plot using genomation The pattern we see is expected, there is a dip just around TSS and signal is more intense on the downstream of the TSS. We can also plot a heatmap where each row is a region around TSS and color coded by enrichment. This can show us not only the general pattern as in the meta-region plot but also how many of the regions produce such a pattern. heatMatrix(sm,order=TRUE,xcoords = c(-500,500),xlab=&quot;bases around TSS&quot;) FIGURE 4.8: Heatmap of enrichment of H3K4me2 around TSS Here we saw that about half of the regions do not have any signal. In addition it seems the multi-modal profile we have observed earlier is more complicated. Certain regions seems to have signal on both sides of the TSS, whereas others have signal mostly on the downstream side. Normally, there would be more than one experiment or we can integrate datasets from public repositories. In this case, we can see how different signals look like on the regions we are interested in. Now, we will also use DNAse-seq data and create a list of matrices with our datasets and plot the average profile of the signals from both datasets. DNAseFile=system.file(&quot;extdata&quot;, &quot;H1.ESC.dnase.chr20.bw&quot;, package=&quot;compGenomRData&quot;) sml=ScoreMatrixList(c(H3K4me3=H3K4me3File, DNAse=DNAseFile),prom, type=&quot;bigWig&quot;,strand.aware = TRUE) plotMeta(sml) FIGURE 4.9: Average profiles of DNAse and H3K4me3 ChIP-seq We should now look at the heatmaps side by side and we should also cluster the rows based on their similarity. We will be using multiHeatMatrix since we have multiple ScoreMatrix objects in the list. In this case, we will also use winsorize argument to limit extreme values, every score above 95th percentile will be equalized the the value of the 95th percentile. In addition, heatMatrix and multiHeatMatrix can cluster the rows. Below, we will be using k-means clustering with 3 clusters. set.seed(1029) multiHeatMatrix(sml,order=TRUE,xcoords = c(-500,500), xlab=&quot;bases around TSS&quot;,winsorize = c(0,95), matrix.main = c(&quot;H3K4me3&quot;,&quot;DNAse&quot;), column.scale=TRUE, clustfun=function(x) kmeans(x, centers=3)$cluster) (#fig:Heatmaps of H3K4me3 &amp; DNAse data)multiHeatMatrix This revealed a different picture than we have observed before. Almost half of the promoters have no signal for DNAse or H3K4me3; these regions are probably not active and associated genes are not expressed. For regions with H3K4me3 signal, there are two major patterns. One pattern where both downstream and upstream of the TSS are enriched. On the other pattern, mostly downstream of the TSS is enriched. 4.5.3 Making karyograms and circos plots Chromosomal karyograms and circos plots are beneficial for displaying data over the whole genome of chromosomes of interest. Although,the information that can be displayed over these large regions are usually not very clear and only large trends can be discerned by eye, such as loss of methylation in large regions or genome-wide. Below, we are showing how to use ggbio package for plotting. This package has a slightly different syntax than base graphics. The syntax follows grammar of graphics logic. It is a deconstructed way of thinking about the plot. You add your data and apply mappings and transformations in order to achieve the final output. In ggbio, things are relatively easy since a high-level function autoplot function will recognize most of the datatypes and guess the most appropriate plot type. You can change it is behavior by applying low-level functions. We first get the sizes of chromosomes and make a karyogram template. library(ggbio) data(ideoCyto, package = &quot;biovizBase&quot;) p &lt;- autoplot(seqinfo(ideoCyto$hg19), layout = &quot;karyogram&quot;) p FIGURE 4.10: Karyogram example Next, we would like to plot CpG islands on this Karyogram. We simply do this by adding a layer with layout_karyogram function. # read CpG islands from a generic text file CpGiFile=filePath=system.file(&quot;extdata&quot;, &quot;CpGi.hg19.table.txt&quot;, package=&quot;compGenomRData&quot;) cpgi.gr=genomation::readGeneric(CpGiFile, chr = 1, start = 2, end = 3,header=TRUE, keep.all.metadata =TRUE,remove.unusual=TRUE ) p + layout_karyogram(cpgi.gr) FIGURE 4.11: Karyogram of CpG islands Next, we would like to plot some data over the chromosomes. This could be ChIP-seq signal or any other signal over the genome, we will use CpG island scores from the data set we read earlier. We will plot a point proportional to “obsExp” column in the data set. We use ylim argument to squish the chromosomal rectangles and plot on top of those. aes argument defines how the data is mapped to geometry. In this case, it says the points will have x coordinate from CpG island start positions and y coordinate from obsExp score of CpG islands. p + layout_karyogram(cpgi.gr, aes(x= start, y = obsExp), geom=&quot;point&quot;, ylim = c(2,50), color = &quot;red&quot;, size=0.1,rect.height=1) FIGURE 4.12: Karyogram of CpG islands observed/expected scores Another way to depict regions or quantitative signals on the chromosomes is circos plots. These are circular plots usually used for showing chromosomal rearrangements, but can also be used for depicting signals.ggbio package can produce all kinds of circos plots. Below, we will show how to use that for our CpG island score example. # set the chromsome in a circle # color set to white to look transparent p &lt;- ggplot() + layout_circle(ideoCyto$hg19, geom = &quot;ideo&quot;, fill = &quot;white&quot;, colour=&quot;white&quot;,cytoband = TRUE, radius = 39, trackWidth = 2) # plot the scores as points p &lt;- p + layout_circle(cpgi.gr, geom = &quot;point&quot;, grid=TRUE, size = 0.01, aes(y = obsExp),color=&quot;red&quot;, radius = 42, trackWidth = 10) # set the chromosome names p &lt;- p + layout_circle(as(seqinfo(ideoCyto$hg19),&quot;GRanges&quot;), geom = &quot;text&quot;, aes(label = seqnames), vjust = 0, radius = 55, trackWidth = 7, size=3) # display the plot p FIGURE 4.13: circos plot for CpG islands scores "],
["exercises-2.html", "4.6 Exercises", " 4.6 Exercises &gt; dir() [1] &quot;GenomicInterval.exercises.html&quot; The data for the exercises is located at GenomicIntervals_data/data folder. Run the following to see the data files. dir(&quot;GenomicIntervals_data/data&quot;) 4.6.1 Operations on Genomic Intervals with GenomicRanges package 4.6.1.1 Create a GRanges object using the information in the table below: chr start end strand score chr1 10000 10300 + 10 chr1 11100 11500 - 20 chr2 20000 20030 + 15 4.6.1.2 use start(), end(), strand(),seqnames() and width() functions on the GRanges object you created. Figure out what they are doing. Can you get a subset of GRanges object for intervals that are only on + strand? If you can do that, try getting intervals that are on chr1. HINT: GRanges objects can be subset using operator similar to data.frames but you may need to use start(), end() and strand(),seqnames() within the . 4.6.1.3 Import mouse (mm9 assembly) CpG islands and refseq transcripts for chr12 from UCSC browser as GRanges objects using rtracklayer functions. HINT: Check the lecture material and modify the code there as necessary. If that somehow does not work, go to UCSC browser and download it as a BED file. The trackname for Refseq genes is “RefSeq Genes” and table name is “refGene”. 4.6.1.4 Following from the exercise above, get the promoters of Refseq transcripts (-1000bp and +1000 bp of the TSS) and calculate what percentage of them overlap with CpG islands. HINT: You have to get the promoter coordinates and use findOverlaps() or subsetByOverlaps() from GenomicRanges package. To get promoters, type ?promoters on the R console and see how to use that function to get promoters or calculate their coordinates as shown in the lecture material. 4.6.1.5 Plot the distribution of CpG island lengths for CpG islands that overlap with the promoters. 4.6.1.6 Get canonical peaks for SP1 (peaks that are in both replicates) on chr21. Peaks for each replicate are located in GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep1.broadPeak.gz and GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep2.broadPeak.gz files. HINT: You need to use findOverlaps() or subsetByOverlaps() to get the subset of peaks that occur in both replicates. You can try to read *broadPeak.gz files using genomation function readBroadPeak, broadPeak is just an extended BED format. EXTRA credit: Try use coverage() and slice() functions to get canonical peaks. library(genomation) rep1=readBroadPeak(&quot;GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep1.broadPeak.gz&quot;) rep2=readBroadPeak(&quot;GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep2.broadPeak.gz&quot;) 4.6.2 Dealing with mapped high-throughput sequencing reads 4.6.2.1 Count the reads overlapping with canonical Sp1 peaks using the BAM file for one of the replicates: GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xAlnRep1.chr21.bam. HINT: Use functions from GenomicAlignments, see lecture notes. 4.6.3 Dealing with contiguous scores over the genome 4.6.3.1 Extract Views object for the promoters on chr20 from GenomicIntervals_data/data/H1.ESC.H3K4me1.chr20.bw file. Plot the first “View” as a line plot. HINT: see lecture notes, adapt the code from there. 4.6.3.2 Make a histogram of the maximum signal for the Views in the object you extracted above. You can use any of the view summary functions or use lapply() and write your own summary function. 4.6.3.3 Get the genomic positions of maximum signal in each view and make a GRanges object. HINT: See ?viewRangeMaxs help page. Try to make a GRanges object out of the returned object. 4.6.4 Visualizing and summarizing genomic intervals 4.6.4.1 Extract -500,+500 bp regions around TSSes on chr21, there are refseq files in the GenomicIntervals_data/data folder or you can pull the data out of UCSC browser. Use SP1 ChIP-seq data (GenomicIntervals_data/data/wgEncodeHaibTfbsGm12878Sp1Pcr1xAlnRep1.chr21.bam ) to create an average profile of read coverage around TSSes. Following that, visualize the read coverage with a heatmap. HINT: All of these possible using genomation package functions. 4.6.4.2 Extract -500,+500 bp regions around TSSes on chr20. Use H3K4me3 (GenomicIntervals_data/data/H1.ESC.H3K4me3.chr20.bw) and H3K27ac (GenomicIntervals_data/data/H1.ESC.H3K27ac.chr20.bw) ChIP-seq enrichment data in the data folder and create heatmaps and average signal profiles for regions around the TSSes. 4.6.4.3 Visualize one of the -500,+500 bp regions around TSS using Gviz functions. You should visualize both H3K4me3 and H3K27ac and the gene models. "],
["processingReads.html", "Chapter 5 Quality check, processing and alignment of high-throughput sequencing reads", " Chapter 5 Quality check, processing and alignment of high-throughput sequencing reads Advances in sequencing technology are helping researchers sequence the genome deeper than ever. These sequencing experiments typically yield millions of reads. These reads have to be further processed, quality checked and aligned before we can quantify the genomic signal of interest and apply statistics and/or machine learning methods. For example, you may want to count how many reads overlapping with your promoter set of interest or you may want to quantify RNA-seq reads overlapping with exons. Post-alignment operations are usually but not always similar to operations on genomic intervals. Dealing with mapped reads are described previously in chapter 4. In addition, we have introduced high-throughput sequencing and its applications in general in chapter 1. In this chapter we will introduce the fundamentals of read processing and quality check, and we will show how to do those tasks in R. For a long time, quality check and mapping tasks were outside the R domain. However, nowadays certain packages in R/Bioconductor can accomplish those tasks. "],
["fasta-and-fastq-formats.html", "5.1 FASTA and FASTQ formats", " 5.1 FASTA and FASTQ formats High-throughput sequencing reads are usually output from sequencing facilities as text files in a format called “FASTQ” or “fastq”. This format depends on an earlier format called FASTA. The FASTA format is developed as a text-based format to represent nucleotide or protein sequences (See Figure 5.1 for an example). FIGURE 5.1: An example fasta file showing first part of PAX6 gene The first line in a FASTA file usually starts with a “&gt;” (greater-than) symbol. This first line is called the “description line”, and can contain descriptive information about the sequence in the subsequent lines. The description can be id or name of the sequence such as gene names. However, very infrequently you may see lines starting with a “;” (semicolon). These lines will be taken as a comment, and can hold additional descriptive information about the sequence in subsequent lines. An extension of the FASTA format is FASTQ format. This format is designed to handle base quality metrics output from sequencing machines. In this format, both the sequence and quality scores are represented as single ASCII characters. The format uses for lines for each sequence, and these four lines are stacked on top of each other in text files output by sequencing workflows. Each of the 4 lines will represent a read. Figure 5.2 shows those four lines with brief explanations for each line. FIGURE 5.2: FASTQ format and brief explanation of each line in the format Line 1 begins with a ‘@’ character and is followed by a sequence identifier and an optional description. This line is utilized by the sequencing technology, and usually contains specific information for the technology. It can contain flow cell ids, lane numbers, information on read pairs. Line 2 is the sequence letters. Line 3 begins with a ‘+’ character, it marks the end of sequence and is optionally followed by the same sequence identifier again in line 1. Line 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence. Each letter corresponds to a quality score. Although there might be different definitions of the quality scores, a de facto standard in the field is to use “Phred quality scores”. These scores represent the likelihood of base being called wrong. Formally, \\({\\displaystyle Q_{\\text{phred}}=-10\\log _{\\text{10}}e}\\), where \\(e\\) is probability that the base is called wrong.Since the score is in minus log scale, the higher the score, the more unlikely that the base is called wrong. "],
["quality-check-on-sequencing-reads.html", "5.2 Quality check on sequencing reads", " 5.2 Quality check on sequencing reads The sequencing technologies usually produce basecalls with varying quality. In addition, there could be sample specific issues in your sequencing run, such as adapter contamination. It is standard procedure to check the quality of the reads and identify problems before doing further analysis. Checking the quality and making some decisions for the downstream analysis can influence the outcome of your project. Below, we will walk you through the quality check steps using Rqc package. First, we need to feed fastq files to rqc() function and obtain an object with sequence quality related results. We are using example fastq files from ShortRead package. library(Rqc) folder = system.file(package=&quot;ShortRead&quot;, &quot;extdata/E-MTAB-1147&quot;) # feeds fastq.qz files in &quot;folder&quot; to quality check function qcRes=rqc(path = folder, pattern = &quot;.fastq.gz&quot;, openBrowser=FALSE) 5.2.1 Sequence quality per base/cycle Now that we have qcRes object, we can plot various sequence quality metrics for our fastq files. We will first plot &quot; sequence quality per base/cycle“. This plot depicts the quality scores across all bases at each position in the reads. rqcCycleQualityBoxPlot(qcRes) FIGURE 5.3: Per base sequence quality boxplot In our case, the x-axis in the plot is labeled as “cycle”. This is because in each sequencing “cycle” a fluorescently labeled nucleotide is added to complement the template sequence, and the sequencing machine identifies which nucleotide is added. Therefore, cycles corresponds to bases/nucleotides along the read, and the number of cycles is equivalent to the read length. Long sequences can have degraded quality towards the ends of the reads. Looking at quality distribution over base positions can help us decide to do trimming towards the end of the reads or not. A good sample will have median quality scores per base above 28. If scores are below 20 towards the ends, you can think about trimming the reads. 5.2.2 Sequence content per base/cycle Per base sequence content shows nucleotide proportions for each position. In a random sequencing library there should be no nucleotide bias and the lines should be almost parallel with each other. rqcCycleBaseCallsLinePlot(qcRes) FIGURE 5.4: Per base sequence quality boxplot However some types of sequencing libraries can produce a biased sequence composition. For example, in RNA-Seq , it is common to have bias at the beginning of the reads. This happens because of random primers annealing to the start of reads during RNA-Seq library preparation. These primers are not truly random, and it leads to a variation at the beginning of the reads. Although RNA-seq experiments will usually have these biases, this will not affect the ability of measuring gene expression. In addition, some libraries are inherently biased in their sequence composition. For example, in bisulfite sequencing experiments most of the cytosines will be converted to thymines. This will create a difference in C and T base compositions over the read, however this type of difference is normal for bisulfite sequencing experiments. 5.2.3 Read frequency plot This plot shows the degree of duplication for every read in the library. A high level of duplication, non-unique reads, is likely to indicate an enrichment bias. Technical duplicates arising from PCR artefacts could cause this. PCR is a common step in library preparation which creates many copies of the sequence fragment. In RNA-seq data, non-unique read proportion can reach more than 20%. However, these duplications may stem from simply genes being expressed at high levels. This means that there will many copies of transcripts and many copies of the same fragment. Since, we can not be sure these duplicated reads are due to PCR bias or an effect of high transcription we should not remove duplicated reads in RNA-seq analysis. However, in ChIP-seq experiments duplicated reads are more likely to be due to PCR bias. rqcReadFrequencyPlot(qcRes) FIGURE 5.5: Per base sequence quality boxplot 5.2.4 Other quality metrics and QC tools Over-represented k-mers along the reads can be an additional check. If there are such sequences it may point to adapter contamination and should be trimmed. Adapters are known sequences that are added to the ends of the reads. This kind of contamination could also be visible at “sequence content per base” plots. In addition, if you know the adapter sequences you can match it to the end of the reads and trim them. The most popular tool for sequencing quality control is the fastQC tool (“Babraham Bioinformatics - FastQC A Quality Control Tool for High Throughput Sequence Data” 2018), which is written in Java. It produces the plots that we described above in addition to k-mer overrepresentation and adapter overrepresentation plots. The R package fastqcr can run this Java tool and produce R based plots and reports. This package simply calls the Java tool and parses its results. Below, we are showing how to do that. library(fastqcr) # install the FASTQC java tool fastqc_install() # call FASTQC and record the resulting statistics # in fastqc_results folder fastqc(fq.dir = folder,qc.dir = &quot;fastqc_results&quot;) Now that we have run FastQC on our fastq files, we can read the results to R and construct plots or reports. gc_report function can create an Rmarkdown based report from FastQC output. # view the report rendered by R functions qc_report(qc.path=&quot;fastqc_results&quot;, result.file=&quot;reportFile&quot;, preview = TRUE) Alternatively, we can read the results with qc_read() and make specific plots we are interested in with qc_plot(). # read QC results to R for one fastq file qc &lt;- qc_read(&quot;fastqc_results/ERR127302_1_subset_fastqc.zip&quot;) # make plots, example &quot;Per base sequence quality plot&quot; qc_plot(qc, &quot;Per base sequence quality&quot;) Apart from this, the bioconductor packages Rqc (de Souza, Carvalho, and Lopes-Cendes 2018) (see Rqc::rqcReport function), QuasR (Gaidatzis et al. 2015) (see QuasR::qQCReport function), systemPipeR (Backman and Girke 2016) (see systemPipeR::seeFastq function), and ShortRead (Morgan et al. 2009) (see ShortRead::report function) packages can all generate quality reports in a similar fashion to FastQC with some differences in plot content and number. References "],
["filtering-and-trimming-reads.html", "5.3 Filtering and trimming reads", " 5.3 Filtering and trimming reads Based on the results of the quality check, you may want to trim or filter the reads. Quality check might have shown number of reads that have low quality scores. These reads will probably not align very well because of the potential mistakes in base calling, or they may align to wrong places in the genome. Therefore, you may want to remove these reads from your fastq file. Another potential scenario is that part of your reads needs to be trimmed in order align the read. In some cases, adapters will be present in either side of the read, in other cases technical errors will lead to decreasing base quality towards the ends of the reads. Both in these cases, portion of the read should be trimmed so that read can align or better align the genome. We will show how to use QuasR package to trim the reads. Other packages such as ShortRead also have capabilities to trim and filter reads. However, QuasR::preprocessReads() function provides a single interface to multiple preprocessing possibilities. With this function, we match adapter sequences and remove them. We can remove low-complexity reads (reads containing repetitive sequences). We can trim start or ends of the reads by a pre-defined length. Below we will first set up the file paths to fastq files and filter them based on their length and whether or not they contain “N” character, which stands for unidentified base. With the same function we will also trim 3 bases from the end of the reads and also trim segments from the start of the reads if they match the “ACCCGGGA” sequence. library(QuasR) # obtain a list of fastq file paths fastqFiles &lt;- system.file(package=&quot;ShortRead&quot;, &quot;extdata/E-MTAB-1147&quot;, c(&quot;ERR127302_1_subset.fastq.gz&quot;, &quot;ERR127302_2_subset.fastq.gz&quot;) ) # defined processed fastq file names outfiles &lt;- paste(tempfile(pattern=c(&quot;processed_1_&quot;, &quot;processed_2_&quot;)),&quot;.fastq&quot;,sep=&quot;&quot;) # process fastq files # remove reads that have more than 1 N, (nBases) # trim 3 bases from the end of the reads (truncateEndBases) # Remove ACCCGGGA patern if it occurs at the start (Lpattern) # remove reads shorter than 40 base-pairs (minLength) preprocessReads(fastqFiles, outfiles, nBases=1, truncateEndBases=3, Lpattern=&quot;ACCCGGGA&quot;, minLength=40) As we have mentioned, ShortRead package has low-level functions, which QuasR::preprocessReads() also depends on. We can use these low level functions to filter reads in ways that are not possible using QuasR::preprocessReads() function. Below we are going to read in a fastq file and filter the reads where every quality score is below 20. library(ShortRead) # obtain a list of fastq file paths fastqFile &lt;- system.file(package=&quot;ShortRead&quot;, &quot;extdata/E-MTAB-1147&quot;, &quot;ERR127302_1_subset.fastq.gz&quot;) # read fastq file fq = readFastq(fastqFile) # get quality scores per base as a matrix qPerBase = as(quality(fq), &quot;matrix&quot;) # get number of bases per read that have quality score below 20 # we use this qcount = rowSums( qPerBase &lt;= 20) # Number of reads where all Phred scores &gt;= 20 fq[qcount == 0] ## class: ShortReadQ ## length: 10699 reads; width: 72 cycles We can finally write out the filtered fastq file with ShortRead::writeFastq() function. # write out fastq file with only reads where all # quality scores per base are above 20 writeFastq(fq[qcount == 0], paste(fastqFile, &quot;Qfiltered&quot;, sep=&quot;_&quot;)) As fastq files can be quite large, it may not be feasible to read a 30 Gigabyte file into memory. A more memory efficient way would be to read the file piece by piece. We can do our filtering operations for each piece, write the filtered part out and read a new piece. Fortunately, this is possible by ShortRead::FastqStreamer() function. This function enables “streaming” the fastq file in pieces, which are blocks of the fastq file with a pre-defined number of reads . We can access the successive blocks with yield() function. Each time we call yield() function after opening the fastq file with FastqStreamer(), a new part of the file will be read to the memory. # set up streaming with block size 1000 # every time we call the yield() function 1000 read portion # of the file will be read successively. f &lt;- FastqStreamer(fastqFile,readerBlockSize=1000) # we set up a while loop to call yield() function to # go through the file while(length(fq &lt;- yield(f))) { # remove reads where all quality scores are &lt; 20 # get quality scores per base as a matrix qPerBase = as(quality(fq), &quot;matrix&quot;) # get number of bases per read that have Q score &lt; 20 qcount = rowSums( qPerBase &lt;= 20) # write fastq file with mode=&quot;a&quot;, so every new block # is written out to the same file writeFastq(fq[qcount == 0], paste(fastqFile, &quot;Qfiltered&quot;, sep=&quot;_&quot;), mode=&quot;a&quot;) } "],
["mappingaligning-reads-to-the-genome.html", "5.4 Mapping/aligning reads to the genome", " 5.4 Mapping/aligning reads to the genome After quality check and potential pre-processing, the reads are ready to be mapped or aligned to the reference genome. This process simply finds most probable the origin of each read in the genome. Since there might be errors in sequencing and mutations in the genomes, we may not find exact matches of reads in the genomes. An important feature of the alignment algorithms is to tolerate potential mismatches between reads and the reference genome. In addition, effienct algorithms and data structures are needed for the alignment to be completed in a reasonable amount of time. Alignment methods usually create data structures to store and efficiently search the genome for matching reads. These data structures are called genome indices and creating these indices is the first step for the read alignment. Based on how indices are created, there are two major types of methods. One class of methods rely on “hash tables”, to store and search the genomes. Hash tables are simple lookup tables, in which all possible k-mers point to locations in the genome. The general idea is that overlapping k-mers constructed from a read goes through this look up table. Each k-mer points to potential locations in the genome. Then, final location for the read is obtained by optimizing k-mer chain by their distances in the genome and in the read. This optimization process removes k-mer locations that are distant from other k-mers that map nearby each other. Another class of algorithms build genome indices by creating Burrows-Wheeler transformation of the genome. This in essence creates a compact and searchable data structure for all reads. Although, details are out of scope for this section, these alignment tools provide faster alignment and use less memory. BWA(H. Li and Durbin 2009), Bowtie1/2(Langmead and Salzberg 2012) and SOAP(R. Li et al. 2009) are examples of such algorithms. The read mapping in R can be done with gmapR (Barr, Wu, and Lawrence 2019), QuasR (Gaidatzis et al. 2015), Rsubread (Liao, Smyth, and Shi 2013), and systemPipeR (Backman and Girke 2016) packages. We will demonstrate read mapping with QuasR which uses Rbowtie package, which wraps the Bowtie aligner. Below, we show how to map reads from a ChIP-seq experiment using QuasR/bowtie. We will use qAlign() function which requires two mandatory arugments: 1) a genome file either in fasta format or as a BSgenome package 2) a sample file which is a text file and contains file paths to fastq files and sample names. In the case, below sample file looks like this: FileName SampleName chip_1_1.fq.bz2 Sample1 chip_2_1.fq.bz2 Sample2 library(QuasR) # copy example data to current working directory file.copy(system.file(package=&quot;QuasR&quot;, &quot;extdata&quot;), &quot;.&quot;, recursive=TRUE) # genome file in fasta format genomeFile &lt;- &quot;extdata/hg19sub.fa&quot; # text file containing sample names and fastq file paths sampleFile &lt;- &quot;extdata/samples_chip_single.txt&quot; # create alignments proj &lt;- qAlign(sampleFile, genomeFile) It is good to explain what is going on here as the qAlign() function makes things look simple. This function is designed to be easy. For example, it creates a genome index automatically if it does not exist, and will look for existing indices before it creates one. We provided only two arguments, a text file containing sample names and fastq file paths and a reference genome file. In fact, this function also has many knobs and you can change its behavior by supplying different arguments in order to affect the behavior of Bowtie. For example, you can supply parameters to Bowtie using alignmentParameter argument. However the qAlign() function is optimized for different types of alignment problems and selects alignment parameters automatically. It is designed to work with alignment and quantification tasks for RNA-seq, ChIP-seq, small-RNA sequencing, Bisulfite sequencing (DNA methylation) and allele specific analysis. If you want to change default bowtie parameters only do it for simple alignment problems such as ChIP-seq and RNA-seq. Want to know more ? More on hash tables and Burrows-Wheeler based aligners A survey of sequence alignment algorithms for next-generation sequencing H Li, N Homer - Briefings in bioinformatics, 2010 More on QuasR and all the alignment and post-processing capabilities. References "],
["further-processing-of-aligned-reads.html", "5.5 Further processing of aligned reads", " 5.5 Further processing of aligned reads After alignment some further processing might be necessary. However, these steps are usually sequencing protocol specific. For example, for methylation C-&gt;T mismatches should be counted. For gene expression measurements, reads that overlap with transcripts should be counted. These further processing tasks are either done by specialized alignment related software or can be done in R. We will explain these further processing steps when they become relevant in the context of following chapters. "],
["exercises-3.html", "Chapter 6 Exercises", " Chapter 6 Exercises TBD "],
["rna-seq-analysis-overview.html", "Chapter 7 RNA-seq analysis overview", " Chapter 7 RNA-seq analysis overview RNA sequencing (RNA-seq) has proven as a revolutionary tool since the time it has been introduced. The throughput, accuracy, and resolution of data produced with RNA-seq has been instrumental in the study of transcriptomics in the last decade (Z. Wang, Gerstein, and Snyder 2009). There is a variety of applications of transcriptome sequencing and each application may consist of different chains of tools each with many alternatives (Conesa et al. 2016). In this chapter, we are going to demonstrate a common workflow of how to do differential expression analysis with downstream applications such as GO term and gene set enrichment analysis. We assume that the sequencing data was generated using one of the NGS sequencing platforms. Where applicable, we will try to provide alternatives to the reader in terms of both the tools to carry out a demonstrated analysis and also the other applications of the same sequencing data depending on the different biological questions. References "],
["what-is-gene-expression.html", "7.1 What is gene expression?", " 7.1 What is gene expression? Gene expression is a term used to describe the contribution of a gene to the overall functions and phenotype of a cell through the activity of the molecular products, which are encoded in the specific nucleotide sequence of the gene. RNA is the primary product encoded in a gene, which is transcribed in the nucleus of a cell. A class of RNA molecules, messenger RNAs, are transported from the nucleus to the cytoplasm, where the translation machinery of the cell translates the nucleotide sequence of the mRNA into proteins. The functional protein repertoire in a given cell is the primary factor that dictates the shape, function, and phenotype of a cell. Due to the prime roles of proteins for a cell’s fate, most molecular biology literature is focused on protein-coding genes. However, a bigger proportion of a eukaryotic gene repertoire is reserved for non-coding genes, which code for RNA molecules that are not translated into proteins, yet carry out many important cellular functions. All in all, the term gene expression refers to the combined activity of protein-coding or non-coding products of a gene. In a cell, there are many layers of quality controls and modifications that act upon a gene’s product until the end-product attains a particular function. These layers of regulation include epigenetic, transcriptional, post-transcriptional, translational, and post-translational control mechanisms, the latter two applying only to protein-coding genes. A protein or RNA molecule, is only functional if it is produced at the right time, at the right cellular compartment, with the neccessary base or amino-acid modifications, with the correct secondary/tertiary structure (or unstructure wherever applicable), among the availability of other metabolites or molecules, which are needed to form complexes to altogether accomplish a certain cellular function. However, traditionally, the number of copies of a gene’s products is considered a quantitative measure of a gene’s activity. Although this approach does not reflect all of the complexity of what defines a functional molecule, quantification of the abundance of transcripts from a gene has proven to be a cost-effective method in understanding genes’ functions. "],
["methods-to-detect-gene-expression.html", "7.2 Methods to detect gene expression", " 7.2 Methods to detect gene expression Quantification of how much expression levels of genes deviate from a baseline gives clues about which genes are actually important for, for instance, disease outcome or cell/tissue identity. The methods of detecting and quantifying gene expression has evolved from low-throughput methods such as the usage of a reporter gene with a flourescent protein product to find out if a single gene is expressed at all, to high-throughput methods such as massively parallel RNA-sequencing that can profile -at a single-nucleotide resolution- the abundance of tens of thousands of distinct transcripts encoded in the largest eukaryotic genomes. "],
["gene-expression-analysis-using-high-throughput-sequencing-technologies.html", "7.3 Gene Expression Analysis Using High-throughput Sequencing Technologies", " 7.3 Gene Expression Analysis Using High-throughput Sequencing Technologies With the advent of the second-generation (a.k.a next-generation or high-throughput) sequencing technologies, the number of genes that can be profiled for expression levels with a single experiment has increased to the order of tens of thousands of genes. Therefore, the bottleneck in this process has become the data analysis rather than the data generation. Many statistical methods and computational tools are required for getting meaningful results from the data, which comes with a lot of valuable information along with a lot of sources of noise. Fortunately, most of the steps of RNA-seq analysis have become quite mature over the years. Below we will first describe how to reach a read count table from raw fastq reads obtained from an Illumina sequencing run. We will then demonstrate in R how to process the count table, make a case-control differential expression analysis, and do some downstream functional enrichment analysis. 7.3.1 Processing raw data 7.3.1.1 Quality check The first step in any experiment that involves high-throughput short-read sequencing should be to check the sequencing quality of the reads before starting to do any downstream analysis. The quality of the input sequences holds fundamental importance in the confidence for the biological conclusions drawn from the experiment. Probably the most popular tool for sequencing quality control is the fastQC tool (“Babraham Bioinformatics - FastQC A Quality Control Tool for High Throughput Sequence Data” 2018), which is written in Java. It produces plots and tables that summarize the quality of the sequencing data with respect to a variety of metrics such as per base sequencing quality scores and over-represented sequences that might suggest adapter sequence contamination or amplification biases. There are also similar libraries available in R to do sequencing quality assessments. For instance, the bioconductor packages Rqc (de Souza, Carvalho, and Lopes-Cendes 2018) (see Rqc::rqcReport function), QuasR (Gaidatzis et al. 2015) (see QuasR::qQCReport function), systemPipeR (Backman and Girke 2016) (see systemPipeR::seeFastq function), and ShortRead (Morgan et al. 2009) (see ShortRead::report function) packages can all generate quality reports in a similar fashion to fastQC. 7.3.1.2 Improving the quality The second step in the RNA-seq analysis workflow is to improve the quality of the input reads. This step could be regarded as an optional step when the sequencing quality is very good. However, even with the highest quality sequencing datasets, this step may still improve the quality of the input sequences. The most common technical artifacts that can be filtered out are the adapter sequences that contaminate the sequenced reads, and the low quality bases that are usually found at the ends of the sequences. Commonly used tools in the field (trimmomatic (Bolger, Lohse, and Usadel 2014), trimGalore (“Babraham Bioinformatics - Trim Galore!” 2018)) are again not written in R, however there are alternative R libraries for carrying out the same functionality, for instance, QuasR (Gaidatzis et al. 2015) (see QuasR::preprocessReads function) and ShortRead (Morgan et al. 2009) (see ShortRead::filterFastq function). The sequencing quality control and read pre-processing steps can be visited multiple times until achieving a satisfactory level of quality in the sequence data before moving onto the dowstream analysis steps. 7.3.2 Alignment Once a decent level of quality in the sequences is reached, the expression level of the genes can be quantified by first mapping the sequences to a reference genome, and secondly matching the aligned reads to the gene annotations, in order to count the number of reads mapping to each gene. If the species under study has a well annotated transcriptome, the reads can be aligned to the transcript sequences instead of the reference genome. In cases where there is no good quality reference genome or transcriptome, it is possible to de novo assemble the transcriptome from the sequences and then quantify the expression levels of genes/transcripts. For RNA-seq read alignments, apart from the availability of reference genomes and annotations, probably the most important factor to consider when choosing an alignment tool is whether the alignment method considers the absence of intronic regions in the sequenced reads, while the target genome may contain introns. Therefore, it is important to choose alignment tools that take into account alternative splicing. In the basic setting where a read, which originates from a cDNA sequence corresponding to an exon-exon junction, needs to be split into two parts when aligned against the genome. There are various tools that consider this factor such as STAR (Dobin et al. 2013), Tophat2 (Kim et al. 2013), Hisat2 (Kim, Langmead, and Salzberg 2015), GSNAP (T. D. Wu et al. 2016). Most alignment tools are written in C/C++ languages because of performance concerns. There are also R libraries that can do short read alignments, however, they would still require the existence of one of the alignment tools in the execution environment. Examples of R libraries that can run short-read alignments using one of these tools are gmapR (Barr, Wu, and Lawrence 2019), QuasR (Gaidatzis et al. 2015), Rsubread (Liao, Smyth, and Shi 2013), and systemPipeR (Backman and Girke 2016). 7.3.3 Quantification After the reads are aligned to the target, a SAM/BAM file sorted by coordinates should have been obtained. The BAM file contains all alignment related information of all the reads that have been attempted to be aligned to the target sequence. This information consists of - most basically - the genomic coordinates (chromosome, start, end, strand) of where a sequence was matched (if at all) in the target, specific insertions/deletions/mismatches that describes the differences between the input and target sequences. These pieces of information are used along with the genomic coordinates of genome annotations such as gene/transcript models in order to count how many reads have been sequenced from a gene/transcript. As simple as it may sound, it is not a trivial task to assign reads to a gene/transcript just by comparing the genomic coordinates of the annotations and the sequences, because of the confounding factors such as overlapping gene annotations, overlapping exon annotations from different transcript isoforms of a gene, overlapping annotations from opposite DNA strands in the absence of a strand-specific sequencing protocol. Therefore, for read counting, it is important to consider: Strand specificity of the sequencing protocol: are the reads expected to originate from the forward strand, reverse strand, or unspecific? Counting mode: when counting at the gene-level: when there are overlapping annotations, which features should the read be assigned to? Tools usually have a parameter that lets the user to select a counting mode. when counting at the transcript-level: when there are multiple isoforms of a gene, which isoform should the read be assigned to? This consideration is usually an algorithmic consideration that is not modifiable by the end-user. Some tools can couple alignment to quantification (e.g. STAR), while some assume the alignments are already calculated and require BAM files as input. On the other hand, in the presence of good transcriptome annotations, alignment-free methods (Salmon (Patro et al. 2017), Kallisto (Bray et al. 2016), Sailfish (Patro, Mount, and Kingsford 2014)) can also be used to estimate the expression levels of transcripts/genes. There are also reference-free quantification methods that can first de novo assemble the transcriptome and estimate the expression levels based on this assembly. Such a strategy can be useful in discovering novel transcripts or may be required in cases when a good reference does not exist. If a reference transcriptome exists but of low quality, a reference-based transcriptome assembler such as Cufflinks (Trapnell et al. 2010) can be used to improve the transcriptome. In case there is no available transcriptome annotation, a de novo assembler such as Trinity (Haas et al. 2013) or Trans-ABySS (Robertson et al. 2010) can be used to assemble the transcriptome from scratch. Within R, quantification can be done using Rsubread::featureCounts QuasR::qCount GenomicAlignments::summarizeOverlaps 7.3.4 Normalization of read counts The most common application after a gene’s expression is quantified (as the number of reads aligned to the gene), is to compare the gene’s expression in different conditions, for instance, in a case-control setting (e.g. disease versus normal) or in a time-series (e.g. along different developmental stages). Making such comparisons help identify the genes that might be responsible for a disease or an impaired developmental trajectory. However, there are multiple caveats that needs to be addressed before making a comparison between the read counts of a gene in different conditions (Maza et al. 2013) Library size (i.e. sequencing depth) varies between samples coming from different lanes of the flow cell of the sequencing machine. Longer genes will have higher number of reads. Library composition (i.e. relative size of the studied transcriptome) can be different in two different biological conditions. GC content biases across different samples may lead to a biased sampling of genes (Risso et al. 2011). Read coverage of a transcript can be biased and non-uniformly distributed along the transcript (Mortazavi et al. 2008). Therefore these factors need to be taken into account before making comparisons. The most basic normalization approaches address the sequencing depth bias. Such procedures normalize the read counts per gene by dividing each gene’s read count by a certain value and multiplying it by 10^6. These normalized values are usually referred to as CPM (counts per million reads): Total Counts Normalization (divide counts by the sum of all counts) Upper Quartile Normalization (divide counts by the upper quartile value of the counts) Median Normalization (divide counts by the median of all counts) Popular metrics that improve upon CPM are RPKM/FPKM (reads/fragments per kilobase of million reads) and TPM (transcripts per million). RPKM is obtained by dividing the CPM value by another factor, which is the length of the gene per kilobases. FPKM is the same as RPKM, but is used for paired-end reads. Thus, RPKM/FPKM methods account for, firstly the library size, and secondly the gene lengths. TPM also controls for both the library size and the gene lengths, however, with the TPM method, the read counts are first normalized by the gene length (per kilobase), and then gene-length normalized values are divided by the sum of the gene-length normalized values and multiplied by 10^6. Thus, the sum of normalized values for TPM will always be equal to 10^6 for each library, while the sum of RPKM/FPKM values do not sum to 10^6. Therefore, it is easier to interpret TPM values than RPKM/FPKM values. 7.3.5 Demonstration in R Here we will assume that there is an RNA-seq count table comprising of raw-counts, meaning the number of reads counted for each gene has not been exposed to any kind of normalization and consists of integers. The rows of the count table correspond to the genes and the columns represent different samples. Here we will use a subset of the RNA-seq count table from a colorectal cancer study. We have filtered the original count table for only protein-coding genes (to improve the speed of calculation) and also selected only five metastasized colorectal cancer samples along with five normal colon samples. There is an additional column width that contains the length of the corresponding gene in the unit of base pairs. The length of the genes are important to compute RPKM and TPM values. The original count tables can be found from the recount2 database [REF] using the SRA project code SRP029880 and the experimental setup along with other accessory information can be found from the NCBI Trace archive using the SRA project code SRP029880 here. #colorectal cancer counts_file &lt;- system.file(&quot;extdata/rna-seq/SRP029880.raw_counts.tsv&quot;, package = &quot;compGenomRData&quot;) coldata_file &lt;- system.file(&quot;extdata/rna-seq/SRP029880.colData.tsv&quot;, package = &quot;compGenomRData&quot;) counts &lt;- as.matrix(read.table(counts_file, header = T, sep = &#39;\\t&#39;)) 7.3.5.1 Computing CPM Let’s do a summary of the counts table: summary(counts) ## CASE_1 CASE_2 ## Min. :0.00e+00 Min. :0.00e+00 ## 1st Qu.:5.16e+03 1st Qu.:6.46e+03 ## Median :8.00e+04 Median :8.51e+04 ## Mean :2.96e+05 Mean :2.73e+05 ## 3rd Qu.:2.52e+05 3rd Qu.:2.45e+05 ## Max. :2.05e+08 Max. :1.05e+08 ## CASE_3 CASE_4 ## Min. :0.00e+00 Min. :0.00e+00 ## 1st Qu.:3.97e+03 1st Qu.:7.09e+03 ## Median :6.41e+04 Median :9.91e+04 ## Mean :2.63e+05 Mean :3.65e+05 ## 3rd Qu.:2.11e+05 3rd Qu.:3.03e+05 ## Max. :2.23e+08 Max. :1.62e+08 ## CASE_5 CTRL_1 ## Min. :0.00e+00 Min. :0.00e+00 ## 1st Qu.:5.74e+03 1st Qu.:6.90e+03 ## Median :8.55e+04 Median :8.68e+04 ## Mean :3.46e+05 Mean :3.39e+05 ## 3rd Qu.:2.74e+05 3rd Qu.:2.86e+05 ## Max. :1.27e+08 Max. :2.73e+08 ## CTRL_2 CTRL_3 ## Min. :0.00e+00 Min. :0.00e+00 ## 1st Qu.:4.75e+03 1st Qu.:4.76e+03 ## Median :6.31e+04 Median :6.15e+04 ## Mean :2.73e+05 Mean :2.64e+05 ## 3rd Qu.:2.15e+05 3rd Qu.:2.12e+05 ## Max. :3.99e+08 Max. :2.82e+08 ## CTRL_4 CTRL_5 ## Min. :0.00e+00 Min. :0.00e+00 ## 1st Qu.:2.72e+03 1st Qu.:4.56e+03 ## Median :3.72e+04 Median :6.23e+04 ## Mean :1.83e+05 Mean :2.76e+05 ## 3rd Qu.:1.34e+05 3rd Qu.:2.19e+05 ## Max. :2.63e+08 Max. :2.92e+08 ## width ## Min. : 117 ## 1st Qu.: 9668 ## Median : 27090 ## Mean : 67685 ## 3rd Qu.: 70374 ## Max. :2473537 To compute the CPM values for each sample (excluding the width column): cpm &lt;- apply(subset(counts, select = c(-width)), 2, function(x) x/sum(as.numeric(x)) * 10^6) head(cpm) ## CASE_1 CASE_2 CASE_3 CASE_4 CASE_5 ## TSPAN6 133.0525 69.0267 118.0347 63.45320 75.1706 ## TNMD 0.2541 0.1497 0.5774 0.04131 0.1603 ## DPM1 62.5345 50.9434 47.9547 51.61492 47.6836 ## SCYL3 17.7536 18.1283 18.9681 16.32642 14.9281 ## C1ORF112 15.5608 10.9995 11.8489 15.14763 6.5660 ## FGR 9.7817 27.0412 5.1668 23.44216 23.2660 ## CTRL_1 CTRL_2 CTRL_3 CTRL_4 CTRL_5 ## TSPAN6 83.6818 90.8358 63.7171 66.1201 116.5649 ## TNMD 0.6926 0.3495 0.8603 0.5436 0.7309 ## DPM1 31.5817 22.8556 21.7976 18.6671 36.4578 ## SCYL3 24.0552 19.8292 20.5182 17.9990 18.6608 ## C1ORF112 7.8228 6.3511 7.3078 5.1346 10.6632 ## FGR 8.9318 4.5419 4.1459 5.1712 4.3533 Check that the sum of each column after normalization equals to 10^6 (except the width column). colSums(cpm) ## CASE_1 CASE_2 CASE_3 CASE_4 CASE_5 CTRL_1 CTRL_2 ## 1e+06 1e+06 1e+06 1e+06 1e+06 1e+06 1e+06 ## CTRL_3 CTRL_4 CTRL_5 ## 1e+06 1e+06 1e+06 7.3.5.2 Computing RPKM # create a vector of gene lengths geneLengths &lt;- as.vector(subset(counts, select = c(width))) # compute rpkm rpkm &lt;- apply(X = subset(counts, select = c(-width)), MARGIN = 2, FUN = function(x) 10^9 * x / geneLengths / sum(as.numeric(x))) head(rpkm) ## CASE_1 CASE_2 CASE_3 CASE_4 CASE_5 ## TSPAN6 10.32776 5.357969 9.16205 4.925343 5.83487 ## TNMD 0.01685 0.009922 0.03828 0.002739 0.01063 ## DPM1 2.63981 2.150508 2.02434 2.178856 2.01290 ## SCYL3 0.39773 0.406127 0.42494 0.365760 0.33443 ## C1ORF112 0.08101 0.057267 0.06169 0.078864 0.03418 ## FGR 0.42137 1.164864 0.22257 1.009828 1.00224 ## CTRL_1 CTRL_2 CTRL_3 CTRL_4 CTRL_5 ## TSPAN6 6.49552 7.05083 4.94583 5.13235 9.04796 ## TNMD 0.04592 0.02317 0.05704 0.03604 0.04845 ## DPM1 1.33318 0.96482 0.92016 0.78801 1.53902 ## SCYL3 0.53891 0.44423 0.45967 0.40323 0.41806 ## C1ORF112 0.04073 0.03307 0.03805 0.02673 0.05552 ## FGR 0.38476 0.19565 0.17860 0.22276 0.18753 Check the sample sizes of RPKM. Notice that the sums of samples are all different colSums(rpkm) ## CASE_1 CASE_2 CASE_3 CASE_4 CASE_5 CTRL_1 CTRL_2 ## 158291 153324 161775 173047 172761 210033 301764 ## CTRL_3 CTRL_4 CTRL_5 ## 241418 291675 252006 7.3.5.3 Computing TPM #find gene length normalized values rpk &lt;- apply( subset(counts, select = c(-width)), 2, function(x) x/(geneLengths/1000)) #normalize by the sample size using rpk values tpm &lt;- apply(rpk, 2, function(x) x / sum(as.numeric(x)) * 10^6) head(tpm) ## CASE_1 CASE_2 CASE_3 CASE_4 CASE_5 ## TSPAN6 65.2454 34.94535 56.6344 28.46239 33.77413 ## TNMD 0.1064 0.06471 0.2366 0.01583 0.06153 ## DPM1 16.6770 14.02588 12.5133 12.59109 11.65132 ## SCYL3 2.5127 2.64881 2.6267 2.11364 1.93581 ## C1ORF112 0.5118 0.37350 0.3813 0.45573 0.19787 ## FGR 2.6620 7.59739 1.3758 5.83556 5.80130 ## CTRL_1 CTRL_2 CTRL_3 CTRL_4 CTRL_5 ## TSPAN6 30.9263 23.36535 20.4865 17.59617 35.9038 ## TNMD 0.2186 0.07678 0.2363 0.12356 0.1923 ## DPM1 6.3475 3.19727 3.8115 2.70166 6.1071 ## SCYL3 2.5658 1.47212 1.9040 1.38247 1.6589 ## C1ORF112 0.1939 0.10958 0.1576 0.09165 0.2203 ## FGR 1.8319 0.64836 0.7398 0.76373 0.7441 Check the sample sizes of tpm. Notice that the sums of samples are all equal to 10^6. colSums(tpm) ## CASE_1 CASE_2 CASE_3 CASE_4 CASE_5 CTRL_1 CTRL_2 ## 1e+06 1e+06 1e+06 1e+06 1e+06 1e+06 1e+06 ## CTRL_3 CTRL_4 CTRL_5 ## 1e+06 1e+06 1e+06 None of these metrics (CPM, RPKM/FPKM, TPM) account for the other important confounding factor when comparing expression levels of genes across samples: the library composition, which may also be referred to as the relative size of the compared transcriptomes. This factor is not dependent on the sequencing technology, it is rather biological. For instance, when comparing transcriptomes of different tissues, there can be sets of genes in one tissue that consume a big chunk of the reads, while in the other tissue not expressed at all. This kind of imbalances in the composition of compared transcriptomes can lead to wrong conclusions about which genes are actually differentially expressed. This consideration is addressed in two popular R packages: DESeq2 (Love, Huber, and Anders 2014) and edgeR (Robinson, McCarthy, and Smyth 2010) each with a different algorithm. edgeR uses a normalization procecure called Trimmed Mean of M-values (TMM). DESeq2 implements a normalization procedure using Median of Ratios, which is obtained by finding the ratio of log-transformed count of a gene divided by the average of log-transformed values of the gene in all samples (geometric mean), and then taking the median of these values for all genes. The raw read count of the gene is finally divided by this value (median of ratios) to obtain the normalized counts of genes. In order to see a demonstration of DESeq2’s normalization method, see insert section id about differential expression analysis. 7.3.6 Exploratory analysis of the read count table A typical quality control, in this case interrogating the RNA-seq experiment design, is to measure the similariy of the samples with each other in terms of the quantified expression level profiles accross a set of genes. One important observation to make is to see, whether the most similar samples to any given sample are the biological replicates of that sample. This can be computed using unsupervised clustering techniques such as hierarchical clustering and visualized as a heatmap with dendrograms. Another most commonly applied technique is a dimensionality reduction technique called Principal Component Analysis (PCA) and visualized as a two-dimensional (or in some cases three-dimensional) scatter plot. In order to find out more about the clustering methods and PCA, please refer to sections insert section and insert section, respectively. 7.3.6.1 Clustering We can combine clustering and visualization of the clustering results by using heatmap functions that are available in a variety of R libraries. The basic R installation comes with the stats::heatmap function. However, there are other libraries available in CRAN (e.g. pheatmap (Kolde 2019)) or Bioconductor (e.g. ComplexHeatmap (Gu, Eils, and Schlesner 2016)) that come with more flexibility and more appealing visualisations. Here we demonstrate a heatmap using pheatmap package and the previously calculated tpm matrix. As these matrices can be quite large, both computing the clustering and rendering the heatmaps can take a lot of resources and time. Therefore, a quick and informative way to compare samples is to select a subset of genes that are, for instance, most variable across samples, and use that subset to do the clustering and visualization. Let’s select top 100 most variable genes among the samples. #compute the variance of each gene across samples V &lt;- apply(tpm, 1, var) #sort the results by variance in decreasing order and select the top 100 genes selectedGenes &lt;- names(V[order(V, decreasing = T)][1:100]) Now we can quickly produce a heatmap where samples and genes are clustered pheatmap::pheatmap(tpm[selectedGenes,], scale = &#39;row&#39;, show_rownames = FALSE) We can also overlay some annotation tracks to observe the clusters. Here it is important to observe whether the replicates of the same sample cluster most closely with each other, or not. Overlaying the heatmap with such annotation and displaying sample groups with distinct colors helps quickly see if there are samples that don’t cluster as expected. colData &lt;- read.table(coldata_file, header = T, sep = &#39;\\t&#39;, stringsAsFactors = TRUE) pheatmap::pheatmap(tpm[selectedGenes,], scale = &#39;row&#39;, show_rownames = FALSE, annotation_col = colData) 7.3.6.2 PCA Let’s make a PCA plot to see the clustering of replicates as a scatter plot in two dimensions. #transpose the matrix M &lt;- t(tpm[selectedGenes,]) # transform the counts to log2 scale M &lt;- log2(M + 1) #compute PCA pcaResults &lt;- stats::prcomp(M) #plot PCA results making use of ggplot2&#39;s autoplot function #ggfortify is needed to let ggplot2 know about PCA data structure. ggplot2::autoplot(pcaResults, data = colData, colour = &#39;group&#39;) We should observe here whether the samples from the case group (CASE) and samples from the control group (CTRL) can be split into two distinct clusters on the scatter plot of the first two largest principal components. We can use the summary function to summarize the PCA results to observe the contribution of the principal components in the explained variation. summary(pcaResults) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 24.396 2.5051 2.39327 1.93841 ## Proportion of Variance 0.957 0.0101 0.00921 0.00604 ## Cumulative Proportion 0.957 0.9671 0.97627 0.98231 ## PC5 PC6 PC7 PC8 ## Standard deviation 1.79193 1.6357 1.46059 1.30902 ## Proportion of Variance 0.00516 0.0043 0.00343 0.00276 ## Cumulative Proportion 0.98747 0.9918 0.99520 0.99796 ## PC9 PC10 ## Standard deviation 1.12657 4.41e-15 ## Proportion of Variance 0.00204 0.00e+00 ## Cumulative Proportion 1.00000 1.00e+00 7.3.6.3 Correlation plots Another complementary approach to see the reproducibility of the experiments is to compute the correlation scores between each pair of samples and draw a correlation plot. Let’s first compute pairwise correlation scores between every pair of samples. correlationMatrix &lt;- stats::cor(tpm) Let’s have a look at how the correlation matrix looks: knitr::kable(correlationMatrix) CASE_1 CASE_2 CASE_3 CASE_4 CASE_5 CTRL_1 CTRL_2 CTRL_3 CTRL_4 CTRL_5 CASE_1 1.0000 0.9925 0.9959 0.9934 0.9902 0.9594 0.9636 0.9550 0.9549 0.9303 CASE_2 0.9925 1.0000 0.9887 0.9936 0.9898 0.9726 0.9794 0.9675 0.9739 0.9448 CASE_3 0.9959 0.9887 1.0000 0.9951 0.9929 0.9649 0.9618 0.9556 0.9552 0.9401 CASE_4 0.9934 0.9936 0.9951 1.0000 0.9923 0.9740 0.9749 0.9643 0.9710 0.9511 CASE_5 0.9902 0.9898 0.9929 0.9923 1.0000 0.9632 0.9661 0.9502 0.9587 0.9313 CTRL_1 0.9594 0.9726 0.9649 0.9740 0.9632 1.0000 0.9880 0.9902 0.9874 0.9864 CTRL_2 0.9636 0.9794 0.9618 0.9749 0.9661 0.9880 1.0000 0.9820 0.9970 0.9642 CTRL_3 0.9550 0.9675 0.9556 0.9643 0.9502 0.9902 0.9820 1.0000 0.9851 0.9903 CTRL_4 0.9549 0.9739 0.9552 0.9710 0.9587 0.9874 0.9970 0.9851 1.0000 0.9739 CTRL_5 0.9303 0.9448 0.9401 0.9511 0.9313 0.9864 0.9642 0.9903 0.9739 1.0000 We can also draw more visually appealing correlation plots using the corrplot package. # The correlation plot order by the results of the hierarchical clustering corrplot::corrplot(correlationMatrix, order = &#39;hclust&#39;) # The clusters are split into 2 groups by adding rectangles around the computed clusters. corrplot::corrplot(correlationMatrix, order = &#39;hclust&#39;, addrect = 2) # We can add the pairwise correlation scores on the plot corrplot::corrplot(correlationMatrix, order = &#39;hclust&#39;, addrect = 2, addCoef.col = &#39;white&#39;) Here pairwise correlation levels are visualized as colored circles. Blue indicates positive correlation, while Red indicates negative correlation. We could also plot this correlation matrix as a heatmap: # basic correlation heatmap pheatmap::pheatmap(correlationMatrix) # add annotations on the columns pheatmap::pheatmap(correlationMatrix, annotation_col = colData) # split the clusters into two based on the clustering similarity pheatmap::pheatmap(correlationMatrix, annotation_col = colData, cutree_cols = 2) 7.3.6.4 Exercises Basic: Re-do the heatmaps using top 500 most variable genes. Compare with the heatmap obtained using 100 most variable genes. Re-do the heatmaps setting scale argument to none, and column. Compare the results with scale = 'row'. Draw a correlation plot for the samples depicting the sample differences as ‘ellipses’, drawing only the upper end of the matrix, order samples by hierarchical clustering results based on average linkage clustering method. Intermediate: How else could the count matrix be subsetted to obtain quick and accurate clusters? Try selecting top 100 genes that have the highest total expression in all samples and re-draw the cluster heatmaps and PCA plots. Add an additional column to the annotation data.frame object to annotate the samples and use the updated annotation data.frame to plot the heatmaps. (Hint: assign different batch values to CASE and CTRL samples). Make a PCA plot and color samples by the added variable (e.g. batch). Advanced: Try making the heatmaps using all the genes in the count table, rather than sub-selecting. Use tsne package [REF] to draw a t-SNE plot of the expression values. Color the points by sample group. Compare the results with the PCA plots. 7.3.7 Differential expression analysis Differential expression analysis allows to test tens of thousands of hypotheses (one test for each gene) against the null hypothesis that the activity of the gene stays the same in two different conditions. There are multiple limiting factors that influence the power of detecting genes that have real changes between two biological conditions. Among these are the limited number of biological replicates, non-normality of the distribution of the read counts, and higher uncertainty of measurements for lowly expressed genes than highly expressed genes (Love, Huber, and Anders 2014). Tools such as edgeR and DESeq2 address these limitations using sophisticated statistical models in order to maximize the amount of knowledge that can be extracted from such noisy datasets. DESeq2: Normalizes the read counts by computing size factors, which addresses the differences not only in the library sizes, but also the library compositions. For each gene, estimates the dispersion value of the gene among the biological replicates. The dispersion value computed by DESeq2 is equal to the squared coefficient of variation (variation divided by the mean). A line is fitted across the dispersion estimates of all genes computed in 2) versus the mean normalized counts of the genes. Dispersion values of each gene is shrunken towards the fitted line in 3). A Generalized Linear Model is fitted which considers additional confounding variables related to the experimental design such as sequencing batches, treatment, temperature, patient’s age, sequencing technology etc. For a given contrast (e.g. treatment type: drug-A versus untreated), a test for differential expression is carried out against the null hypothesis that the log fold change of the normalized counts of the gene in the given pair of groups is exactly zero. Adjusts p-values for multiple-testing. In order to carry out a differential expression analysis using DESeq2, three kinds of inputs are necessary: The read count table: must be raw read counts as integers that are not processed in any form by a normalization technique. The rows represent features (e.g. genes, transcripts, genomic intervals) and columns represent samples. A colData table: this table describes the experimental design. A design formula: this formula is needed to describe the variable of interest in the analysis (e.g. treatment status) along with (optionally) other covariates (e.g. batch, temperature, sequencing technology). Let’s define these inputs: #remove the &#39;width&#39; column countData &lt;- as.matrix(subset(counts, select = c(-width))) #define the experimental setup colData &lt;- read.table(coldata_file, header = T, sep = &#39;\\t&#39;, stringsAsFactors = TRUE) #define the design formula designFormula &lt;- &quot;~ group&quot; Now, we are ready to run DESeq2. #create a DESeq dataset object from the count matrix and the colData dds &lt;- DESeq2::DESeqDataSetFromMatrix(countData = countData, colData = colData, design = stats::as.formula(designFormula)) #print dds object to see the contents print(dds) ## class: DESeqDataSet ## dim: 19719 10 ## metadata(1): version ## assays(1): counts ## rownames(19719): TSPAN6 TNMD ... MYOCOS HSFX3 ## rowData names(0): ## colnames(10): CASE_1 CASE_2 ... CTRL_4 CTRL_5 ## colData names(2): source_name group The DESeqDataSet object contains all the information about the experimental setup, the read counts, and the design formulas. Certain functions can be used to access these information separately: rownames(dds) shows which features are used in the study (e.g. genes), colnames(dds) displays the studied samples, counts(dds) displays the count table, colData(dds) displays the experimental setup. Remove genes that have almost no information in any of the given samples. #For each gene, we count the total number of reads for that gene in all samples #and remove those that don&#39;t have at least 1 read. dds &lt;- dds[ rowSums(counts(dds)) &gt; 1, ] Now, we can use the DESeq function of DESeq2, which is a wrapper function that implements estimation of size factors to normalize the counts, estimation of dispersion values, and computing a GLM model based on the experimental design formula. This function returns a DESeqDataSet object, which is an updated version of the dds variable that we pass to the function as input. dds &lt;- DESeq2::DESeq(dds) Now, we can compare and contrast the samples based on different variables of interest. In this case, we currently have only one variable, which is the group variable that determines if a sample belongs to the CASE group or the CTRL group. #compute the contrast for the &#39;group&#39; variable where &#39;CTRL&#39; samples are used as the control group. DEresults = DESeq2::results(dds, contrast = c(&quot;group&quot;, &#39;CASE&#39;, &#39;CTRL&#39;)) #sort results by increasing p-value DEresults &lt;- DEresults[order(DEresults$pvalue),] Thus we have obtained a table containing the differential expression status of case samples compared to the control samples. It is important to note that the sequence of the elements provided in the contrast argument determines which group of samples are to be used as control. This impacts the way the results are interpreted, for instance, if a gene is found up-regulated (has a positive log2 fold change), the up-regulation status is only relative to the factor that is provided as control. In this case, we used samples from the CTRL group as control and contrasted the samples from the CASE group with respect to the CTRL samples. Thus genes with a positive log2 fold change are called up-regulated in the case samples with respect to the control, while genes with a negative log2 fold change are down-regulated in the case samples. Whether the deregulation is significant or not, warrants assessment of the adjusted p-values. Let’s have a look into the contents of the DEresults table. #shows a summary of the results print(DEresults) ## log2 fold change (MLE): group CASE vs CTRL ## Wald test p-value: group CASE vs CTRL ## DataFrame with 19097 rows and 6 columns ## baseMean log2FoldChange ## &lt;numeric&gt; &lt;numeric&gt; ## CYP2E1 4829888.92231712 9.36024435305687 ## FCGBP 10349993.1789843 -7.57578665890778 ## ASGR2 426421.665773775 8.01830102661517 ## GCKR 100182.667004114 7.82841275978615 ## APOA5 438053.955454865 10.2024847632603 ## ... ... ... ## CCDC195 20.4981398045639 -0.215606612258398 ## SPEM3 23.6369514506586 -22.1547650648436 ## AC022167.5 21.8450828694606 -2.05624008484309 ## BX276092.9 29.963634159758 0.407325618882749 ## ETDC 22.5674885843896 -1.79527360789704 ## lfcSE stat ## &lt;numeric&gt; &lt;numeric&gt; ## CYP2E1 0.215223173019895 43.4908761065038 ## FCGBP 0.18643292339242 -40.6354549456998 ## ASGR2 0.216206795652736 37.0862581002954 ## GCKR 0.233376221798576 33.5441747212052 ## APOA5 0.312502755796718 32.6476633373977 ## ... ... ... ## CCDC195 2.8925516729234 -0.0745385516451266 ## SPEM3 3.02784692796743 -7.317003003093 ## AC022167.5 2.89545307238825 -0.710161772073565 ## BX276092.9 2.8904762978251 0.140919895862573 ## ETDC 2.89421003612133 -0.620298314735642 ## pvalue padj ## &lt;numeric&gt; &lt;numeric&gt; ## CYP2E1 0 0 ## FCGBP 0 0 ## ASGR2 4.6789767784482e-301 2.87741475285303e-297 ## GCKR 1.09479013375331e-246 5.04944579440372e-243 ## APOA5 8.64905801850778e-234 3.191329427669e-230 ## ... ... ... ## CCDC195 NA NA ## SPEM3 NA NA ## AC022167.5 NA NA ## BX276092.9 NA NA ## ETDC NA NA The first three lines in this output shows the contrast and the statistical test that were used to compute these results, along with the dimensions of the resulting table (number of columns and rows). Below these lines is the actual table with 6 columns: baseMean represents the average normalized expression of the gene across all considered samples. log2FoldChange represents the base-2 logarithm of the fold change of the normalized expression of the gene in the given contrast. lfcSE represents the standard error of log2 fold change estimate, and stat is the statistic calculated in the contrast which is translated into a pvalue and adjusted for multiple testing in the padj column. To find out about the importance of adjusting for multiple testing, refer to section insert section for multiple testing. 7.3.7.1 Diagnostic plots At this point, before proceeding to do any downstream analysis and jumping to conclusions about the biological insights that are reachable with the experimental data at hand, it is important to do some more diagnostic tests to improve our confidence about the quality of the data and the experimental setup. An MA plot is useful to observe if the data normalisation worked well. MA plot is a scatterplot where x axis denotes the average of normalized counts across samples and the y axis denotes the log fold change in the given contrast. Most points are expected to be on the horizontal 0 line (most genes are expected to be not differentially expressed). 7.3.7.1.1 MA plot DESeq2::plotMA(object = dds, ylim = c(-5, 5)) It is also important to observe the distribution of raw p-values. We expect to see a peak around low p-values and a uniform distribution at p-values above 0.1. Otherwise, adjustment for multiple testing does not work and the results are not meaningful. 7.3.7.1.2 p-value distribution ggplot(data = as.data.frame(DEresults), aes(x = pvalue)) + geom_histogram(bins = 100) A final diagnosis is to check the biological reproducibility of the sample replicates in a PCA plot or a heatmap. To plot the PCA results, we need to extract the normalized counts from the DESeqDataSet object. It is possible to color the points in the scatterplot by the variable of interest, which helps to see if the replicates cluster well. 7.3.7.1.3 PCA plot # extract normalized counts from the DESeqDataSet object countsNormalized &lt;- DESeq2::counts(dds, normalized = TRUE) # select top 500 most variable genes selectedGenes &lt;- names(sort(apply(countsNormalized, 1, var), decreasing = TRUE)[1:500]) plotPCA(countsNormalized[selectedGenes,], col = as.numeric(colData$group)) Alternatively, the normalized counts can be transformed using DESeq2::rlog function and DESeq2::plotPCA can be readily used to plot the PCA results. rld &lt;- DESeq2::rlog(dds) DESeq2::plotPCA(rld, ntop = 500, intgroup = &#39;group&#39;) 7.3.7.1.4 Relative Log Expression (RLE) plot A similar plot to the MA plot is the RLE (Relative Log Expression) plot that is useful in finding out if the data at hand needs normalization (Gandolfo and Speed 2018). Sometimes, even the datasets normalized using the explained methods above may need further normalization due to unforeseen sources of variation that might stem from the library preparation, the person who carries out the experiment, the date of sequencing, the temperature changes in the laboratory at the time of library preparation, and so on and sofort. RLE plot is a quick diagnostic that can be applied on the raw or normalized count matrices to see if further processing is required. Let’s do RLE plots on the raw counts and normalized counts using the EDASeq package (Risso et al. 2011). par(mfrow = c(1, 2)) plotRLE(countData, outline=FALSE, ylim=c(-4, 4), col=as.numeric(colData$group), main = &#39;Raw Counts&#39;) plotRLE(counts(dds, normalized = TRUE), outline=FALSE, ylim=c(-4, 4), col = as.numeric(colData$group), ylim = c(-4,4), main = &#39;Normalized Counts&#39;) Here the RLE plot is comprised of box plots, where each box-plot represents the distribution of the relative log expression of the genes expressed in the corresponding sample. Each gene’s expression is divided by the median expression value of that gene across all samples. Then this is transformed to log scale, which gives the relative log expression value for a single gene. The RLE values for all the genes from a sample is visualized as a boxplot. Ideally the boxplots are centered around the horizontal zero line and are as tightly distributed as possible (Risso et al. 2014). From the plots that we have made for the raw and normalized count data, we can observe how the normalized dataset has improved upon the raw count data for all the samples. However, in some cases, it is important to visualize RLE plots in combination with other diagnostic plots such as PCA plots, heatmaps, and correlation plots to see if there is more unwanted variation in the data, which can be further accounted for using packages such as RUVseq (Risso et al. 2014) and sva (Leek et al. 2012). See about how to use RUVSeq package to account for unwanted sources of noise in the count tables. 7.3.7.2 Exercises Basic Make a volcano plot using the differential expression analysis results. (Hint: x axis denotes the log2FoldChange and the y-axis represents the -log10(pvalue)). Use DESeq2::plotDispEsts to make a dispersion plot and find out the meaning of this plot. (Hint: type ?DESeq2::plotDispEsts) Intermediate Re-do the differential expression analysis using edgeR package. Find out how much DESeq2 and edgeR agree on the list of differentially expressed genes. Advanced Use compcodeR package to run the differential expression analysis using at least three different tools and compare and contrast the results following the compcodeR vignette. 7.3.8 Functional Enrichment Analysis 7.3.8.1 GO term analysis In a typical differential expression analysis, thousands of genes are found differentially expressed between two groups of samples. While prior knowledge of the functions of individual genes can give some clues about what kind of cellular processes have been affected, e.g. by a drug treatment, manually going through the whole list of thousands of genes would be very cumbersome and not be very informative in the end. Therefore a commonly used tool to address this problem is to do enrichment analyses of functional terms that appear associated to the given set of differentially expressed genes more often than expected by chance. The functional terms usually are associated to multiple genes. Thus, genes can be grouped into sets by shared functional terms. However, it is important to have an agreed upon controlled vocabulary on the list of terms used to describe the functions of genes. Otherwise, it would be impossible to exchange scientific results globally. That’s why initiatives such as Gene Ontology Consortium have collated a list of Gene Ontology (GO) terms for each gene. GO term enrichment analysis is probably the most common analysis applied after a differential expression analysis. GO term analysis helps quickly find out systematic changes that can describe differences between groups of samples. In R, the simplest way to do enrichment analysis is via the gProfileR package. Let’s select the genes that are significantly differentially expressed between the case and control samples. Let’s remove genes that have an adjusted p-value below 0.1 and that show a 2-fold change (either negative or positive) in the case compared to control. # extract differential expression results DEresults &lt;- DESeq2::results(dds, contrast = c(&#39;group&#39;, &#39;CASE&#39;, &#39;CTRL&#39;)) #remove genes with NA values DE &lt;- DEresults[!is.na(DEresults$padj),] #select genes with adjusted p-values below 0.1 DE &lt;- DE[DE$padj &lt; 0.1,] #select genes with absolute log2 fold change above 1 (two-fold change) DE &lt;- DE[abs(DE$log2FoldChange) &gt; 1,] #get the list of genes of interest genesOfInterest &lt;- rownames(DE) #calculate enriched GO terms goResults &lt;- gProfileR::gprofiler(query = genesOfInterest, organism = &#39;hsapiens&#39;, src_filter = &#39;GO&#39;, hier_filtering = &#39;moderate&#39;) # sort the enriched GO terms by pvalue and print the top 10 terms # for the selected columns from the go results knitr::kable(goResults[order(goResults$p.value), c(3:4, 7, 10, 12)][1:10,]) p.value term.size precision domain term.name 53 0 2740 0.223 CC plasma membrane part 31 0 1609 0.136 BP ion transport 26 0 3656 0.258 BP regulation of biological quality 37 0 385 0.042 BP extracellular structure organization 12 0 7414 0.452 BP multicellular organismal process 81 0 1069 0.090 MF transmembrane transporter activity 45 0 1073 0.090 BP organic acid metabolic process 47 0 975 0.083 BP response to drug 9 0 1351 0.107 BP biological adhesion 15 0 4760 0.302 BP system development 7.3.8.2 Gene set enrichment analysis A gene set is a collection of genes with some common property. This shared property among a set of genes could be a GO term, a common biological pathway, a shared interaction partner, or any biologically relevant commonality that is meaningful in the context of the pursued experiment. Gene set enrichment analysis (GSEA) is a valuable exploratory analysis tool that can associate systematic changes to a high-level function rather than individual genes. Analysis of coordinated changes of expression levels of gene sets can provide complementary benefits on top of per-gene based differential expression analyses. For instance, consider a gene set belonging to a biological pathway where each member of the pathway displays a slight deregulation in a disease sample compared to a normal sample. In such a case individual genes might not be picked up by the per-gene based differential expression analysis. Thus, the GO/Pathway enrichment on the differentially expressed list of genes would not show an enrichment of this pathway. However, the additive effect of slight changes of the genes could amount to a large effect at the level of the gene set, thus the pathway could be detected as a significant pathway that could explain the mechanistic problems in the disease sample. We use the bioconductor package gage (Luo et al. 2009) to demonstrate how to do GSEA using normalized expression data of the samples as input. Here we are using only two gene sets: one from the top GO term discovered from the previous GO analysis, one that we compile by randomly selecting a list of genes. However, annotated gene sets can be used from databases such as MSIGDB (Subramanian et al. 2005), which compile gene sets from a variety of resources such as KEGG (Kanehisa et al. 2016) and REACTOME (Fabregat et al. 2018). #Let&#39;s define the first gene set as the list of genes from one of the significant GO terms # found in the GO analysis. # order go results by pvalue goResults &lt;- goResults[order(goResults$p.value),] #restrict the terms that have at most 100 genes overlapping with the query go &lt;- goResults[goResults$overlap.size &lt; 100,] # use the top term from this table to create a gene set geneSet1 &lt;- unlist(strsplit(go[1,]$intersection, &#39;,&#39;)) #Define another gene set by just randomly selecting 25 genes from the counts table #get normalized counts from DESeq2 results normalizedCounts &lt;- counts(dds, normalized = TRUE) geneSet2 &lt;- sample(rownames(normalizedCounts), 25) geneSets &lt;- list(&#39;top_GO_term&#39; = geneSet1, &#39;random_set&#39; = geneSet2) Using the defined gene sets, we’d like to do a group comparison between the case samples with respect to the control samples. #use the normalized counts to carry out a GSEA. gseaResults &lt;- gage::gage(exprs = log2(normalizedCounts+1), ref = match(rownames(colData[colData$group == &#39;CTRL&#39;,]), colnames(normalizedCounts)), samp = match(rownames(colData[colData$group == &#39;CASE&#39;,]), colnames(normalizedCounts)), gsets = geneSets, compare = &#39;as.group&#39;) We can observe if there is a significant upregulation or downregulation of the gene set in the case group compared to the controls by accessing gseaResults$greater or gseaResults$less. knitr::kable(gseaResults$greater, caption = &#39;Up-regulation statistics&#39;) TABLE 7.1: Up-regulation statistics p.geomean stat.mean p.val q.val set.size exp1 top_GO_term 0.0000 7.199 0.0000 0.0000 32 0.0000 random_set 0.1578 1.014 0.1578 0.1578 25 0.1578 knitr::kable(gseaResults$less, caption = &#39;Down-regulation statistics&#39;) TABLE 7.2: Down-regulation statistics p.geomean stat.mean p.val q.val set.size exp1 random_set 0.8422 1.014 0.8422 1 25 0.8422 top_GO_term 1.0000 7.199 1.0000 1 32 1.0000 We can see that the random gene set shows no significant up or down-regulation, while the gene set we defined using the top GO term shows a significant down-regulation (adjusted p-value &lt; 0.0007). It is worthwhile to visualize these systematic changes in a heatmap: # get the expression data for the gene set of interest M &lt;- normalizedCounts[rownames(normalizedCounts) %in% geneSet1, ] # log transform the counts for visualization # scaling by row helps visualizing relative change of expression of a gene in multiple conditions pheatmap::pheatmap(log2(M+1), annotation_col = colData, show_rownames = FALSE, scale = &#39;row&#39;, cutree_cols = 2, cutree_rows = 2) We can see that about two thirds of the genes from this gene set have been up-regulated in the case samples compared to the controls. 7.3.8.3 Exercises Basic: Re-run gProfileR, this time using pathway annotations such as KEGG, REACTOME, and protein complex databases such as CORUM, in addition to the GO terms. Sort the resulting tables by columns precision and/or recall. How do the top GO terms change when sorted for precision, recall, or p.value? Repeat the gene set enrichment analysis by trying different options for the compare argument of GAGE:gage function. How do the results differ? Intermediate: Make a scatter plot of GO term sizes and obtained p-values by setting the gProfiler::gprofiler argument significant = FALSE. Is there a correlation of term sizes and p-values? (Hint: take -log10 of p-values). If so, how can this bias be mitigated? Do a gene-set enrichment analysis using gene sets from top 10 GO terms. What are the other available R packages that can carry out gene set enrichment analysis for RNA-seq datasets? Advanced: Use the topGO package (https://bioconductor.org/packages/release/bioc/html/topGO.html) to re-do the GO term analysis. Compare and contrast the results with what has been obtained using gProfileR package. Which tool is faster? gProfileR or topGO? Why? Given a gene set annotated for human, how can it be utilized to work on C. elegans data? (Hint: see biomaRt::getLDS [REF] function or RCAS::retrieveOrthologs [REF] function). Import curated pathway gene sets with Entrez identifiers from the MSIGDB database and re-do the GSEA for all curated gene sets. 7.3.9 Accounting for additional sources of variation When doing a differential expression analysis in a case-control setting, the variable of interest, i.e. the variable that explains the separation of the case samples from the control is usually the treatment, genotypic differences, a certain phenotype so on and sofort. However, in reality, depending on how the experiment and the sequencing was designed, there may be additional factors that might contribute to the variation between the compared samples. Sometimes, such variables are known, for instance, the date of the sequencing for each sample (batch information), the temperature under which samples were kept, etc. Such variables are not necessarily biological but rather technical, however, they still impact the measurements obtained from an RNA-seq experiment. Such variables can introduces systematic shifts in the obtained measurements. Here, we will demonstrate: firstly how to account for such variables using DESeq2, when the possibles sources of variation are actually known; secondly, how to account for such variables when all we have is just a count table but we observe that the variable of interest only explains a small proportion of the differences between case and control samples. 7.3.9.1 Accounting for covariates using DESeq2 For demonstration purposes, we will use a subset of the count table obtained for a heart disease study, where there are RNA-seq samples from subjects with normal and failing hearts. We again use a subset of the samples, focusing on 6 case and 6 control samples and we only consider protein-coding genes (for speed concerns). Let’s import count and colData for this experiment. counts_file &lt;- system.file(&#39;extdata/rna-seq/SRP021193.raw_counts.tsv&#39;, package = &#39;compGenomRData&#39;) colData_file &lt;- system.file(&#39;extdata/rna-seq/SRP021193.colData.tsv&#39;, package = &#39;compGenomRData&#39;) counts &lt;- read.table(counts_file) colData &lt;- read.table(colData_file, header = T, sep = &#39;\\t&#39;, stringsAsFactors = TRUE) Let’s take a look at how the samples cluster by calculating the TPM counts. #find gene length normalized values geneLengths &lt;- counts$width rpk &lt;- apply( subset(counts, select = c(-width)), 2, function(x) x/(geneLengths/1000)) #normalize by the sample size using rpk values tpm &lt;- apply(rpk, 2, function(x) x / sum(as.numeric(x)) * 10^6) selectedGenes &lt;- names(sort(apply(tpm, 1, var), decreasing = T)[1:100]) pheatmap::pheatmap(tpm[selectedGenes,], scale = &#39;row&#39;, annotation_col = colData, show_rownames = FALSE) Here we can see from the clusters that the dominating variable is the ‘Library Selection’ variable rather than the ‘diagnosis’ variable that determines the state of the organ from which the sample was taken. Case and control samples are all mixed in both two major clusters. However, ideally, we’d like to see a separation of the case and control samples regardless of the additional covariates. Luckily, DESeq2 can account for such confounding variables. Below is a demonstration of how we instruct DESeq2 to account for the ‘library selection’ variable: # remove the &#39;width&#39; column from the counts matrix countData &lt;- as.matrix(subset(counts, select = c(-width))) # set up a DESeqDataSet object dds &lt;- DESeq2::DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ LibrarySelection + group) When constructing the design formula, it is very important to pay attention to the sequence of variables. We leave the variable of interest to the last and we can add as many covariates as we want to the beginning of the design formula. Please refer to the DESeq2 vignette if you’d like to learn more about how to construct design formulas. Now, we can run the differential expression analysis as has been demonstrated previously. # run DESeq dds &lt;- DESeq2::DESeq(dds) # extract results DEresults &lt;- DESeq2::results(dds, contrast = c(&#39;group&#39;, &#39;CASE&#39;, &#39;CTRL&#39;)) 7.3.9.2 Accounting for estimated covariates using RUVSeq In cases when the sources of potential variation are not known, it is worthwhile to use tools such as RUVSeq or sva that can estimate potential sources of variation and clean up the counts table from those sources of variation. Later on, the estimated covariates can be integrated into DESeq2’s design formula. Let’s see how to utilize RUVseq package to first diagnose the problem and then solve it. Here, for demonstration purposes, we’ll use a count table from a lung carcinoma study in which a transcription factor (Ets homologous factor - EHF) is overexpressed and compared to the control samples with baseline EHF expression. Again, we only consider protein coding genes and use only five case and five control samples. The original data can be found on the recount2 database with the accession ‘SRP049988’. counts_file &lt;- system.file(&#39;extdata/rna-seq/SRP049988.raw_counts.tsv&#39;, package = &#39;compGenomRData&#39;) colData_file &lt;- system.file(&#39;extdata/rna-seq/SRP049988.colData.tsv&#39;, package = &#39;compGenomRData&#39;) counts &lt;- read.table(counts_file) colData &lt;- read.table(colData_file, header = T, sep = &#39;\\t&#39;, stringsAsFactors = TRUE) Let’s start by making heatmaps of the samples using TPM counts #find gene length normalized values geneLengths &lt;- counts$width rpk &lt;- apply( subset(counts, select = c(-width)), 2, function(x) x/(geneLengths/1000)) #normalize by the sample size using rpk values tpm &lt;- apply(rpk, 2, function(x) x / sum(as.numeric(x)) * 10^6) selectedGenes &lt;- names(sort(apply(tpm, 1, var), decreasing = T)[1:100]) pheatmap::pheatmap(tpm[selectedGenes,], scale = &#39;row&#39;, annotation_col = colData, cutree_cols = 2, show_rownames = FALSE) We can see that the overal clusters look fine except for one of the case samples (CASE_5) clusters more closely with the control samples than the other case samples. This mis-clustering could be a result of some batch effect, or any other technical preparation steps. However, the colData object doesn’t contain any variables that we can directly distinguish this sample from others. So, let’s use RUVSeq to estimate potential covariates to see if the clustering results can be improved. # remove &#39;width&#39; column from counts countData &lt;- as.matrix(subset(counts, select = c(-width))) # create a seqExpressionSet object using EDASeq package set &lt;- EDASeq::newSeqExpressionSet(counts = countData, phenoData = colData) # make an RLE plot and a PCA plot on raw count data and color samples by group par(mfrow = c(1,2)) plotRLE(set, outline=FALSE, ylim=c(-4, 4), col=as.numeric(colData$group)) plotPCA(set, col = as.numeric(colData$group)) ## make RLE and PCA plots on TPM matrix par(mfrow = c(1,2)) plotRLE(tpm, outline=FALSE, ylim=c(-4, 4), col=as.numeric(colData$group)) plotPCA(tpm, col=as.numeric(colData$group), cex=1.2) Both RLE and PCA plots look better on normalized data compared to raw data, but still suggest the necessity of further improvement, because the the CASE_5 sample still clusters with the control samples. We haven’t yet accounted for the source of unwanted variation. 7.3.9.3 Removing unwanted variation from the data RUVSeq has three main functions for removing unwanted variation: RUVg, RUVs, and RUVr. Here, we will demonstrate how to use RUVg and RUVs. RUVr will be left as an exercise for the reader. 7.3.9.3.1 Using RUVg One way of removing unwanted variation is dependent on using a set of reference genes that are not expected to change by the sources of technical variation. One strategy along this line is to use spike-in genes, which are artifically introduced into the sequencing run (Jiang et al. 2011). However, there are many sequencing datasets that don’t have this spike-in data available. In such cases, an emprical set of genes can be collected from the expression data by doing a differential expression analysis and discovering genes that are unchanged in the given conditions. These unchanged genes are used to center the complete expression data to clean up the data from systematic shifts in expression due to the unwanted sources of variation. Another stragety could be to use a set of house-keeping genes as negative controls, and use them as a reference to correct the systematic biases in the data. Let’s use a list of ~500 house-keeping genes compiled here: http://www.stat.berkeley.edu/~johann/ruv/resources/hk.txt. #source for house-keeping genes collection: https://m.tau.ac.il/~elieis/HKG/HK_genes.txt house_keeping_genes &lt;- read.table(file = system.file(&quot;extdata/rna-seq/HK_genes.txt&quot;, package = &#39;compGenomRData&#39;), header = FALSE) # let&#39;s take an intersection of the house-keeping genes with the genes available in the count table house_keeping_genes &lt;- intersect(rownames(set), house_keeping_genes$V1) # now, we use these genes as the emprical set of genes as input to RUVg. # we try different values of k and see how the PCA plots look par(mfrow = c(3, 2)) for(k in 1:6) { set_g &lt;- RUVg(x = set, cIdx = house_keeping_genes, k = k) plotPCA(set_g, col=as.numeric(colData$group), cex=1.2, main = paste0(&#39;with RUVg, k = &#39;,k)) } # choose k = 1 set_g &lt;- RUVg(x = set, cIdx = house_keeping_genes, k = 1) # now let&#39;s do diagnostics: compare the count matrices with or without RUVg processing # RLE plots par(mfrow = c(1,2)) plotRLE(set, outline=FALSE, ylim=c(-4, 4), col=as.numeric(colData$group), main = &#39;without RUVg&#39;) plotRLE(set_g, outline=FALSE, ylim=c(-4, 4), col=as.numeric(colData$group), main = &#39;with RUVg&#39;) # PCA plots par(mfrow = c(1,2)) plotPCA(set, col=as.numeric(colData$group), cex=1.2, main = &#39;without RUVg&#39;) plotPCA(set_g, col=as.numeric(colData$group), cex=1.2, main = &#39;with RUVg&#39;) We can observe that using RUVg using house-keeping genes as reference has improved the clusters, however not ideal. There is another strategy of RUVSeq that works better in the presence of replicates without a confounded design, which is the RUVs function. Let’s see how that performs with this data. This time we don’t use the house-keeping genes. We rather use all genes as input to RUVs. 7.3.9.3.2 Using RUVs # make a table of sample groups from colData differences &lt;- makeGroups(colData$group) ## looking for two different sources of unwanted variation (k = 2) ## use information from all genes in the expression object par(mfrow = c(3, 2)) for(k in 1:6) { set_s &lt;- RUVs(set, unique(rownames(set)), k=k, differences) #all genes plotPCA(set_s, col=as.numeric(colData$group), cex=1.2, main = paste0(&#39;with RUVs, k = &#39;,k)) } # choose k = 3 set_s &lt;- RUVs(set, unique(rownames(set)), k=3, differences) # # repeat the RLE and PCA plots ## compare the initial and processed objects ## RLE plots par(mfrow = c(1,2)) plotRLE(set, outline=FALSE, ylim=c(-4, 4), col=as.numeric(colData$group), main = &#39;without RUVs&#39;) plotRLE(set_s, outline=FALSE, ylim=c(-4, 4), col=as.numeric(colData$group), main = &#39;with RUVs&#39;) ## PCA plots par(mfrow = c(1,2)) plotPCA(set, col=as.numeric(colData$group), cex=1.2, main = &#39;without RUVs&#39;) plotPCA(set_s, col=as.numeric(colData$group), cex=1.2, main = &#39;with RUVs&#39;) Let’s compare PCA results from RUVs and RUVg with the initial raw counts matrix: par(mfrow = c(1,3)) plotPCA(countData, col=as.numeric(colData$group), main = &#39;without RUV - raw counts&#39;) plotPCA(set_g, col=as.numeric(colData$group), main = &#39;with RUVg&#39;) plotPCA(set_s, col=as.numeric(colData$group), main = &#39;with RUVs&#39;) It looks like RUVs has performed better than RUVg in this case. So, let’s use count data that is processed by RUVs to re-do the initial heatmap. # extract normalized counts that are cleared from unwanted variation using RUVs normCountData &lt;- EDASeq::normCounts(set_s) selectedGenes &lt;- names(sort(apply(normCountData, 1, var), decreasing = TRUE))[1:500] pheatmap::pheatmap(normCountData[selectedGenes,], annotation_col = colData, show_rownames = FALSE, cutree_cols = 2, scale = &#39;row&#39;) As can be observed the replicates from different groups cluster much better with each other after processing with RUVs. It is important to note that RUVs uses information from replicates to shift the expression data and it would not work in a confounding design where the replicates of case samples and replicates of the control samples are sequenced in different batches. 7.3.9.4 Re-run DESeq2 with the computed covariates Having computed the sources of variation using RUVs, we can actually integrate these variables with DESeq2 to re-do the differential expression analysis. #set up DESeqDataSet object dds &lt;- DESeq2::DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ group) # filter for low count genes dds &lt;- dds[rowSums(counts(dds)) &gt; 10] # insert the covariates computed using RUVs into DESeqDataSet object colData(dds) &lt;- cbind(colData(dds), pData(set_s)[rownames(colData(dds)), grep(&#39;W_[0-9]&#39;, colnames(pData(set_s)))]) # update the design formula for the DESeq analysis (save the variable of interest to the last!) design(dds) &lt;- ~ W_1 + W_2 + W_3 + group # repeat the analysis dds &lt;- DESeq(dds) # extract deseq results res &lt;- DESeq2::results(dds, contrast = c(&#39;group&#39;, &#39;CASE&#39;, &#39;CTRL&#39;)) res &lt;- res[order(res$padj),] 7.3.9.5 Exercises Basic: Run RUVSeq using multiple values of k from 1 to 10 and compare and contrast the PCA plots obtained from the normalized counts of each RUVSeq run. Intermediate: Re-run RUVSeq using the RUVr function. Compare PCA plots from RUVs, RUVg and RUVr using the same k values and find out which one performs the best. Do the necessary diagnostic plots using the differential expression results from the EHF count table. Advanced: Use the sva package to discover sources of unwanted variation and re-do the differential expression analysis using variables from the output of sva and compare the results with DESeq2 results using RUVSeq corrected normalization counts. References "],
["other-applications-of-rna-seq.html", "7.4 Other applications of RNA-seq", " 7.4 Other applications of RNA-seq RNA-seq generates valuable data that contains information not only at the gene level but also at the level of exons and transcripts. Moreover, the kind of information that we can extract from RNA-seq is not limited to expression quantification. It is possible to detect alternative splicing events such as novel isoforms (Trapnell et al. 2010), differential usage of exons (Anders, Reyes, and Huber 2012) . It is also possible to observe sequence variants (substitutions, insertions, deletions, RNA-editing) that may change the translated protein product (McKenna et al. 2010). In the context of cancer genomes, gene-fusion events can be detected with RNA-seq (McPherson et al. 2011). Finally, for the purposes of gene prediction or improving existing gene predictions, RNA-seq is a valuable method (Stanke and Morgenstern 2005). In order to learn more about how to implement these, it is recommended to go through the tutorials of the cited tools. References "],
["reproducibility.html", "7.5 Reproducibility", " 7.5 Reproducibility The computational analysis of RNA-seq count tables demonstrated in this chapter require the following R library dependencies (only listing libraries outside of base R). DESeq2: Differential expression analysis RUVseq and EDASeq: Removing unwanted variation and diagnostics gProfileR: Functional term enrichment analysis Visualization: ggplot2 ggfortify pheatmap corrplot knitr DESeq2, RUVSeq, and EDASeq can be installed from the Bioconductor repository via: if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(c(&#39;DESeq2&#39;, &#39;RUVSeq&#39;, &#39;EDASeq&#39;)) The remaining package can be installed from CRAN via: install.packages(c('gProfileR', 'ggplot2', 'ggfortify', 'pheatmap', 'corrplot', 'knitr')) "],
["references.html", "References", " References "]
]
