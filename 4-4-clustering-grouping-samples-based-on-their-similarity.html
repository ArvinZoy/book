<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Computational Genomics With R" />
<meta property="og:type" content="book" />
<meta property="og:url" content="https://compmgenomr.github.io/book/" />
<meta property="og:image" content="https://compmgenomr.github.io/book/images/cover.jpg" />
<meta property="og:description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience" />
<meta name="github-repo" content="rstudio/bookdown" />

<meta name="author" content="Altuna Akalin" />

<meta name="date" content="2019-01-22" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience">

<title>Computational Genomics With R</title>

<script src="libs/jquery/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap/js/bootstrap.min.js"></script>
<script src="libs/bootstrap/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap/shim/respond.min.js"></script>
<script src="libs/navigation/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#preface">Preface</a><ul>
<li class="has-sub"><a href="who-is-this-book-for.html#who-is-this-book-for">Who is this book for?</a><ul>
<li><a href="who-is-this-book-for.html#what-will-you-get-out-of-this">What will you get out of this?</a></li>
</ul></li>
<li><a href="structure-of-the-book.html#structure-of-the-book">Structure of the book</a></li>
<li><a href="software-information-and-conventions.html#software-information-and-conventions">Software information and conventions</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
</ul></li>
<li><a href="1-how-to-contribute.html#how-to-contribute"><span class="toc-section-number">1</span> How to contribute</a></li>
<li><a href="about-the-authors.html#about-the-authors">About the Authors</a></li>
<li class="has-sub"><a href="2-intro.html#intro"><span class="toc-section-number">2</span> Introduction to Genomics</a><ul>
<li class="has-sub"><a href="2-1-genes-dna-and-central-dogma.html#genes-dna-and-central-dogma"><span class="toc-section-number">2.1</span> Genes, DNA and central dogma</a><ul>
<li><a href="2-1-genes-dna-and-central-dogma.html#what-is-a-genome"><span class="toc-section-number">2.1.1</span> What is a genome?</a></li>
<li><a href="2-1-genes-dna-and-central-dogma.html#what-is-a-gene"><span class="toc-section-number">2.1.2</span> What is a gene?</a></li>
<li><a href="2-1-genes-dna-and-central-dogma.html#how-genes-are-controlled-the-transcriptional-and-the-post-transcriptional-regulation"><span class="toc-section-number">2.1.3</span> How genes are controlled ? The transcriptional and the post-transcriptional regulation</a></li>
<li><a href="2-1-genes-dna-and-central-dogma.html#what-does-a-gene-look-like"><span class="toc-section-number">2.1.4</span> What does a gene look like?</a></li>
</ul></li>
<li class="has-sub"><a href="2-2-elements-of-gene-regulation.html#elements-of-gene-regulation"><span class="toc-section-number">2.2</span> Elements of gene regulation</a><ul>
<li><a href="2-2-elements-of-gene-regulation.html#transcriptional-regulation"><span class="toc-section-number">2.2.1</span> Transcriptional regulation</a></li>
<li><a href="2-2-elements-of-gene-regulation.html#post-transcriptional-regulation"><span class="toc-section-number">2.2.2</span> Post-transcriptional regulation</a></li>
</ul></li>
<li><a href="2-3-shaping-the-genome-dna-mutation.html#shaping-the-genome-dna-mutation"><span class="toc-section-number">2.3</span> Shaping the genome: DNA mutation</a></li>
<li class="has-sub"><a href="2-4-high-throughput-experimental-methods-in-genomics.html#high-throughput-experimental-methods-in-genomics"><span class="toc-section-number">2.4</span> High-throughput experimental methods in genomics</a><ul>
<li><a href="2-4-high-throughput-experimental-methods-in-genomics.html#the-general-idea-behind-high-throughput-techniques"><span class="toc-section-number">2.4.1</span> The general idea behind high-throughput techniques</a></li>
<li><a href="2-4-high-throughput-experimental-methods-in-genomics.html#high-throughput-sequencing"><span class="toc-section-number">2.4.2</span> High-throughput sequencing</a></li>
</ul></li>
<li><a href="2-5-visualization-and-data-repositories-for-genomics.html#visualization-and-data-repositories-for-genomics"><span class="toc-section-number">2.5</span> Visualization and data repositories for genomics</a></li>
</ul></li>
<li class="has-sub"><a href="3-Rintro.html#Rintro"><span class="toc-section-number">3</span> Introduction to R for genomic data analysis</a><ul>
<li class="has-sub"><a href="3-1-steps-of-genomic-data-analysis.html#steps-of-genomic-data-analysis"><span class="toc-section-number">3.1</span> Steps of (genomic) data analysis</a><ul>
<li><a href="3-1-steps-of-genomic-data-analysis.html#data-collection"><span class="toc-section-number">3.1.1</span> Data collection</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#data-quality-check-and-cleaning"><span class="toc-section-number">3.1.2</span> Data quality check and cleaning</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#data-processing"><span class="toc-section-number">3.1.3</span> Data processing</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#exploratory-data-analysis-and-modeling"><span class="toc-section-number">3.1.4</span> Exploratory data analysis and modeling</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#visualization-and-reporting"><span class="toc-section-number">3.1.5</span> Visualization and reporting</a></li>
<li><a href="3-1-steps-of-genomic-data-analysis.html#why-use-r-for-genomics"><span class="toc-section-number">3.1.6</span> Why use R for genomics ?</a></li>
</ul></li>
<li class="has-sub"><a href="3-2-getting-started-with-r.html#getting-started-with-r"><span class="toc-section-number">3.2</span> Getting started with R</a><ul>
<li><a href="3-2-getting-started-with-r.html#installing-packages"><span class="toc-section-number">3.2.1</span> Installing packages</a></li>
<li><a href="3-2-getting-started-with-r.html#installing-packages-in-custom-locations"><span class="toc-section-number">3.2.2</span> Installing packages in custom locations</a></li>
<li><a href="3-2-getting-started-with-r.html#getting-help-on-functions-and-packages"><span class="toc-section-number">3.2.3</span> Getting help on functions and packages</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="4-stats.html#stats"><span class="toc-section-number">4</span> Statistics and Exploratory Data Analysis for Genomics</a><ul>
<li class="has-sub"><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions"><span class="toc-section-number">4.1</span> How to summarize collection of data points: The idea behind statistical distributions</a><ul>
<li><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-central-tendency-mean-and-median"><span class="toc-section-number">4.1.1</span> Describing the central tendency: mean and median</a></li>
<li><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-spread-measurements-of-variation"><span class="toc-section-number">4.1.2</span> Describing the spread: measurements of variation</a></li>
<li><a href="4-1-how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#precision-of-estimates-confidence-intervals"><span class="toc-section-number">4.1.3</span> Precision of estimates: Confidence intervals</a></li>
</ul></li>
<li class="has-sub"><a href="4-2-how-to-test-for-differences-between-samples.html#how-to-test-for-differences-between-samples"><span class="toc-section-number">4.2</span> How to test for differences between samples</a><ul>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#randomization-based-testing-for-difference-of-the-means"><span class="toc-section-number">4.2.1</span> randomization based testing for difference of the means</a></li>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#using-t-test-for-difference-of-the-means-between-two-samples"><span class="toc-section-number">4.2.2</span> Using t-test for difference of the means between two samples</a></li>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#multiple-testing-correction"><span class="toc-section-number">4.2.3</span> multiple testing correction</a></li>
<li><a href="4-2-how-to-test-for-differences-between-samples.html#moderated-t-tests-using-information-from-multiple-comparisons"><span class="toc-section-number">4.2.4</span> moderated t-tests: using information from multiple comparisons</a></li>
</ul></li>
<li class="has-sub"><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#relationship-between-variables-linear-models-and-correlation"><span class="toc-section-number">4.3</span> Relationship between variables: linear models and correlation</a><ul>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#how-to-fit-a-line"><span class="toc-section-number">4.3.1</span> How to fit a line</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#how-to-estimate-the-error-of-the-coefficients"><span class="toc-section-number">4.3.2</span> How to estimate the error of the coefficients</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#accuracy-of-the-model"><span class="toc-section-number">4.3.3</span> Accuracy of the model</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#regression-with-categorical-variables"><span class="toc-section-number">4.3.4</span> Regression with categorical variables</a></li>
<li><a href="4-3-relationship-between-variables-linear-models-and-correlation.html#regression-pitfalls"><span class="toc-section-number">4.3.5</span> Regression pitfalls</a></li>
</ul></li>
<li class="has-sub"><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#clustering-grouping-samples-based-on-their-similarity"><span class="toc-section-number">4.4</span> Clustering: grouping samples based on their similarity</a><ul>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#distance-metrics"><span class="toc-section-number">4.4.1</span> Distance metrics</a></li>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#hiearchical-clustering"><span class="toc-section-number">4.4.2</span> Hiearchical clustering</a></li>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#k-means-clustering"><span class="toc-section-number">4.4.3</span> K-means clustering</a></li>
<li><a href="4-4-clustering-grouping-samples-based-on-their-similarity.html#how-to-choose-k-the-number-of-clusters"><span class="toc-section-number">4.4.4</span> how to choose “k”, the number of clusters</a></li>
</ul></li>
<li class="has-sub"><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d"><span class="toc-section-number">4.5</span> Dimensionality reduction techniques: visualizing complex data sets in 2D</a><ul>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#principal-component-analysis"><span class="toc-section-number">4.5.1</span> Principal component analysis</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#other-dimension-reduction-techniques-using-other-matrix-factorization-methods"><span class="toc-section-number">4.5.2</span> Other dimension reduction techniques using other matrix factorization methods</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#multi-dimensional-scaling"><span class="toc-section-number">4.5.3</span> Multi-dimensional scaling</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#t-distributed-stochastic-neighbor-embedding-t-sne"><span class="toc-section-number">4.5.4</span> t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1"><span class="toc-section-number">4.5.5</span> How to summarize collection of data points: The idea behind statistical distributions</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#how-to-test-for-differences-in-samples"><span class="toc-section-number">4.5.6</span> How to test for differences in samples</a></li>
<li><a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#relationship-between-variables-linear-models-and-correlation-1"><span class="toc-section-number">4.5.7</span> Relationship between variables: linear models and correlation</a></li>
</ul></li>
</ul></li>
<li><a href="5-genomicIntervals.html#genomicIntervals"><span class="toc-section-number">5</span> Operations on Genomic Intervals and Genome Arithmetic</a></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="clustering-grouping-samples-based-on-their-similarity" class="section level2">
<h2><span class="header-section-number">4.4</span> Clustering: grouping samples based on their similarity</h2>
<p>In genomics, we would very frequently want to assess how our samples relate to each other. Are our replicates similar to each other? Do the samples from the same treatment group have the similar genome-wide signals ? Do the patients with similar diseases have similar gene expression profiles ?
Take the last question for example. We need to define a distance or similarity metric between patients’ expression profiles and use that metric to find groups of patients that are more similar to each other than the rest of the patients. This, in essence, is the general idea behind clustering. We need a distance metric and a method to utilize that distance metric to find self-similar groups. Clustering is a ubiquitous procedure in bioinformatics as well as any field that deals with high-dimensional data. It is very likely every genomics paper containing multiple samples have some sort of clustering. Due to this ubiquity and general usefulness, it is an essential technique to learn.</p>
<div id="distance-metrics" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Distance metrics</h3>
<p>The first required step for clustering is the distance metric. This is simply a measurement of how similar gene expressions to each other are. There are many options for distance metrics and the choice of the metric is quite important for clustering. Consider a simple example where we have four patients and expression of three genes measured. Which patients look similar to each other based on their gene expression profiles ?</p>
<table>
<caption><span id="tab:expTable">TABLE 4.1: </span>Gene expressions from patients</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">IRX4</th>
<th align="right">OCT4</th>
<th align="right">PAX6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>patient1</td>
<td align="right">11</td>
<td align="right">10</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td>patient2</td>
<td align="right">13</td>
<td align="right">13</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td>patient3</td>
<td align="right">2</td>
<td align="right">4</td>
<td align="right">10</td>
</tr>
<tr class="even">
<td>patient4</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">9</td>
</tr>
</tbody>
</table>
<p>It may not be obvious from the table at first sight but if we plot the gene expression profile for each patient, we will see that expression profiles of patient 1 and patient 2 is more similar to each other than patient 3 or patient 4.</p>
<div class="figure" style="text-align: center"><span id="fig:expPlot"></span>
<img src="compgenomrReloaded_files/figure-html/expPlot-1.png" alt="Gene expression values for different patients. Certain patients have similar gene expression values to each other." width="60%" />
<p class="caption">
FIGURE 4.19: Gene expression values for different patients. Certain patients have similar gene expression values to each other.
</p>
</div>
<p>But how can we quantify what see by eye ? A simple metric for distance between gene expression vectors between a given patient pair is the sum of absolute difference between gene expression values This can be formulated as follows: <span class="math inline">\(d_{AB}={\sum _{i=1}^{n}|e_{Ai}-e_{Bi}|}\)</span>, where <span class="math inline">\(d_{AB}\)</span> is the distance between patient A and B, and <span class="math inline">\(e_{Ai}\)</span> and <span class="math inline">\(e_{Bi}\)</span> expression value of the <span class="math inline">\(i\)</span>th gene for patient A and B. This distance metric is called <strong>“Manhattan distance”</strong> or <strong>“L1 norm”</strong>.</p>
<p>Another distance metric using sum of squared distances and taking a square root of resulting value, that can be formulaized as: <span class="math inline">\(d_{AB}={{\sqrt {\sum _{i=1}^{n}(e_{Ai}-e_{Bi})^{2}}}}\)</span>. This distance is called <strong>“Euclidean Distance”</strong> or <strong>“L2 norm”</strong>. This is usually the default distance metric for many clustering algorithms. due to squaring operation values that are very different get higher contribution to the distance. Due to this, compared to Manhattan distance it can be more affected by outliers but generally if the outliers are rare this distance metric works well.</p>
<p>The last metric we will introduce is the <strong>“correlation distance”</strong>. This is simply <span class="math inline">\(d_{AB}=1-\rho\)</span>, where <span class="math inline">\(\rho\)</span> is the pearson correlation coefficient between two vectors, in our case those vectors are gene expression profiles of patients. Using this distance the gene expression vectors that have a similar pattern will have a small distance whereas when the vectors have different patterns they will have a large distance. In this case, the linear correlation between vectors matters, the the scale of the vectors might be different.</p>
<p>Now let’s see how we can calculate these distance in R. First, we have our gene expression per patient table.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" title="1">df</a></code></pre></div>
<pre><code>##          IRX4 OCT4 PAX6
## patient1   11   10    1
## patient2   13   13    3
## patient3    2    4   10
## patient4    1    3    9</code></pre>
<p>Next, we calculate the distance metrics using <code>dist</code> function and <code>1-cor()</code>.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" title="1"><span class="kw">dist</span>(df,<span class="dt">method=</span><span class="st">&quot;manhattan&quot;</span>)</a></code></pre></div>
<pre><code>##          patient1 patient2 patient3
## patient2        7                  
## patient3       24       27         
## patient4       25       28        3</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb60-1" title="1"><span class="kw">dist</span>(df,<span class="dt">method=</span><span class="st">&quot;euclidean&quot;</span>)</a></code></pre></div>
<pre><code>##          patient1 patient2 patient3
## patient2    4.123                  
## patient3   14.071   15.843         
## patient4   14.595   16.733    1.732</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" title="1"><span class="kw">as.dist</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">cor</span>(<span class="kw">t</span>(df)))</a></code></pre></div>
<pre><code>##          patient1 patient2 patient3
## patient2 0.004129                  
## patient3 1.988522 1.970725         
## patient4 1.988522 1.970725 0.000000</code></pre>
<div id="scaling-before-calculating-the-distance" class="section level4">
<h4><span class="header-section-number">4.4.1.1</span> Scaling before calculating the distance</h4>
<p>Before we proceed to the clustering, one more thing we need to take care. Should we normalize our data ? Scale of the vectors in our expression matrix can affect the distance calculation. Gene expression tables are usually have some sort of normalization, so the values are in comparable scales. But somehow if a gene’s expression values were on much higher scale than the other genes, that gene will effect the distance more than other when using Euclidean or Manhattan distance. If that is the case we can scale the variables.The traditional way of scaling variables is to subtract their mean, and divide by their standard deviation, this operation is also called “standardization”. If this is done on all genes, each gene will have the same affect on distance measures. The decision to apply scaling ultimately depends on our data and what you want to achieve. If the gene expression values are previously normalized between patients, having genes that dominate the distance metric could have a biological meaning and therefore it may not be desireable to further scale variables. In R, the standardization is done via <code>scale()</code> function. Here we scale the gene expression values.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" title="1">df</a></code></pre></div>
<pre><code>##          IRX4 OCT4 PAX6
## patient1   11   10    1
## patient2   13   13    3
## patient3    2    4   10
## patient4    1    3    9</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" title="1"><span class="kw">scale</span>(df)</a></code></pre></div>
<pre><code>##             IRX4    OCT4    PAX6
## patient1  0.6933  0.5213 -1.0734
## patient2  1.0195  1.1468 -0.6214
## patient3 -0.7748 -0.7298  0.9604
## patient4 -0.9379 -0.9383  0.7344
## attr(,&quot;scaled:center&quot;)
## IRX4 OCT4 PAX6 
## 6.75 7.50 5.75 
## attr(,&quot;scaled:scale&quot;)
##  IRX4  OCT4  PAX6 
## 6.131 4.796 4.425</code></pre>
</div>
</div>
<div id="hiearchical-clustering" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Hiearchical clustering</h3>
<p>This is one of the most ubiqutous clustering algorithms. Using this algorithm you can see the relationship of individual data points and relationships of clusters. This is achieved succesively joining small clusters to each other based on the intercluster distance. Eventually, you get a tree structure or a dendrogram that shows the relationship between the individual data points and clusters. The height of the dendrogram is the distance between clusters. Here we can show how to use this on our toy data set from four patients. The base function in R to do hierarchical clustering in <code>hclust()</code>. Below, we apply that function on Euclidean distances between patients.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" title="1">d=<span class="kw">dist</span>(df)</a>
<a class="sourceLine" id="cb68-2" title="2">hc=<span class="kw">hclust</span>(d,<span class="dt">method=</span><span class="st">&quot;complete&quot;</span>)</a>
<a class="sourceLine" id="cb68-3" title="3"><span class="kw">plot</span>(hc)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:toyClust"></span>
<img src="compgenomrReloaded_files/figure-html/toyClust-1.png" alt="Dendrogram of distance matrix" width="60%" />
<p class="caption">
FIGURE 4.20: Dendrogram of distance matrix
</p>
</div>
<p>In the above code snippet, we have used <code>method="complete"</code> argument without explaining it. The <code>method</code> argument defines the criteria that directs how the sub-clusters are merged. During clustering starting with single-member clusters, the clusters are merged based on the distance between them. There are many different ways to define distance between clusters and based on which definition you use the hierarchical clustering results change. So the <code>method</code> argument controls that. There are a couple of values this argument can take, we list them and their description below:</p>
<ul>
<li><strong>“complete”</strong> stands for “Complete Linkage” and the distance between two clusters is defined as largest distance between any members of the two clusters.</li>
<li><strong>“single”</strong> stands for “Single Linkage” and the distance between two clusters is defined as smallest distance between any members of the two clusters.</li>
<li><strong>“average”</strong> stands for “Average Linkage” or more precisely UPGMA (Unweighted Pair Group Method with Arithmetic Mean) method. In this case, the distance between two clusters is defined as average distance between any members of the two clusters.</li>
<li><strong>“ward.D2”</strong> and <strong>“ward.D”</strong> stands for different implementations of Ward’s minimum variance method. This method aims to find compact, spherical clusters by selecting clusters to merge based on the change in the cluster variances. The clusters are merged if the increase in the combined variance over the sum of the cluster specific variances is minimum compared to alternative merging operations.</li>
</ul>
<p>In real life, we would get expression profiles from thousands of genes and we will typically have many more patients than our toy example. One such data set is gene expression values from 60 bone marrow samples of patients with one of the four main types of leukemia (ALL, AML, CLL, CML) or no-leukemia controls. We trimmed that data set down to top 1000 most variable genes to be able to work with it easier and in addition genes that are not very variable do not contribute much to the distances between patients. We will now use this data set to cluster the patients and display the values as a heatmap and a dendrogram. The heatmap shows the expression values of genes across patients in a color coded manner. The heatmap function, <code>pheatmap()</code>, we will use performs the clustering as well. The matrix that contains gene expressions has the genes in the rows and the patients in the columns. Therefore, we will also use a column-side color code to mark the patients based on their leukemia type. For the hierarchical clustering, we will use Ward’s method designated by <code>clustering_method</code> argument to <code>pheatmap()</code> function.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" title="1"><span class="kw">library</span>(pheatmap)</a>
<a class="sourceLine" id="cb69-2" title="2">expFile=<span class="kw">system.file</span>(<span class="st">&quot;extdata&quot;</span>,<span class="st">&quot;leukemiaExpressionSubset.rds&quot;</span>,<span class="dt">package=</span><span class="st">&quot;compGenomRData&quot;</span>)</a>
<a class="sourceLine" id="cb69-3" title="3">mat=<span class="kw">readRDS</span>(expFile)</a>
<a class="sourceLine" id="cb69-4" title="4"></a>
<a class="sourceLine" id="cb69-5" title="5"><span class="co"># set the leukemia type annotation for each sample</span></a>
<a class="sourceLine" id="cb69-6" title="6">annotation_col =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb69-7" title="7">                    <span class="dt">LeukemiaType =</span><span class="kw">substr</span>(<span class="kw">colnames</span>(mat),<span class="dv">1</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb69-8" title="8"><span class="kw">rownames</span>(annotation_col)=<span class="kw">colnames</span>(mat)</a>
<a class="sourceLine" id="cb69-9" title="9">  </a>
<a class="sourceLine" id="cb69-10" title="10"></a>
<a class="sourceLine" id="cb69-11" title="11"><span class="kw">pheatmap</span>(mat,<span class="dt">show_rownames=</span><span class="ot">FALSE</span>,<span class="dt">show_colnames=</span><span class="ot">FALSE</span>,<span class="dt">annotation_col=</span>annotation_col,<span class="dt">scale =</span> <span class="st">&quot;none&quot;</span>,<span class="dt">clustering_method=</span><span class="st">&quot;ward.D2&quot;</span>,<span class="dt">clustering_distance_cols=</span><span class="st">&quot;euclidean&quot;</span>)</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/heatmap1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As we can observe in the heatmap each cluster has a distinct set of expression values. The main clusters almost perfectly distinguish the leukemia types. Only one CML patient is clustered as a non-leukemia sample. This could mean that gene expression profiles are enough to classify leukemia type. More detailed analysis and experiments are needed to verify that but by looking at this exploratory analysis we can decide where to focus our efforts next.</p>
<div id="where-to-cut-the-tree" class="section level4">
<h4><span class="header-section-number">4.4.2.1</span> where to cut the tree ?</h4>
<p>The example above seems like a clear cut example where we can pick by eye clusters from the dendrogram. This is mostly due to the Ward’s method where compact clusters are preffered. However, as it is usually the case we do not have patient labels and it would be difficult to tell which leaves (patients) in the dendrogram we should consider as part of the same cluster. In other words, how deep we should cut the dendrogram so that every patient sample still connected via the remaining sub-dendrograms constitute clusters. The <code>cutree()</code> function provides the functionality to output either desired number of clusters or clusters obtained from cutting the dendrogram at a certain height. Below, we will cluster the patients with hierarchical clustering using the default method “complete linkage” and cut the dendrogram at a certain height. In this case, you will also observe that, changing from Ward’s distance to complete linkage had an effect on clustering. Now two clusters that are defined by Ward’s distance are closer to each other and harder to separate from each other.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" title="1">hcl=<span class="kw">hclust</span>(<span class="kw">dist</span>(<span class="kw">t</span>(mat)))</a>
<a class="sourceLine" id="cb70-2" title="2"><span class="kw">plot</span>(hcl,<span class="dt">labels =</span> <span class="ot">FALSE</span>, <span class="dt">hang=</span> <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb70-3" title="3"><span class="kw">rect.hclust</span>(hcl, <span class="dt">h =</span> <span class="dv">80</span>, <span class="dt">border =</span> <span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/hclustNcut-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" title="1">clu.k5=<span class="kw">cutree</span>(hcl,<span class="dt">k=</span><span class="dv">5</span>) <span class="co"># cut tree so that there are 4 clusters</span></a>
<a class="sourceLine" id="cb71-2" title="2"></a>
<a class="sourceLine" id="cb71-3" title="3">clu.h80=<span class="kw">cutree</span>(hcl,<span class="dt">h=</span><span class="dv">80</span>) <span class="co"># cut tree/dendrogram from height 80</span></a>
<a class="sourceLine" id="cb71-4" title="4"><span class="kw">table</span>(clu.k5) <span class="co"># number of samples for each cluster</span></a></code></pre></div>
<pre><code>## clu.k5
##  1  2  3  4  5 
## 12  3  9 12 24</code></pre>
<p>Apart from the arbitrary values for the height or the number of the clusters, how can we define clusters more systematically? As this is a general question, we will show later how to decide the optimal number of clusters later in this chapter.</p>
</div>
</div>
<div id="k-means-clustering" class="section level3">
<h3><span class="header-section-number">4.4.3</span> K-means clustering</h3>
<p>Another, very common clustering algorithm is k-means.This method divides or partitions the data points, our working example patients, into a pre-determined, “k” number of clusters. Hence, this type of methods are generally called “partioning” methods. The algorithm is initialized with randomly choosen <span class="math inline">\(k\)</span> centers or centroids. In a sense, a centroid is a data point with multiple values. In our working example, it is a hypothetical patient with gene expression values. But in the initialization phase, those gene expression values are choosen randomly within the boundaries of the gene expression distributions from real patients. As the next step in the algorithm, each patient is assigned to the closest centroid and in the next iteration centroids are set to the mean of values of the genes in the cluster. This process of setting centroids and assigning patients to the clusters repeats itself until sum of squared distances to cluster centroids is minimized.</p>
<p>As you might see, the cluster algorithm starts with random initial centroids. This feature might yield different results for each run of the algorithm. We will know show how to use k-means method on the gene expression data set. We will use <code>set.seed()</code> for reproducbility. In the wild, you might want to run this algorithm multiple times to see if your clustering results are stable.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" title="1"><span class="kw">set.seed</span>(<span class="dv">101</span>)</a>
<a class="sourceLine" id="cb73-2" title="2"></a>
<a class="sourceLine" id="cb73-3" title="3"><span class="co"># we have to transpore the matrix t()</span></a>
<a class="sourceLine" id="cb73-4" title="4"><span class="co"># so that we calculate distances between patients</span></a>
<a class="sourceLine" id="cb73-5" title="5">kclu=<span class="kw">kmeans</span>(<span class="kw">t</span>(mat),<span class="dt">centers=</span><span class="dv">5</span>)  </a>
<a class="sourceLine" id="cb73-6" title="6"></a>
<a class="sourceLine" id="cb73-7" title="7"><span class="co"># number of data points in each cluster</span></a>
<a class="sourceLine" id="cb73-8" title="8"><span class="kw">table</span>(kclu<span class="op">$</span>cluster)</a></code></pre></div>
<pre><code>## 
##  1  2  3  4  5 
## 12 12 14 11 11</code></pre>
<p>Now let us check the percentage of each leukemia type in each cluster. We can visualize this as a table. Looking at the table below, we see that each of the 5 clusters are predominantly representing one of the 4 leukemia types or the control patients without leukemia.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" title="1">type2kclu =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb75-2" title="2">                    <span class="dt">LeukemiaType =</span><span class="kw">substr</span>(<span class="kw">colnames</span>(mat),<span class="dv">1</span>,<span class="dv">3</span>),</a>
<a class="sourceLine" id="cb75-3" title="3">                    <span class="dt">cluster=</span>kclu<span class="op">$</span>cluster)</a>
<a class="sourceLine" id="cb75-4" title="4"></a>
<a class="sourceLine" id="cb75-5" title="5"><span class="kw">table</span>(type2kclu)</a></code></pre></div>
<pre><code>##             cluster
## LeukemiaType  1  2  3  4  5
##          ALL 12  0  0  0  0
##          AML  0  0  1  0 11
##          CLL  0 12  0  0  0
##          CML  0  0  1 11  0
##          NoL  0  0 12  0  0</code></pre>
<p>Another related and maybe more robust algorithm is called <strong>“k-medoids”</strong> clustering. The procedure is almost identical to k-means clustering with a couple of differences. In this case, centroids choosen are real data points in our case patients, and the metric we are trying to optimize in each iteration is based on manhattan distance to the centroid. In k-means this was based on sum of squared distances so euclidean distance. Below we are showing how to use k-medoids clustering function <code>pam()</code> from the <code>cluster</code> package.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" title="1">kmclu=cluster<span class="op">::</span><span class="kw">pam</span>(<span class="kw">t</span>(mat),<span class="dt">k=</span><span class="dv">5</span>) <span class="co">#  cluster using k-medoids</span></a>
<a class="sourceLine" id="cb77-2" title="2"></a>
<a class="sourceLine" id="cb77-3" title="3"><span class="co"># make a data frame with Leukemia type and cluster id</span></a>
<a class="sourceLine" id="cb77-4" title="4">type2kmclu =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb77-5" title="5">                    <span class="dt">LeukemiaType =</span><span class="kw">substr</span>(<span class="kw">colnames</span>(mat),<span class="dv">1</span>,<span class="dv">3</span>),</a>
<a class="sourceLine" id="cb77-6" title="6">                    <span class="dt">cluster=</span>kmclu<span class="op">$</span>cluster)</a>
<a class="sourceLine" id="cb77-7" title="7"></a>
<a class="sourceLine" id="cb77-8" title="8"><span class="kw">table</span>(type2kmclu)</a></code></pre></div>
<pre><code>##             cluster
## LeukemiaType  1  2  3  4  5
##          ALL 12  0  0  0  0
##          AML  0 10  1  1  0
##          CLL  0  0  0  0 12
##          CML  0  0  0 12  0
##          NoL  0  0 12  0  0</code></pre>
<p>We can not visualize the clustering from partioning methods with a tree like we did for hierarchical clustering. Even if we can get the distances between patients the algorithm does not return the distances between clusters out of the box. However, if we had a way to visualize the distances between patients in 2 dimensions we could see the how patients and clusters relate each other. It turns out, that there is a way to compress between patient distances to a 2-dimensional plot. There are many ways to do this and we introduce these dimension reduction methods including the one we will use now later in this chapter. For now, we are going to use a method called “multi-dimensional scaling” and plot the patients in a 2D plot color coded by their cluster assignments.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" title="1"><span class="co"># Calculate distances</span></a>
<a class="sourceLine" id="cb79-2" title="2">dists=<span class="kw">dist</span>(<span class="kw">t</span>(mat))</a>
<a class="sourceLine" id="cb79-3" title="3"></a>
<a class="sourceLine" id="cb79-4" title="4"><span class="co"># calculate MDS</span></a>
<a class="sourceLine" id="cb79-5" title="5">mds=<span class="kw">cmdscale</span>(dists)</a>
<a class="sourceLine" id="cb79-6" title="6"></a>
<a class="sourceLine" id="cb79-7" title="7"><span class="co"># plot the patients in the 2D space</span></a>
<a class="sourceLine" id="cb79-8" title="8"><span class="kw">plot</span>(mds,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span><span class="kw">rainbow</span>(<span class="dv">5</span>)[kclu<span class="op">$</span>cluster])</a>
<a class="sourceLine" id="cb79-9" title="9"></a>
<a class="sourceLine" id="cb79-10" title="10"><span class="co"># set the legend for cluster colors</span></a>
<a class="sourceLine" id="cb79-11" title="11"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>,</a>
<a class="sourceLine" id="cb79-12" title="12">       <span class="dt">legend=</span><span class="kw">paste</span>(<span class="st">&quot;clu&quot;</span>,<span class="kw">unique</span>(kclu<span class="op">$</span>cluster)),</a>
<a class="sourceLine" id="cb79-13" title="13">       <span class="dt">fill=</span><span class="kw">rainbow</span>(<span class="dv">5</span>)[<span class="kw">unique</span>(kclu<span class="op">$</span>cluster)],</a>
<a class="sourceLine" id="cb79-14" title="14">       <span class="dt">border=</span><span class="ot">NA</span>,<span class="dt">box.col=</span><span class="ot">NA</span>)</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/kmeansmds-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The plot we obtained shows the separetion between clusters. However, it does not do a great job showing the separation between cluster 3 and 4, which represent CML and “no leukemia” patients. We might need another dimension to properly visualize that separation. In addition, those two clusters were closely related in the hierarhical clustering as well.</p>
</div>
<div id="how-to-choose-k-the-number-of-clusters" class="section level3">
<h3><span class="header-section-number">4.4.4</span> how to choose “k”, the number of clusters</h3>
<p>Up to this point, we have avoided the question of selecting optimal number clusters. How do we know where to cut our dendrogram or which k to choose ?
First of all, this is a difficult question. Usually, clusters have different granuality. Some clusters are tight and compact and some are wide,and both these types of clusters can be in the same data set. When visualized, some large clusters may look like they may have sub-clusters. So should we consider the large cluster as one cluster or should we consider the sub-clusters as individual clusters ? There are some metrics to help but there is no definite answer. We will show a couple of them below.</p>
<div id="silhouhette" class="section level4">
<h4><span class="header-section-number">4.4.4.1</span> Silhouhette</h4>
<p>One way to determine how well the clustering is to measure the expected self-similar nature of the points in a set of clusters. The silhouette value does just that and it is a measure of how similar a data point is to its own cluster compared to other clusters. The silhouette value ranges from -1 to +1, where values that are positive indicates that the data point is well matched to its own cluster, if the value is zero it is a borderline case and if the value is minus it means that the data point might be mis-clustered because it is more simialr to a neighboring cluster. If most data points have a high value, then the clustering is appropriate. Ideally, one can create many different clusterings with different parameters such as <span class="math inline">\(k\)</span>,number of clusters and assess their appropriateness using the average
silhouette values. In R, silhouette values are refered to as silhouette widths in the documentation.</p>
<p>A silhouette value is calculated for each data point. In our working example, each patient will get silhouette values showing how well they are matched to their assigned clusters. Formally this calculated as follows. For each data point <span class="math inline">\(i\)</span>, we calculate <span class="math inline">\({\displaystyle a(i)}\)</span>, which denotes the average distance between <span class="math inline">\(i\)</span> and all other data points within the same cluster. This shows how well the point fits into that cluster. For the same data point, we also calculate <span class="math inline">\({\displaystyle b(i)}\)</span> b(i) denotes the lowest average distance of <span class="math inline">\({\displaystyle i}\)</span> to all points in any other cluster, of which <span class="math inline">\({\displaystyle i}\)</span> is not a member. The cluster with this lowest average <span class="math inline">\(b(i)\)</span> is the “neighbouring cluster” of data point <span class="math inline">\({\displaystyle i}\)</span> since it is the next best fit cluster for that data point. Then, the silhouette value for a given data point is:</p>
<p><span class="math inline">\(s(i) = \frac{b(i) - a(i)}{\max\{a(i),b(i)\}}\)</span></p>
<p>As described, this quantity is positive when <span class="math inline">\(b(i)\)</span> is hight and <span class="math inline">\(a(i)\)</span> is low, meaning that the data point <span class="math inline">\(i\)</span> is self-similar to its cluster. And the silhouette value, <span class="math inline">\(s(i)\)</span>, is negative if it is more similar to its neighbours than its assigned cluster.</p>
<p>In R, we can calculate silhouette values using <code>cluster::silhouette()</code> function. Below, we calculate the silhouette values for k-medoids clustering with <code>pam()</code> function with <code>k=5</code>.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" title="1"><span class="kw">library</span>(cluster)</a>
<a class="sourceLine" id="cb80-2" title="2"><span class="kw">set.seed</span>(<span class="dv">101</span>)</a>
<a class="sourceLine" id="cb80-3" title="3">pamclu=cluster<span class="op">::</span><span class="kw">pam</span>(<span class="kw">t</span>(mat),<span class="dt">k=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb80-4" title="4"><span class="kw">plot</span>(<span class="kw">silhouette</span>(pamclu),<span class="dt">main=</span><span class="ot">NULL</span>)</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/sill-1.png" width="672" style="display: block; margin: auto;" />
Now, let us calculate average silhouette value different <span class="math inline">\(k\)</span> values and compare. We will use <code>sapply()</code> function to get average silhouette values accross <span class="math inline">\(k\)</span> values between 2 and 7. Within <code>sapply()</code> there is an anonymous function that that does the clustering and calculates average silhouette values for each <span class="math inline">\(k\)</span>.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" title="1">Ks=<span class="kw">sapply</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">7</span>,</a>
<a class="sourceLine" id="cb81-2" title="2">    <span class="cf">function</span>(i) </a>
<a class="sourceLine" id="cb81-3" title="3">      <span class="kw">summary</span>(<span class="kw">silhouette</span>(<span class="kw">pam</span>(<span class="kw">t</span>(mat),<span class="dt">k=</span>i)))<span class="op">$</span>avg.width)</a>
<a class="sourceLine" id="cb81-4" title="4"><span class="kw">plot</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">7</span>,Ks,<span class="dt">xlab=</span><span class="st">&quot;k&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;av. silhouette&quot;</span>,<span class="dt">type=</span><span class="st">&quot;b&quot;</span>,</a>
<a class="sourceLine" id="cb81-5" title="5">     <span class="dt">pch=</span><span class="dv">19</span>)</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/sillav-1.png" width="672" style="display: block; margin: auto;" />
In this case, it seems the best value for <span class="math inline">\(k\)</span> is 4. The k-medoids function <code>pam()</code> will usually cluster CML and noLeukemia cases together when <code>k=4</code>, which are also related clusters according to hierarchical clustering we did earlier.</p>
</div>
<div id="gap-statistic" class="section level4">
<h4><span class="header-section-number">4.4.4.2</span> Gap statistic</h4>
<p>As clustering aims to find self-similar data points, it would be reasonable to expect with the correct number of clusters the total within-cluster variation is minimized. Within-cluster variation for a single cluster can simply be defined as sum of squares from the cluster mean, which in this case is the centroid we defined in k-means algorithm. The total within-cluster variation is then sum of within-cluster variations for each cluster. This can be formally defined as follows:</p>
<p><span class="math inline">\(\displaystyle W_k = \sum_{k=1}^K \sum_{\mathrm{x}_i \in C_k} (\mathrm{x}_i - \mu_k )^2\)</span></p>
<p>Where <span class="math inline">\(\mathrm{x}_i\)</span> is data point in cluster <span class="math inline">\(k\)</span>, and <span class="math inline">\(\mu_k\)</span> is the cluster mean, and <span class="math inline">\(W_k\)</span> is the total within-cluster variation quantity we described. However, the problem is that the variation quantity decreases with number of clusters. The more centroids we have, the smaller the distances to the centroids get. A more reliable approach would be somehow calculating the expected variation from a reference null distribution and compare that to the observed variation for each <span class="math inline">\(k\)</span>. In gap statistic approach, the expected distribution is calculated via sampling points from the boundaries of the original data and calculating within-cluster variation quantity for multiple rounds of sampling. This way we have an expectation how about the variability when there is no expected clustering, and then compare that expected variation to the observed within-cluster variation. The expected variation should also go down with increasing number of clusters, but for the optimal number of clusters the expected variation will be furthest away from observed variation. This distance is called the <strong>“gap statistic”</strong> and defined as follows:
<span class="math inline">\(\displaystyle \mathrm{Gap}_n(k) = E_n^*\{\log W_k\} - \log W_k\)</span>, where <span class="math inline">\(E_n^*\{\log W_k\}\)</span> is the expected variation in log-scale under a sample size <span class="math inline">\(n\)</span> from the reference distribution and <span class="math inline">\(\log W_k\)</span> is the observed variation. Our aim is choose the <span class="math inline">\(k\)</span>, number of clusters, that maximizes <span class="math inline">\(\mathrm{Gap}_n(k)\)</span>.</p>
<p>We can easily calculate the gap statistic with <code>cluster::clusGap()</code> function. We will now use that function to calculate the gap statistic for our patient gene expression data.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" title="1"><span class="kw">library</span>(cluster)</a>
<a class="sourceLine" id="cb82-2" title="2"><span class="kw">set.seed</span>(<span class="dv">101</span>)</a>
<a class="sourceLine" id="cb82-3" title="3"><span class="co"># define the clustering function</span></a>
<a class="sourceLine" id="cb82-4" title="4">pam1 &lt;-<span class="st"> </span><span class="cf">function</span>(x,k) </a>
<a class="sourceLine" id="cb82-5" title="5">  <span class="kw">list</span>(<span class="dt">cluster =</span> <span class="kw">pam</span>(x,k, <span class="dt">cluster.only=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb82-6" title="6"></a>
<a class="sourceLine" id="cb82-7" title="7"><span class="co"># calculate the gap statistic</span></a>
<a class="sourceLine" id="cb82-8" title="8">pam.gap=<span class="st"> </span><span class="kw">clusGap</span>(<span class="kw">t</span>(mat), <span class="dt">FUN =</span> pam1, <span class="dt">K.max =</span> <span class="dv">8</span>,<span class="dt">B=</span><span class="dv">50</span>)</a>
<a class="sourceLine" id="cb82-9" title="9"></a>
<a class="sourceLine" id="cb82-10" title="10"><span class="co"># plot the gap statistic accross k values</span></a>
<a class="sourceLine" id="cb82-11" title="11"><span class="kw">plot</span>(pam.gap, <span class="dt">main =</span> <span class="st">&quot;Gap statistic for the &#39;Leukemia&#39; data&quot;</span>)</a></code></pre></div>
<p><img src="compgenomrReloaded_files/figure-html/clusGap-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In this case, gap statistic shows <span class="math inline">\(k=7\)</span> is the best. However, after <span class="math inline">\(K=6\)</span> the statistic has more or less a stable curve. In this case, we know that there are 5 main patient categories but this does not mean there is no sub-categories or sub-types for the cancers we are looking at.</p>
<p><a href="https://statweb.stanford.edu/~gwalther/gap" class="uri">https://statweb.stanford.edu/~gwalther/gap</a></p>
</div>
<div id="other-methods" class="section level4">
<h4><span class="header-section-number">4.4.4.3</span> Other methods</h4>
<p>There are several other methods that provide insight into how many clusters. In fact, the package <code>NbClust</code> provides 30 different ways to determine the number of optimal clusters and can offer a voting mechanism to pick the best number. Below, we are showing how to use this function for some of the optimal number of cluster detection methods.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" title="1"><span class="kw">library</span>(NbClust)</a>
<a class="sourceLine" id="cb83-2" title="2">nb =<span class="st"> </span><span class="kw">NbClust</span>(<span class="dt">data=</span><span class="kw">t</span>(mat), </a>
<a class="sourceLine" id="cb83-3" title="3">             <span class="dt">distance =</span> <span class="st">&quot;euclidean&quot;</span>, <span class="dt">min.nc =</span> <span class="dv">2</span>,</a>
<a class="sourceLine" id="cb83-4" title="4">        <span class="dt">max.nc =</span> <span class="dv">7</span>, <span class="dt">method =</span> <span class="st">&quot;kmeans&quot;</span>,</a>
<a class="sourceLine" id="cb83-5" title="5">        <span class="dt">index=</span><span class="kw">c</span>(<span class="st">&quot;kl&quot;</span>,<span class="st">&quot;ch&quot;</span>,<span class="st">&quot;cindex&quot;</span>,<span class="st">&quot;db&quot;</span>,<span class="st">&quot;silhouette&quot;</span>,</a>
<a class="sourceLine" id="cb83-6" title="6">                <span class="st">&quot;duda&quot;</span>,<span class="st">&quot;pseudot2&quot;</span>,<span class="st">&quot;beale&quot;</span>,<span class="st">&quot;ratkowsky&quot;</span>,</a>
<a class="sourceLine" id="cb83-7" title="7">                <span class="st">&quot;gap&quot;</span>,<span class="st">&quot;gamma&quot;</span>,<span class="st">&quot;mcclain&quot;</span>,<span class="st">&quot;gplus&quot;</span>,</a>
<a class="sourceLine" id="cb83-8" title="8">                <span class="st">&quot;tau&quot;</span>,<span class="st">&quot;sdindex&quot;</span>,<span class="st">&quot;sdbw&quot;</span>))</a>
<a class="sourceLine" id="cb83-9" title="9"></a>
<a class="sourceLine" id="cb83-10" title="10"><span class="kw">table</span>(nb<span class="op">$</span>Best.nc[<span class="dv">1</span>,]) <span class="co"># consensus seems to be 3 clusters </span></a></code></pre></div>
<p>However, the readers should keep in mind that clustering is an exploratory technique. If you have solid labels for your data points maybe clustering is just a sanity check, and you should just do predictive modeling instead. However, in biology there are rarely solid labels and things have different granularity. Take the leukemia patients case we have been using for example, it is know that leukemia types have subtypes and those sub-types that have different mutation profiles and consequently have different molecular signatures. Because of this, it is not surprising that some optimal cluster number techniques will find more clusters to be appropriate. On the other hand, CML (Chronic myeloid leukemia ) is a slow progressing disease and maybe as molecular signatures goes could be the closest to no leukemia patients, clustering algorithms may confuse the two depending on what granuality they are operating with. It is always good to look at the heatmaps after clustering, if you have meaningful self-similar data points even if the labels you have do not agree that there can be different clusters you can perform downstream analysis to understand the sub-clusters better. As we have seen, we can estimate optimal number of clusters but we can not take that estimation as the absolute truth, given more data points or different set of expression signatures you may have different optimal clusterings, or the supposed optimal clustering might overlook previously known sub-groups of your data.</p>
</div>
</div>
</div>
<p style="text-align: center;">
<a href="4-3-relationship-between-variables-linear-models-and-correlation.html"><button class="btn btn-default">Previous</button></a>
<a href="4-5-dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
